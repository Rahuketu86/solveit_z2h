<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Read the markdown file â€“ solveit_z2h</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d09c2ebb0ad6a85e36aef4f9137b58e4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Read the markdown file â€“ solveit_z2h">
<meta property="og:description" content="Zero to hero course done using solveit">
<meta property="og:site_name" content="solveit_z2h">
<meta name="twitter:title" content="Read the markdown file â€“ solveit_z2h">
<meta name="twitter:description" content="Zero to hero course done using solveit">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">solveit_z2h</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./summary.html">Read the markdown file</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">solveit_z2h</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">core</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./micrograd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">micrograd</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./utils.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">utils</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contextcollector.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Now you can call it:</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Read the markdown file</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lesson-1" id="toc-lesson-1" class="nav-link active" data-scroll-target="#lesson-1">Lesson 1</a>
  <ul class="collapse">
  <li><a href="#mindmap-of-the-lecture-building-understanding-micrograd" id="toc-mindmap-of-the-lecture-building-understanding-micrograd" class="nav-link" data-scroll-target="#mindmap-of-the-lecture-building-understanding-micrograd">ğŸ§  Mindmap of the Lecture â€œBuilding &amp; Understanding Microgradâ€</a>
  <ul class="collapse">
  <li><a href="#how-to-read-the-mindmap" id="toc-how-to-read-the-mindmap" class="nav-link" data-scroll-target="#how-to-read-the-mindmap">How to Read the Mindmap</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#makemore-characterlevel-language-modeling-mindmap" id="toc-makemore-characterlevel-language-modeling-mindmap" class="nav-link" data-scroll-target="#makemore-characterlevel-language-modeling-mindmap">ğŸ§  Makeâ€‘More: Characterâ€‘Level Language Modeling Mindâ€‘Map</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">1ï¸âƒ£ Overview</a></li>
  <li><a href="#core-idea-characterlevel-language-model" id="toc-core-idea-characterlevel-language-model" class="nav-link" data-scroll-target="#core-idea-characterlevel-language-model">2ï¸âƒ£ Core Idea: Characterâ€‘Level Language Model</a></li>
  <li><a href="#bigram-2gram-model-the-simple-baseline" id="toc-bigram-2gram-model-the-simple-baseline" class="nav-link" data-scroll-target="#bigram-2gram-model-the-simple-baseline">3ï¸âƒ£ Bigram (2â€‘gram) Model â€“ The Simple Baseline</a>
  <ul class="collapse">
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">3.1 Data Preparation</a></li>
  <li><a href="#extracting-bigrams" id="toc-extracting-bigrams" class="nav-link" data-scroll-target="#extracting-bigrams">3.2 Extracting Bigrams</a></li>
  <li><a href="#counting-frequencies-dictionary-2d-tensor" id="toc-counting-frequencies-dictionary-2d-tensor" class="nav-link" data-scroll-target="#counting-frequencies-dictionary-2d-tensor">3.3 Counting Frequencies (Dictionary â†’ 2â€‘D Tensor)</a></li>
  <li><a href="#visualisation-matplotlib" id="toc-visualisation-matplotlib" class="nav-link" data-scroll-target="#visualisation-matplotlib">3.4 Visualisation (matplotlib)</a></li>
  <li><a href="#refinement-of-token-set" id="toc-refinement-of-token-set" class="nav-link" data-scroll-target="#refinement-of-token-set">3.5 Refinement of Token Set</a></li>
  <li><a href="#from-counts-probabilities" id="toc-from-counts-probabilities" class="nav-link" data-scroll-target="#from-counts-probabilities">3.6 From Counts â†’ Probabilities</a></li>
  <li><a href="#sampling-names-using-the-bigram-table" id="toc-sampling-names-using-the-bigram-table" class="nav-link" data-scroll-target="#sampling-names-using-the-bigram-table">3.7 Sampling Names (using the bigram table)</a></li>
  <li><a href="#model-evaluation-likelihood" id="toc-model-evaluation-likelihood" class="nav-link" data-scroll-target="#model-evaluation-likelihood">3.8 Model Evaluation â€“ Likelihood</a></li>
  <li><a href="#smoothing-to-avoid-zero-probabilities" id="toc-smoothing-to-avoid-zero-probabilities" class="nav-link" data-scroll-target="#smoothing-to-avoid-zero-probabilities">3.9 Smoothing (to avoid zero probabilities)</a></li>
  </ul></li>
  <li><a href="#neuralnetwork-reimplementation-gradientbased" id="toc-neuralnetwork-reimplementation-gradientbased" class="nav-link" data-scroll-target="#neuralnetwork-reimplementation-gradientbased">4ï¸âƒ£ Neuralâ€‘Network Reâ€‘Implementation (Gradientâ€‘Based)</a>
  <ul class="collapse">
  <li><a href="#why-switch" id="toc-why-switch" class="nav-link" data-scroll-target="#why-switch">4.1 Why Switch?</a></li>
  <li><a href="#data-encoding-onehot-vectors" id="toc-data-encoding-onehot-vectors" class="nav-link" data-scroll-target="#data-encoding-onehot-vectors">4.2 Data Encoding â€“ Oneâ€‘Hot Vectors</a></li>
  <li><a href="#model-architecture-initially" id="toc-model-architecture-initially" class="nav-link" data-scroll-target="#model-architecture-initially">4.3 Model Architecture (initially)</a></li>
  <li><a href="#forward-pass-vectorised" id="toc-forward-pass-vectorised" class="nav-link" data-scroll-target="#forward-pass-vectorised">4.4 Forward Pass (vectorised)</a></li>
  <li><a href="#backpropagation-parameter-update" id="toc-backpropagation-parameter-update" class="nav-link" data-scroll-target="#backpropagation-parameter-update">4.5 Backâ€‘Propagation &amp; Parameter Update</a></li>
  <li><a href="#practical-pytorch-tips-from-the-transcript" id="toc-practical-pytorch-tips-from-the-transcript" class="nav-link" data-scroll-target="#practical-pytorch-tips-from-the-transcript">4.6 Practical PyTorch Tips (from the transcript)</a></li>
  </ul></li>
  <li><a href="#regularisation-implicit-smoothing" id="toc-regularisation-implicit-smoothing" class="nav-link" data-scroll-target="#regularisation-implicit-smoothing">5ï¸âƒ£ Regularisation &amp; Implicit Smoothing</a></li>
  <li><a href="#scaling-beyond-bigrams" id="toc-scaling-beyond-bigrams" class="nav-link" data-scroll-target="#scaling-beyond-bigrams">6ï¸âƒ£ Scaling Beyond Bigrams</a>
  <ul class="collapse">
  <li><a href="#wordlevel-modeling" id="toc-wordlevel-modeling" class="nav-link" data-scroll-target="#wordlevel-modeling">6.1 Wordâ€‘Level Modeling</a></li>
  <li><a href="#longer-contexts-ngrams-rnns-transformers" id="toc-longer-contexts-ngrams-rnns-transformers" class="nav-link" data-scroll-target="#longer-contexts-ngrams-rnns-transformers">6.2 Longer Contexts (nâ€‘grams, RNNs, Transformers)</a></li>
  <li><a href="#why-neural-nets-scale" id="toc-why-neural-nets-scale" class="nav-link" data-scroll-target="#why-neural-nets-scale">6.3 Why Neural Nets Scale</a></li>
  </ul></li>
  <li><a href="#future-roadmap-as-hinted-in-the-talk" id="toc-future-roadmap-as-hinted-in-the-talk" class="nav-link" data-scroll-target="#future-roadmap-as-hinted-in-the-talk">7ï¸âƒ£ Future Roadmap (as hinted in the talk)</a></li>
  <li><a href="#quick-reference-cheatsheet" id="toc-quick-reference-cheatsheet" class="nav-link" data-scroll-target="#quick-reference-cheatsheet">8ï¸âƒ£ Quick Reference Cheatâ€‘Sheet</a>
  <ul class="collapse">
  <li><a href="#takeaway" id="toc-takeaway" class="nav-link" data-scroll-target="#takeaway">ğŸ‰ Takeâ€‘away</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#comprehensive-mindmap-of-the-makemore-lecture" id="toc-comprehensive-mindmap-of-the-makemore-lecture" class="nav-link" data-scroll-target="#comprehensive-mindmap-of-the-makemore-lecture">ğŸ§  Comprehensive Mindâ€‘Map of the â€œMakemoreâ€ Lecture</a>
  <ul class="collapse">
  <li><a href="#introduction-recap" id="toc-introduction-recap" class="nav-link" data-scroll-target="#introduction-recap">1ï¸âƒ£ Introduction &amp; Recap</a></li>
  <li><a href="#moving-to-a-multilayer-perceptron-mlp" id="toc-moving-to-a-multilayer-perceptron-mlp" class="nav-link" data-scroll-target="#moving-to-a-multilayer-perceptron-mlp">2ï¸âƒ£ Moving to a Multiâ€‘Layer Perceptron (MLP)</a>
  <ul class="collapse">
  <li><a href="#core-idea-from-the-paper" id="toc-core-idea-from-the-paper" class="nav-link" data-scroll-target="#core-idea-from-the-paper">2.1 Core Idea from the Paper</a></li>
  <li><a href="#adapting-to-characters" id="toc-adapting-to-characters" class="nav-link" data-scroll-target="#adapting-to-characters">2.2 Adapting to Characters</a></li>
  </ul></li>
  <li><a href="#implementation-details-pytorch" id="toc-implementation-details-pytorch" class="nav-link" data-scroll-target="#implementation-details-pytorch">3ï¸âƒ£ Implementation Details (PyTorch)</a>
  <ul class="collapse">
  <li><a href="#data-preparation-1" id="toc-data-preparation-1" class="nav-link" data-scroll-target="#data-preparation-1">3.1 Data Preparation</a></li>
  <li><a href="#embedding-lookup-c" id="toc-embedding-lookup-c" class="nav-link" data-scroll-target="#embedding-lookup-c">3.2 Embedding Lookup (<code>C</code>)</a></li>
  <li><a href="#flattening-the-context" id="toc-flattening-the-context" class="nav-link" data-scroll-target="#flattening-the-context">3.3 Flattening the Context</a></li>
  <li><a href="#hidden-layer" id="toc-hidden-layer" class="nav-link" data-scroll-target="#hidden-layer">3.4 Hidden Layer</a></li>
  <li><a href="#output-layer" id="toc-output-layer" class="nav-link" data-scroll-target="#output-layer">3.5 Output Layer</a></li>
  <li><a href="#loss-computation" id="toc-loss-computation" class="nav-link" data-scroll-target="#loss-computation">3.6 Loss Computation</a></li>
  <li><a href="#training-loop-core-steps" id="toc-training-loop-core-steps" class="nav-link" data-scroll-target="#training-loop-core-steps">3.7 Training Loop (Core Steps)</a></li>
  <li><a href="#minibatch-training" id="toc-minibatch-training" class="nav-link" data-scroll-target="#minibatch-training">3.8 Miniâ€‘Batch Training</a></li>
  </ul></li>
  <li><a href="#hyperparameter-exploration" id="toc-hyperparameter-exploration" class="nav-link" data-scroll-target="#hyperparameter-exploration">4ï¸âƒ£ Hyperâ€‘Parameter Exploration</a>
  <ul class="collapse">
  <li><a href="#learningrate-search-practical-trick" id="toc-learningrate-search-practical-trick" class="nav-link" data-scroll-target="#learningrate-search-practical-trick">4.1 Learningâ€‘Rate Search (Practical Trick)</a></li>
  <li><a href="#overunderfitting-diagnosis" id="toc-overunderfitting-diagnosis" class="nav-link" data-scroll-target="#overunderfitting-diagnosis">4.2 Overâ€‘/Underâ€‘Fitting Diagnosis</a></li>
  </ul></li>
  <li><a href="#data-splits-evaluation" id="toc-data-splits-evaluation" class="nav-link" data-scroll-target="#data-splits-evaluation">5ï¸âƒ£ Data Splits &amp; Evaluation</a></li>
  <li><a href="#embedding-visualization-2d-case" id="toc-embedding-visualization-2d-case" class="nav-link" data-scroll-target="#embedding-visualization-2d-case">6ï¸âƒ£ Embedding Visualization (2â€‘D case)</a></li>
  <li><a href="#sampling-from-the-trained-model" id="toc-sampling-from-the-trained-model" class="nav-link" data-scroll-target="#sampling-from-the-trained-model">7ï¸âƒ£ Sampling from the Trained Model</a></li>
  <li><a href="#practical-tips-extras" id="toc-practical-tips-extras" class="nav-link" data-scroll-target="#practical-tips-extras">8ï¸âƒ£ Practical Tips &amp; Extras</a></li>
  <li><a href="#takeaways-next-steps" id="toc-takeaways-next-steps" class="nav-link" data-scroll-target="#takeaways-next-steps">9ï¸âƒ£ Takeâ€‘aways &amp; Next Steps</a>
  <ul class="collapse">
  <li><a href="#quick-reference-pseudocode" id="toc-quick-reference-pseudocode" class="nav-link" data-scroll-target="#quick-reference-pseudocode">ğŸ“Œ Quick Reference (Pseudoâ€‘code)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#mindmap-of-the-lecture" id="toc-mindmap-of-the-lecture" class="nav-link" data-scroll-target="#mindmap-of-the-lecture">ğŸ§  Mindâ€‘Map of the Lecture</a>
  <ul class="collapse">
  <li><a href="#recap-multilayer-perceptron-mlp-for-characterlevel-language-modelling" id="toc-recap-multilayer-perceptron-mlp-for-characterlevel-language-modelling" class="nav-link" data-scroll-target="#recap-multilayer-perceptron-mlp-for-characterlevel-language-modelling">1ï¸âƒ£ Recap: Multiâ€‘Layer Perceptron (MLP) for characterâ€‘level language modelling</a></li>
  <li><a href="#why-look-deeper" id="toc-why-look-deeper" class="nav-link" data-scroll-target="#why-look-deeper">2ï¸âƒ£ Why look deeper?</a></li>
  <li><a href="#problem-1-bad-initialisation-of-the-mlp" id="toc-problem-1-bad-initialisation-of-the-mlp" class="nav-link" data-scroll-target="#problem-1-bad-initialisation-of-the-mlp">3ï¸âƒ£ Problem #1 â€“ Bad Initialisation of the MLP</a>
  <ul class="collapse">
  <li><a href="#observed-symptom" id="toc-observed-symptom" class="nav-link" data-scroll-target="#observed-symptom">3.1 Observed symptom</a></li>
  <li><a href="#expected-loss-for-a-uniform-softmax" id="toc-expected-loss-for-a-uniform-softmax" class="nav-link" data-scroll-target="#expected-loss-for-a-uniform-softmax">3.2 Expected loss for a uniform softmax</a></li>
  <li><a href="#what-went-wrong" id="toc-what-went-wrong" class="nav-link" data-scroll-target="#what-went-wrong">3.3 What went wrong?</a></li>
  <li><a href="#fixes-applied" id="toc-fixes-applied" class="nav-link" data-scroll-target="#fixes-applied">3.4 Fixes applied</a></li>
  </ul></li>
  <li><a href="#problem-2-saturated-tanh-ğ‘¡ğ‘ğ‘›â„-activations" id="toc-problem-2-saturated-tanh-ğ‘¡ğ‘ğ‘›â„-activations" class="nav-link" data-scroll-target="#problem-2-saturated-tanh-ğ‘¡ğ‘ğ‘›â„-activations">4ï¸âƒ£ Problem #2 â€“ Saturatedâ€¯<code>tanh</code> (ğ‘¡ğ‘ğ‘›â„) activations</a>
  <ul class="collapse">
  <li><a href="#observation" id="toc-observation" class="nav-link" data-scroll-target="#observation">4.1 Observation</a></li>
  <li><a href="#consequence-for-backpropagation" id="toc-consequence-for-backpropagation" class="nav-link" data-scroll-target="#consequence-for-backpropagation">4.2 Consequence for backâ€‘propagation</a></li>
  <li><a href="#diagnostic-check" id="toc-diagnostic-check" class="nav-link" data-scroll-target="#diagnostic-check">4.3 Diagnostic check</a></li>
  <li><a href="#remedy" id="toc-remedy" class="nav-link" data-scroll-target="#remedy">4.4 Remedy</a></li>
  </ul></li>
  <li><a href="#general-weightinitialisation-theory" id="toc-general-weightinitialisation-theory" class="nav-link" data-scroll-target="#general-weightinitialisation-theory">5ï¸âƒ£ General Weightâ€‘Initialisation Theory</a>
  <ul class="collapse">
  <li><a href="#fanin-fanout-concept" id="toc-fanin-fanout-concept" class="nav-link" data-scroll-target="#fanin-fanout-concept">5.1 Fanâ€‘in / Fanâ€‘out concept</a></li>
  <li><a href="#gains-for-different-nonlinearities-he-xavier" id="toc-gains-for-different-nonlinearities-he-xavier" class="nav-link" data-scroll-target="#gains-for-different-nonlinearities-he-xavier">5.2 Gains for different nonâ€‘linearities (He / Xavier)</a></li>
  </ul></li>
  <li><a href="#batch-normalisation-batchnorm-the-gamechanger" id="toc-batch-normalisation-batchnorm-the-gamechanger" class="nav-link" data-scroll-target="#batch-normalisation-batchnorm-the-gamechanger">6ï¸âƒ£ Batch Normalisation (BatchNorm) â€“ The â€œGameâ€‘Changerâ€</a>
  <ul class="collapse">
  <li><a href="#core-idea" id="toc-core-idea" class="nav-link" data-scroll-target="#core-idea">6.1 Core idea</a></li>
  <li><a href="#why-it-works" id="toc-why-it-works" class="nav-link" data-scroll-target="#why-it-works">6.2 Why it works</a></li>
  <li><a href="#training-vs.-inference" id="toc-training-vs.-inference" class="nav-link" data-scroll-target="#training-vs.-inference">6.3 Training vs.&nbsp;Inference</a></li>
  <li><a href="#practical-notes" id="toc-practical-notes" class="nav-link" data-scroll-target="#practical-notes">6.4 Practical notes</a></li>
  </ul></li>
  <li><a href="#diagnostic-toolbox-for-neuralnet-health" id="toc-diagnostic-toolbox-for-neuralnet-health" class="nav-link" data-scroll-target="#diagnostic-toolbox-for-neuralnet-health">7ï¸âƒ£ Diagnostic Toolbox for Neuralâ€‘Net Health</a></li>
  <li><a href="#putting-it-all-together-torchify-the-code" id="toc-putting-it-all-together-torchify-the-code" class="nav-link" data-scroll-target="#putting-it-all-together-torchify-the-code">8ï¸âƒ£ Putting It All Together â€“ â€œTorchâ€‘ifyâ€ the Code</a>
  <ul class="collapse">
  <li><a href="#modular-design-mirrors-torch.nn" id="toc-modular-design-mirrors-torch.nn" class="nav-link" data-scroll-target="#modular-design-mirrors-torch.nn">8.1 Modular design (mirrors <code>torch.nn</code>)</a></li>
  <li><a href="#network-construction-pattern" id="toc-network-construction-pattern" class="nav-link" data-scroll-target="#network-construction-pattern">8.2 Network construction pattern</a></li>
  <li><a href="#training-loop-highlevel" id="toc-training-loop-highlevel" class="nav-link" data-scroll-target="#training-loop-highlevel">8.3 Training loop (highâ€‘level)</a></li>
  </ul></li>
  <li><a href="#takeaways-outlook" id="toc-takeaways-outlook" class="nav-link" data-scroll-target="#takeaways-outlook">9ï¸âƒ£ Takeâ€‘aways &amp; Outlook</a>
  <ul class="collapse">
  <li><a href="#quickreference-cheatsheet" id="toc-quickreference-cheatsheet" class="nav-link" data-scroll-target="#quickreference-cheatsheet">ğŸ“Œ Quickâ€‘Reference Cheatâ€‘Sheet</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#mindmap-manual-backpropagation-for-a-twolayer-mlp" id="toc-mindmap-manual-backpropagation-for-a-twolayer-mlp" class="nav-link" data-scroll-target="#mindmap-manual-backpropagation-for-a-twolayer-mlp">ğŸ§  Mindmap â€“ Manual Backâ€‘Propagation for a Twoâ€‘Layer MLP</a>
  <ul class="collapse">
  <li><a href="#overview-1" id="toc-overview-1" class="nav-link" data-scroll-target="#overview-1">1ï¸âƒ£ Overview</a></li>
  <li><a href="#historical-context" id="toc-historical-context" class="nav-link" data-scroll-target="#historical-context">2ï¸âƒ£ Historical Context</a></li>
  <li><a href="#network-architecture-forward-pass" id="toc-network-architecture-forward-pass" class="nav-link" data-scroll-target="#network-architecture-forward-pass">3ï¸âƒ£ Network Architecture (forward pass)</a></li>
  <li><a href="#manual-backpropagation-core-concepts" id="toc-manual-backpropagation-core-concepts" class="nav-link" data-scroll-target="#manual-backpropagation-core-concepts">4ï¸âƒ£ Manual Backâ€‘Propagation â€“ Core Concepts</a>
  <ul class="collapse">
  <li><a href="#gradient-of-the-loss-w.r.t.-log_probs-d_log_probs" id="toc-gradient-of-the-loss-w.r.t.-log_probs-d_log_probs" class="nav-link" data-scroll-target="#gradient-of-the-loss-w.r.t.-log_probs-d_log_probs">4.1 Gradient of the Loss w.r.t. <code>log_probs</code> (<code>d_log_probs</code>)</a></li>
  <li><a href="#backprop-through-log-probs" id="toc-backprop-through-log-probs" class="nav-link" data-scroll-target="#backprop-through-log-probs">4.2 Backâ€‘prop through <code>log</code> â†’ <code>probs</code></a></li>
  <li><a href="#backprop-through-softmax-logits-probs" id="toc-backprop-through-softmax-logits-probs" class="nav-link" data-scroll-target="#backprop-through-softmax-logits-probs">4.3 Backâ€‘prop through Softmax (logits â†’ probs)</a></li>
  <li><a href="#linear-layer-w2-b2" id="toc-linear-layer-w2-b2" class="nav-link" data-scroll-target="#linear-layer-w2-b2">4.4 Linear Layer (W2, B2)</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">4.5 Batch Normalization</a></li>
  <li><a href="#activation-10h-tanh" id="toc-activation-10h-tanh" class="nav-link" data-scroll-target="#activation-10h-tanh">4.6 Activation <code>10â€‘H</code> (tanh)</a></li>
  <li><a href="#linear-layer-w1-b1" id="toc-linear-layer-w1-b1" class="nav-link" data-scroll-target="#linear-layer-w1-b1">4.7 Linear Layer (W1, B1)</a></li>
  <li><a href="#embedding-lookup-indexing" id="toc-embedding-lookup-indexing" class="nav-link" data-scroll-target="#embedding-lookup-indexing">4.8 Embedding Lookup (indexing)</a></li>
  </ul></li>
  <li><a href="#exercises-progressive-refactoring" id="toc-exercises-progressive-refactoring" class="nav-link" data-scroll-target="#exercises-progressive-refactoring">5ï¸âƒ£ Exercises (Progressive Refactoring)</a></li>
  <li><a href="#intuitive-insights" id="toc-intuitive-insights" class="nav-link" data-scroll-target="#intuitive-insights">6ï¸âƒ£ Intuitive Insights</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">7ï¸âƒ£ Next Steps</a>
  <ul class="collapse">
  <li><a href="#tldr" id="toc-tldr" class="nav-link" data-scroll-target="#tldr">ğŸ“Œ TL;DR</a></li>
  </ul></li>
  <li><a href="#mindmap-of-the-lecture-1" id="toc-mindmap-of-the-lecture-1" class="nav-link" data-scroll-target="#mindmap-of-the-lecture-1">ğŸ“š Mindâ€‘Map of the Lecture</a>
  <ul class="collapse">
  <li><a href="#overview-2" id="toc-overview-2" class="nav-link" data-scroll-target="#overview-2">1ï¸âƒ£ Overview</a></li>
  <li><a href="#data-baseline-model" id="toc-data-baseline-model" class="nav-link" data-scroll-target="#data-baseline-model">2ï¸âƒ£ Data &amp; Baseline Model</a></li>
  <li><a href="#desired-architectural-changes" id="toc-desired-architectural-changes" class="nav-link" data-scroll-target="#desired-architectural-changes">3ï¸âƒ£ Desired Architectural Changes</a></li>
  <li><a href="#refactoring-the-code" id="toc-refactoring-the-code" class="nav-link" data-scroll-target="#refactoring-the-code">4ï¸âƒ£ Refactoring the Code</a></li>
  <li><a href="#implementing-hierarchical-fusion" id="toc-implementing-hierarchical-fusion" class="nav-link" data-scroll-target="#implementing-hierarchical-fusion">5ï¸âƒ£ Implementing Hierarchical Fusion</a></li>
  <li><a href="#batchnorm-bug-fix" id="toc-batchnorm-bug-fix" class="nav-link" data-scroll-target="#batchnorm-bug-fix">6ï¸âƒ£ BatchNorm Bug &amp; Fix</a></li>
  <li><a href="#training-results-hyperparameter-tweaks" id="toc-training-results-hyperparameter-tweaks" class="nav-link" data-scroll-target="#training-results-hyperparameter-tweaks">7ï¸âƒ£ Training Results &amp; Hyperâ€‘parameter Tweaks</a></li>
  <li><a href="#relation-to-convolutional-networks-wavenet" id="toc-relation-to-convolutional-networks-wavenet" class="nav-link" data-scroll-target="#relation-to-convolutional-networks-wavenet">8ï¸âƒ£ Relation to Convolutional Networks (Wavenet)</a></li>
  <li><a href="#development-process-insights" id="toc-development-process-insights" class="nav-link" data-scroll-target="#development-process-insights">9ï¸âƒ£ Development Process Insights</a></li>
  <li><a href="#future-directions-open-topics" id="toc-future-directions-open-topics" class="nav-link" data-scroll-target="#future-directions-open-topics">ğŸ”Ÿ Future Directions (Open Topics)</a></li>
  </ul></li>
  <li><a href="#mermaid-mindmap-copypaste-into-a-mermaidenabled-markdown-viewer" id="toc-mermaid-mindmap-copypaste-into-a-mermaidenabled-markdown-viewer" class="nav-link" data-scroll-target="#mermaid-mindmap-copypaste-into-a-mermaidenabled-markdown-viewer">ğŸ§­ Mermaid Mindâ€‘Map (copyâ€‘paste into a Mermaidâ€‘enabled markdown viewer)</a></li>
  </ul></li>
  <li><a href="#mindmap-of-the-transcript" id="toc-mindmap-of-the-transcript" class="nav-link" data-scroll-target="#mindmap-of-the-transcript">ğŸ§  Mindâ€‘Map of the Transcript</a>
  <ul class="collapse">
  <li><a href="#introduction-motivation" id="toc-introduction-motivation" class="nav-link" data-scroll-target="#introduction-motivation">1. Introduction &amp; Motivation</a></li>
  <li><a href="#languagemodel-basics" id="toc-languagemodel-basics" class="nav-link" data-scroll-target="#languagemodel-basics">2. Languageâ€‘Model Basics</a></li>
  <li><a href="#data-set-tiny-shakespeare" id="toc-data-set-tiny-shakespeare" class="nav-link" data-scroll-target="#data-set-tiny-shakespeare">3. Data Set â€“ â€œTiny Shakespeareâ€</a></li>
  <li><a href="#model-architecture-from-simple-to-full-transformer" id="toc-model-architecture-from-simple-to-full-transformer" class="nav-link" data-scroll-target="#model-architecture-from-simple-to-full-transformer">4. Model Architecture â€“ From Simple to Full Transformer</a>
  <ul class="collapse">
  <li><a href="#simple-baseline-bytelevel-byr-model" id="toc-simple-baseline-bytelevel-byr-model" class="nav-link" data-scroll-target="#simple-baseline-bytelevel-byr-model">4.1. Simple Baseline: Byteâ€‘Level (BYR) Model</a></li>
  <li><a href="#adding-positional-information" id="toc-adding-positional-information" class="nav-link" data-scroll-target="#adding-positional-information">4.2. Adding Positional Information</a></li>
  <li><a href="#selfattention-single-head" id="toc-selfattention-single-head" class="nav-link" data-scroll-target="#selfattention-single-head">4.3. Selfâ€‘Attention (single head)</a></li>
  <li><a href="#multihead-attention" id="toc-multihead-attention" class="nav-link" data-scroll-target="#multihead-attention">4.4. Multiâ€‘Head Attention</a></li>
  <li><a href="#feedforward-network-ffn" id="toc-feedforward-network-ffn" class="nav-link" data-scroll-target="#feedforward-network-ffn">4.5. Feedâ€‘Forward Network (FFN)</a></li>
  <li><a href="#residual-skip-connections" id="toc-residual-skip-connections" class="nav-link" data-scroll-target="#residual-skip-connections">4.6. Residual (Skip) Connections</a></li>
  <li><a href="#layer-normalization" id="toc-layer-normalization" class="nav-link" data-scroll-target="#layer-normalization">4.7. Layer Normalization</a></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">4.8. Dropout</a></li>
  <li><a href="#full-decoderonly-block" id="toc-full-decoderonly-block" class="nav-link" data-scroll-target="#full-decoderonly-block">4.9. Full Decoderâ€‘Only Block</a></li>
  <li><a href="#stacking-blocks" id="toc-stacking-blocks" class="nav-link" data-scroll-target="#stacking-blocks">4.10. Stacking Blocks</a></li>
  <li><a href="#final-projection" id="toc-final-projection" class="nav-link" data-scroll-target="#final-projection">4.11. Final Projection</a></li>
  </ul></li>
  <li><a href="#training-procedure" id="toc-training-procedure" class="nav-link" data-scroll-target="#training-procedure">5. Training Procedure</a></li>
  <li><a href="#scaling-experiments-results" id="toc-scaling-experiments-results" class="nav-link" data-scroll-target="#scaling-experiments-results">6. Scaling Experiments &amp; Results</a></li>
  <li><a href="#decoderonly-vs-encoderdecoder" id="toc-decoderonly-vs-encoderdecoder" class="nav-link" data-scroll-target="#decoderonly-vs-encoderdecoder">7. Decoderâ€‘Only vs Encoderâ€‘Decoder</a></li>
  <li><a href="#finetuning-alignment-chatgpt" id="toc-finetuning-alignment-chatgpt" class="nav-link" data-scroll-target="#finetuning-alignment-chatgpt">8. Fineâ€‘Tuning &amp; Alignment (ChatGPT)</a></li>
  <li><a href="#nanogpt-repository-by-the-presenter" id="toc-nanogpt-repository-by-the-presenter" class="nav-link" data-scroll-target="#nanogpt-repository-by-the-presenter">9. nanogpt Repository (by the presenter)</a></li>
  <li><a href="#takeaways-next-steps-1" id="toc-takeaways-next-steps-1" class="nav-link" data-scroll-target="#takeaways-next-steps-1">10. Takeâ€‘aways &amp; Next Steps</a></li>
  <li><a href="#tokenization-mindmap-markdown-mermaid" id="toc-tokenization-mindmap-markdown-mermaid" class="nav-link" data-scroll-target="#tokenization-mindmap-markdown-mermaid">ğŸ§  Tokenization Mindâ€‘Map (Markdownâ€¯+â€¯Mermaid)</a>
  <ul class="collapse">
  <li><a href="#mermaid-diagram" id="toc-mermaid-diagram" class="nav-link" data-scroll-target="#mermaid-diagram">Mermaid Diagram</a></li>
  <li><a href="#plaintext-outline-for-quick-skim" id="toc-plaintext-outline-for-quick-skim" class="nav-link" data-scroll-target="#plaintext-outline-for-quick-skim">Plainâ€‘Text Outline (for quick skim)</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/Rahuketu86/solveit_z2h/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="summary.html.md"><i class="bi bi-file-code"></i>CommonMark</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Read the markdown file</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<div id="6daea139" class="cell" data-time_run="2025-12-18T06:04:59.540312+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dialoghelper</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dialoghelper <span class="im">import</span> <span class="op">*</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="dfa6fa36" class="cell" data-time_run="2025-12-18T06:04:59.570984+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span>(dialoghelper)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="0">
<pre><code>['Placements',
 '__builtins__',
 '__cached__',
 '__doc__',
 '__file__',
 '__loader__',
 '__name__',
 '__package__',
 '__path__',
 '__spec__',
 '__version__',
 'add_html',
 'add_msg',
 'add_scr',
 'add_styles',
 'ast_grep',
 'ast_py',
 'call_endp',
 'core',
 'curr_dialog',
 'del_msg',
 'dh_settings',
 'empty',
 'event_get',
 'fc_tool_info',
 'find_dname',
 'find_msg_id',
 'find_msgs',
 'find_var',
 'fire_event',
 'gist_file',
 'iife',
 'import_gist',
 'import_string',
 'is_usable_tool',
 'load_gist',
 'md_cls_d',
 'mk_toollist',
 'msg_del_lines',
 'msg_idx',
 'msg_insert_line',
 'msg_replace_lines',
 'msg_str_replace',
 'msg_strs_replace',
 'pop_data',
 'read_msg',
 'run_msg',
 'set_var',
 'tool_info',
 'update_msg',
 'url2note']</code></pre>
</div>
</div>
<div id="0c833cc4" class="cell" data-time_run="2025-12-18T06:08:17.956472+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the markdown file</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>summary_md <span class="op">=</span> pathlib.Path(<span class="st">"./transcripts/Summary.md"</span>).read_text()<span class="op">;</span> summary_md</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>add_msg(summary_md, msg_type<span class="op">=</span><span class="st">'note'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="0">
<pre><code>'_45df39f8'</code></pre>
</div>
</div>
<section id="lesson-1" class="level1">
<h1>Lesson 1</h1>
<section id="mindmap-of-the-lecture-building-understanding-micrograd" class="level2">
<h2 class="anchored" data-anchor-id="mindmap-of-the-lecture-building-understanding-micrograd">ğŸ§  Mindmap of the Lecture â€œBuilding &amp; Understanding Microgradâ€</h2>
<pre class="mermaid"><code>mindmap
  root((Micrograd Lecture â€“ Neuralâ€‘Net Training Under the Hood))

  %% -------------------------------------------------
  %% 1. Introduction &amp; Goals
  %% -------------------------------------------------
  Introduction
    What weâ€™ll do
      â€¢ Start from a blank Jupyter notebook
      â€¢ Define &amp; train a neural net stepâ€‘byâ€‘step
      â€¢ Peek â€œunder the hoodâ€ of backâ€‘propagation
    Why it matters
      â€¢ Intuitive grasp of gradientâ€‘based learning
      â€¢ Build a tiny autograd engine (micrograd)

  %% -------------------------------------------------
  %% 2. Micrograd â€“ The Core Idea
  %% -------------------------------------------------
  Micrograd
    Definition
      â€¢ A **scalarâ€‘valued autograd engine**
      â€¢ Implements **automatic differentiation** (backâ€‘prop)
    Key Concepts
      â€¢ **Value object** â€“ wraps a scalar &amp; tracks graph info
      â€¢ **Expression graph** â€“ nodes = operations, edges = data flow
      â€¢ **Forward pass** â€“ compute output (e.g. g = â€¦)
      â€¢ **Backward pass** â€“ propagate gradients via chain rule
    Operations supported
      â€¢ add (+)          â€“ local derivative = 1
      â€¢ mul (Ã—)          â€“ local derivative = other operand
      â€¢ pow (^)          â€“ local derivative = nÂ·xâ¿â»Â¹
      â€¢ tanh (10h)       â€“ local derivative = 1 â€“ tanhÂ²(x)
      â€¢ neg, sqrt, div, â€¦ (can be added similarly)

  %% -------------------------------------------------
  %% 3. Derivative Intuition
  %% -------------------------------------------------
  Derivative Basics
    â€¢ Definition: limâ‚•â†’0 (f(x+h) â€“ f(x))/h
    â€¢ Numerical approximation with tiny h (e.g. 0.001)
    â€¢ Example: f(x)=3xÂ²â€“4x+5 â†’ fâ€²(3)=14
    â€¢ Sign tells direction of change (positive â†’ increase)

  %% -------------------------------------------------
  %% 4. Building the Value Object
  %% -------------------------------------------------
  Value Object
    Attributes
      â€¢ data  â€“ raw scalar
      â€¢ grad  â€“ âˆ‚output/âˆ‚self (init 0)
      â€¢ _prev â€“ set of child nodes (for graph traversal)
      â€¢ _op   â€“ string name of operation that created it
    Operator Overloads
      â€¢ __add__(self, other) â†’ Value
      â€¢ __radd__(self, other) â†’ Value (handles const + Value)
      â€¢ __mul__(self, other) â†’ Value
      â€¢ __rmul__(self, other) â†’ Value (handles const Ã— Value)
      â€¢ __pow__(self, exponent) â†’ Value
      â€¢ __neg__, __sub__, __truediv__ (via mul &amp; pow)
    Local Backward Functions
      â€¢ add:   self.grad += out.grad ; other.grad += out.grad
      â€¢ mul:   self.grad += other.data * out.grad
               other.grad += self.data * out.grad
      â€¢ pow:   self.grad += exponent * (self.data**(exponent-1)) * out.grad
      â€¢ tanh:  self.grad += (1 - self.data**2) * out.grad

  %% -------------------------------------------------
  %% 5. Graph Visualization
  %% -------------------------------------------------
  Graph Drawing (drawdot)
    â€¢ Uses Graphviz (graphvizâ€‘dot) to render nodes &amp; edges
    â€¢ Shows operation nodes (e.g. â€œ+â€, â€œ*â€) and value nodes
    â€¢ Helpful for debugging &amp; teaching

  %% -------------------------------------------------
  %% 6. Backâ€‘Propagation Mechanics
  %% -------------------------------------------------
  Backâ€‘Propagation
    â€¢ Start at output node â†’ set grad = 1
    â€¢ **Topological sort** (DFS) to order nodes so children processed first
    â€¢ Walk nodes in reverse topological order, calling each nodeâ€™s _backward
    â€¢ **Chain rule**: local derivative Ã— upstream gradient
    â€¢ Gradient accumulation (`+=`) â€“ crucial when a node has multiple parents
      â€“ Fixed bug: previously used `=` causing overwrites (e.g. a + a)

  %% -------------------------------------------------
  %% 7. Numerical Gradient Checks
  %% -------------------------------------------------
  Gradient Check
    â€¢ Perturb a leaf (e.g. a += h) â†’ recompute output
    â€¢ Approximate âˆ‚output/âˆ‚a â‰ˆ (f(a+h) â€“ f(a))/h
    â€¢ Verify against analytically computed grads

  %% -------------------------------------------------
  %% 8. Building Neural Networks with Micrograd
  %% -------------------------------------------------
  Neuralâ€‘Net Construction
    Neuron
      â€¢ Parameters: weights `w_i` (list of Value) + bias `b`
      â€¢ Forward: Î£ w_iÂ·x_i + b â†’ activation (tanh)
    Layer
      â€¢ List of Neurons (fullyâ€‘connected to same inputs)
      â€¢ Forward: apply each neuron, collect outputs
    MLP (Multiâ€‘Layer Perceptron)
      â€¢ Sequence of Layers
      â€¢ Forward: feed output of one layer as input to next
      â€¢ Example architecture: 3â€‘input â†’ [4,4] hidden â†’ 1â€‘output

  %% -------------------------------------------------
  %% 9. Loss Functions &amp; Training Loop
  %% -------------------------------------------------
  Loss &amp; Optimization
    Loss (Meanâ€‘Squaredâ€‘Error)
      â€¢ L = Î£ (y_pred â€“ y_true)Â²
      â€¢ Gradient w.r.t. each prediction = 2Â·(y_pred â€“ y_true)
    Training Loop (Stochastic Gradient Descent)
      1. Zero grads (`p.grad = 0` for all params)
      2. Forward pass â†’ compute loss
      3. Backward pass (`loss.backward()`)
      4. Parameter update: `p.data -= lr * p.grad`
      5. Repeat (adjust learningâ€‘rate, optionally decay)
    Common Pitfalls
      â€¢ Forgetting to zero grads â†’ gradient accumulation bug
      â€¢ Too large learning rate â†’ overshoot / divergence
      â€¢ Too small â†’ slow convergence

  %% -------------------------------------------------
  %% 10. Comparison with PyTorch
  %% -------------------------------------------------
  PyTorch Parallel
    â€¢ Same API: `torch.tensor`, `requires_grad=True`
    â€¢ Autograd automatically builds the same graph (but with tensors)
    â€¢ Forward &amp; backward behave identically for scalar case
    â€¢ Example: replicate micrograd network using `torch.nn.Module`
    â€¢ Extending PyTorch
      â€“ Register custom ops (forward + backward) via `torch.autograd.Function`
      â€“ Example shown for a cubic polynomial

  %% -------------------------------------------------
  â”‚ 11. Advanced Topics (Briefly Mentioned)
  %% -------------------------------------------------
  Advanced Topics
    â€¢ Batching &amp; miniâ€‘batch SGD (process subsets of data)
    â€¢ Crossâ€‘entropy loss for classification
    â€¢ L2 regularization (weight decay) for better generalization
    â€¢ Learningâ€‘rate schedules (decay, momentum, Adam, etc.)
    â€¢ Scaling to billions of parameters (e.g., GPTâ€‘style models)

  %% -------------------------------------------------
  %% 12. Summary &amp; Takeâ€‘aways
  %% -------------------------------------------------
  Summary
    â€¢ Neural nets = **compositional mathematical expressions**
    â€¢ **Backâ€‘prop = recursive application of the chain rule**
    â€¢ Micrograd shows the **minimal code** needed (â‰ˆ150â€¯lines)
    â€¢ Real libraries (PyTorch) add **tensor efficiency** but same math
    â€¢ Understanding the core mechanics helps debug &amp; extend models</code></pre>
<section id="how-to-read-the-mindmap" class="level3">
<h3 class="anchored" data-anchor-id="how-to-read-the-mindmap">How to Read the Mindmap</h3>
<ul>
<li><strong>Indentation</strong> â†’ hierarchy (main topic â†’ subâ€‘topic â†’ details).<br>
</li>
<li><strong>Bold headings</strong> indicate the most important concepts.<br>
</li>
<li><strong>Arrows (â†’)</strong> show data flow or process steps (e.g., forward â†’ backward).<br>
</li>
<li><strong>Bullet points</strong> under each node give concrete examples, code snippets, or key takeâ€‘aways.</li>
</ul>
<p>Feel free to copy the Mermaid block into any Markdown viewer that supports Mermaid (e.g., GitHub, VSâ€¯Code, Jupyter with <code>%%mermaid</code>) to see the visual mindmap. Happy learning! ğŸš€ # Lesson 2</p>
</section>
</section>
</section>
<section id="makemore-characterlevel-language-modeling-mindmap" class="level1">
<h1>ğŸ§  Makeâ€‘More: Characterâ€‘Level Language Modeling Mindâ€‘Map</h1>
<p><em>(All concepts are derived from the transcript. Indentation = hierarchy.)</em></p>
<hr>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">1ï¸âƒ£ Overview</h2>
<ul>
<li><strong>Goal</strong> â€“ Reâ€‘implement <em>micrograd</em>â€‘style learning on a new repo <strong>makeâ€‘more</strong>.<br>
</li>
<li><strong>Dataset</strong> â€“ <code>names.txt</code> (~32â€¯000 unique names scraped from a government site).<br>
</li>
<li><strong>Useâ€‘case</strong> â€“ Generate novel, nameâ€‘like strings (e.g., babyâ€‘name suggestions).</li>
</ul>
<hr>
</section>
<section id="core-idea-characterlevel-language-model" class="level2">
<h2 class="anchored" data-anchor-id="core-idea-characterlevel-language-model">2ï¸âƒ£ Core Idea: Characterâ€‘Level Language Model</h2>
<ul>
<li><strong>Treat each name as a sequence of characters</strong> (including start &amp; end tokens).<br>
</li>
<li><strong>Model task</strong> â€“ Predict the <em>next</em> character given the previous one(s).</li>
</ul>
<hr>
</section>
<section id="bigram-2gram-model-the-simple-baseline" class="level2">
<h2 class="anchored" data-anchor-id="bigram-2gram-model-the-simple-baseline">3ï¸âƒ£ Bigram (2â€‘gram) Model â€“ The Simple Baseline</h2>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">3.1 Data Preparation</h3>
<ul>
<li>Load <code>names.txt</code> â†’ massive string â†’ <code>splitlines()</code> â†’ list <code>words</code>.<br>
</li>
<li>Compute:
<ul>
<li><code>num_words â‰ˆ 32â€¯000</code><br>
</li>
<li><code>min_len = 2</code>, <code>max_len = 15</code></li>
</ul></li>
</ul>
</section>
<section id="extracting-bigrams" class="level3">
<h3 class="anchored" data-anchor-id="extracting-bigrams">3.2 Extracting Bigrams</h3>
<ul>
<li><p>For each word <code>w</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c1, c2 <span class="kw">in</span> <span class="bu">zip</span>(w, w[<span class="dv">1</span>:]):   <span class="co"># sliding window of size 2</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    bigram <span class="op">=</span> (c1, c2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Add <strong>special start token</strong> <code>.</code> before the first char and <strong>end token</strong> <code>.</code> after the last char.</p></li>
</ul>
</section>
<section id="counting-frequencies-dictionary-2d-tensor" class="level3">
<h3 class="anchored" data-anchor-id="counting-frequencies-dictionary-2d-tensor">3.3 Counting Frequencies (Dictionary â†’ 2â€‘D Tensor)</h3>
<ul>
<li><code>counts[prev_char, next_char] += 1</code> (default 0).<br>
</li>
<li>Convert to a <strong>28â€¯Ã—â€¯28</strong> integer tensor (<code>torch.int32</code>).
<ul>
<li>26 letters + <strong>start</strong> (<code>.</code>) + <strong>end</strong> (<code>.</code>).</li>
</ul></li>
</ul>
</section>
<section id="visualisation-matplotlib" class="level3">
<h3 class="anchored" data-anchor-id="visualisation-matplotlib">3.4 Visualisation (matplotlib)</h3>
<ul>
<li>Heatâ€‘map of the count matrix.<br>
</li>
<li>Observations:
<ul>
<li>Rows/columns for start/end tokens are mostly zeros (they never appear in the â€œwrongâ€ position).</li>
</ul></li>
</ul>
</section>
<section id="refinement-of-token-set" class="level3">
<h3 class="anchored" data-anchor-id="refinement-of-token-set">3.5 Refinement of Token Set</h3>
<ul>
<li>Collapse to <strong>27â€¯Ã—â€¯27</strong> matrix (single special token).<br>
</li>
<li>Reâ€‘index:
<ul>
<li><code>.</code> â†’ indexâ€¯0, <code>a</code> â†’ 1, â€¦, <code>z</code> â†’ 26.</li>
</ul></li>
</ul>
</section>
<section id="from-counts-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="from-counts-probabilities">3.6 From Counts â†’ Probabilities</h3>
<ul>
<li><p>Rowâ€‘wise normalization:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts.<span class="bu">float</span>() <span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Each row now sums to <strong>1</strong> â†’ a categorical distribution for the next character.</p></li>
</ul>
</section>
<section id="sampling-names-using-the-bigram-table" class="level3">
<h3 class="anchored" data-anchor-id="sampling-names-using-the-bigram-table">3.7 Sampling Names (using the bigram table)</h3>
<ul>
<li>Start at indexâ€¯0 (<code>.</code>).<br>
</li>
<li>Loop:
<ol type="1">
<li>Grab current row <code>p = probs[current_idx]</code>.<br>
</li>
<li>Sample <code>next_idx = torch.multinomial(p, 1, replacement=True, generator=g)</code>.<br>
</li>
<li>Break if <code>next_idx == 0</code> (end token).<br>
</li>
<li>Append the decoded character.</li>
</ol></li>
</ul>
</section>
<section id="model-evaluation-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="model-evaluation-likelihood">3.8 Model Evaluation â€“ Likelihood</h3>
<ul>
<li><strong>Likelihood</strong> = product of probabilities assigned to the true bigrams.<br>
</li>
<li><strong>Logâ€‘likelihood</strong> = sum of <code>log(p_i)</code>.<br>
</li>
<li><strong>Negative Logâ€‘Likelihood (NLL)</strong> = <code>â€‘log_likelihood</code> â†’ standard loss (lower is better).<br>
</li>
<li>Example: NLL â‰ˆ <strong>2.4â€“2.5</strong> on the full training set.</li>
</ul>
</section>
<section id="smoothing-to-avoid-zero-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="smoothing-to-avoid-zero-probabilities">3.9 Smoothing (to avoid zero probabilities)</h3>
<ul>
<li>Add a small constant (e.g., <code>+1</code>) to every count before normalisation.<br>
</li>
<li>Guarantees nonâ€‘zero probabilities â†’ prevents infinite NLL for unseen bigrams.</li>
</ul>
<hr>
</section>
</section>
<section id="neuralnetwork-reimplementation-gradientbased" class="level2">
<h2 class="anchored" data-anchor-id="neuralnetwork-reimplementation-gradientbased">4ï¸âƒ£ Neuralâ€‘Network Reâ€‘Implementation (Gradientâ€‘Based)</h2>
<section id="why-switch" class="level3">
<h3 class="anchored" data-anchor-id="why-switch">4.1 Why Switch?</h3>
<ul>
<li>Counting works for bigrams but <strong>doesnâ€™t scale</strong> to longer contexts (e.g., 10â€‘grams).<br>
</li>
<li>Neural nets can learn <strong>compact, differentiable</strong> representations for arbitrary context lengths.</li>
</ul>
</section>
<section id="data-encoding-onehot-vectors" class="level3">
<h3 class="anchored" data-anchor-id="data-encoding-onehot-vectors">4.2 Data Encoding â€“ Oneâ€‘Hot Vectors</h3>
<ul>
<li>Map each character index <code>i</code> â†’ 27â€‘dimensional oneâ€‘hot vector <code>x_i</code>.<br>
</li>
<li>Use <code>torch.nn.functional.one_hot(indices, num_classes=27)</code>.<br>
</li>
<li>Cast to <code>float32</code> for NN input.</li>
</ul>
</section>
<section id="model-architecture-initially" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture-initially">4.3 Model Architecture (initially)</h3>
<ul>
<li><p><strong>Linear layer</strong> (no bias): <code>logits = x @ W</code></p>
<ul>
<li><code>W</code> shape <strong>27â€¯Ã—â€¯27</strong> (each row = logâ€‘counts for a given previous character).<br>
</li>
</ul></li>
<li><p><strong>Softmax</strong> â†’ probabilities:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> torch.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)   <span class="co"># exponentiate + normalise internally</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
</section>
<section id="forward-pass-vectorised" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass-vectorised">4.4 Forward Pass (vectorised)</h3>
<ol type="1">
<li><p>Encode all inputs â†’ <code>X</code> (Nâ€¯Ã—â€¯27).<br>
</p></li>
<li><p>Compute <code>logits = X @ W</code>.<br>
</p></li>
<li><p><code>probs = softmax(logits)</code>.<br>
</p></li>
<li><p>Gather the probability of the <em>true</em> next character:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>true_probs <span class="op">=</span> probs[torch.arange(N), targets]   <span class="co"># targets = nextâ€‘char indices</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Compute <strong>NLL loss</strong>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>torch.log(true_probs).mean()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
</section>
<section id="backpropagation-parameter-update" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-parameter-update">4.5 Backâ€‘Propagation &amp; Parameter Update</h3>
<ul>
<li><p>Zero grads: <code>W.grad = None</code>.<br>
</p></li>
<li><p><code>loss.backward()</code> â†’ fills <code>W.grad</code>.<br>
</p></li>
<li><p>Gradient descent step (e.g., SGD):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>W.data <span class="op">-=</span> lr <span class="op">*</span> W.grad</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Repeat for many epochs â†’ loss drops from ~3.8 â†’ ~2.4 (matches counting approach).</p></li>
</ul>
</section>
<section id="practical-pytorch-tips-from-the-transcript" class="level3">
<h3 class="anchored" data-anchor-id="practical-pytorch-tips-from-the-transcript">4.6 Practical PyTorch Tips (from the transcript)</h3>
<ul>
<li><code>torch.tensor</code> vs <code>torch.Tensor</code> â†’ prefer lowercase <code>torch.tensor</code> for float tensors.<br>
</li>
<li><strong>Broadcasting</strong>: dividing a (27â€¯Ã—â€¯27) matrix by a (27â€¯Ã—â€¯1) column works because the column is broadcast across rows.<br>
</li>
<li><code>requires_grad=True</code> on parameters to enable autograd.<br>
</li>
<li>Use <code>torch.Generator</code> with a fixed seed for deterministic sampling.</li>
</ul>
<hr>
</section>
</section>
<section id="regularisation-implicit-smoothing" class="level2">
<h2 class="anchored" data-anchor-id="regularisation-implicit-smoothing">5ï¸âƒ£ Regularisation &amp; Implicit Smoothing</h2>
<ul>
<li><p><strong>L2 regularisation</strong> (weight decay) on <code>W</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> Î» <span class="op">*</span> (W<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>loss_total <span class="op">=</span> loss <span class="op">+</span> reg</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>When <code>W</code> â†’ 0, logits become uniform â†’ equivalent to <strong>label smoothing</strong>.<br>
</p></li>
<li><p>Adjust Î» to control the tradeâ€‘off between fitting data and keeping probabilities smooth.</p></li>
</ul>
<hr>
</section>
<section id="scaling-beyond-bigrams" class="level2">
<h2 class="anchored" data-anchor-id="scaling-beyond-bigrams">6ï¸âƒ£ Scaling Beyond Bigrams</h2>
<section id="wordlevel-modeling" class="level3">
<h3 class="anchored" data-anchor-id="wordlevel-modeling">6.1 Wordâ€‘Level Modeling</h3>
<ul>
<li>Extend the same pipeline to <strong>tokens = words</strong> (instead of characters).<br>
</li>
<li>Larger vocab â†’ larger embedding/linear layers.</li>
</ul>
</section>
<section id="longer-contexts-ngrams-rnns-transformers" class="level3">
<h3 class="anchored" data-anchor-id="longer-contexts-ngrams-rnns-transformers">6.2 Longer Contexts (nâ€‘grams, RNNs, Transformers)</h3>
<ul>
<li>Feed <strong>multiple previous characters</strong> (or embeddings) into deeper networks:
<ul>
<li><strong>RNN / LSTM</strong> â†’ hidden state carries history.<br>
</li>
<li><strong>Transformer</strong> â†’ selfâ€‘attention over the whole context.<br>
</li>
</ul></li>
<li>Output layer always produces <strong>logits â†’ softmax â†’ probability distribution</strong> for the next token.</li>
</ul>
</section>
<section id="why-neural-nets-scale" class="level3">
<h3 class="anchored" data-anchor-id="why-neural-nets-scale">6.3 Why Neural Nets Scale</h3>
<ul>
<li>Counting tables would explode (<code>|V|^k</code> entries for kâ€‘gram).<br>
</li>
<li>Parameter sharing in NN (weights) keeps model size <strong>linear</strong> in vocabulary size, not exponential in context length.</li>
</ul>
<hr>
</section>
</section>
<section id="future-roadmap-as-hinted-in-the-talk" class="level2">
<h2 class="anchored" data-anchor-id="future-roadmap-as-hinted-in-the-talk">7ï¸âƒ£ Future Roadmap (as hinted in the talk)</h2>
<ol type="1">
<li><strong>Wordâ€‘level language model</strong> â€“ generate full sentences.<br>
</li>
<li><strong>Imageâ€‘text models</strong> â€“ e.g., DALLÂ·E, Stable Diffusion.<br>
</li>
<li><strong>Full transformer implementation</strong> â€“ equivalent to GPTâ€‘2 at character level, then scale up.</li>
</ol>
<hr>
</section>
<section id="quick-reference-cheatsheet" class="level2">
<h2 class="anchored" data-anchor-id="quick-reference-cheatsheet">8ï¸âƒ£ Quick Reference Cheatâ€‘Sheet</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 41%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Symbol / Code</th>
<th>Key Insight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Start token</strong></td>
<td><code>.</code> (indexâ€¯0)</td>
<td>Marks beginning of a name</td>
</tr>
<tr class="even">
<td><strong>End token</strong></td>
<td><code>.</code> (indexâ€¯0 after collapse)</td>
<td>Marks termination</td>
</tr>
<tr class="odd">
<td><strong>Bigram count matrix</strong></td>
<td><code>N</code> (28â€¯Ã—â€¯28)</td>
<td>Raw frequencies</td>
</tr>
<tr class="even">
<td><strong>Probability matrix</strong></td>
<td><code>P = N / N.sum(dim=1, keepdim=True)</code></td>
<td>Rowâ€‘wise categorical distribution</td>
</tr>
<tr class="odd">
<td><strong>Oneâ€‘hot encoding</strong></td>
<td><code>x_i = F.one_hot(i, 27).float()</code></td>
<td>Turns integer index into NN input</td>
</tr>
<tr class="even">
<td><strong>Weight matrix</strong></td>
<td><code>W</code> (27â€¯Ã—â€¯27)</td>
<td>Learns logâ€‘counts (logits)</td>
</tr>
<tr class="odd">
<td><strong>Softmax</strong></td>
<td><code>torch.softmax(logits, dim=1)</code></td>
<td>Turns logits â†’ probabilities</td>
</tr>
<tr class="even">
<td><strong>Negative Logâ€‘Likelihood</strong></td>
<td><code>loss = -log(p_true).mean()</code></td>
<td>Optimisation objective</td>
</tr>
<tr class="odd">
<td><strong>Gradient step</strong></td>
<td><code>W.data -= lr * W.grad</code></td>
<td>Simple SGD update</td>
</tr>
<tr class="even">
<td><strong>L2 regularisation</strong></td>
<td><code>Î» * (W**2).mean()</code></td>
<td>Encourages smoother (more uniform) predictions</td>
</tr>
<tr class="odd">
<td><strong>Sampling loop</strong></td>
<td><code>while idx != 0: idx = torch.multinomial(P[idx], 1)</code></td>
<td>Generates a new name</td>
</tr>
</tbody>
</table>
<hr>
<section id="takeaway" class="level3">
<h3 class="anchored" data-anchor-id="takeaway">ğŸ‰ Takeâ€‘away</h3>
<ul>
<li><strong>Counting bigrams</strong> gives a perfect baseline (NLL â‰ˆâ€¯2.4).<br>
</li>
<li><strong>Training the same model with gradient descent</strong> reproduces the baseline <em>and</em> provides a flexible foundation for more powerful architectures (RNNs, Transformers).<br>
</li>
<li>Understanding <strong>tensor shapes, broadcasting, and autograd</strong> is essential for scaling up.</li>
</ul>
<p><em>Happy modeling!</em> ğŸš€ # Lesson 3</p>
</section>
</section>
</section>
<section id="comprehensive-mindmap-of-the-makemore-lecture" class="level1">
<h1>ğŸ§  Comprehensive Mindâ€‘Map of the â€œMakemoreâ€ Lecture</h1>
<p><em>(All points are derived from the transcript. The hierarchy reflects the logical flow of ideas, concepts, and implementation details.)</em></p>
<hr>
<section id="introduction-recap" class="level2">
<h2 class="anchored" data-anchor-id="introduction-recap">1ï¸âƒ£ Introduction &amp; Recap</h2>
<ul>
<li><strong>Previous lecture</strong>
<ul>
<li>Implemented a <strong>bigram language model</strong>
<ul>
<li>Countâ€‘based version â†’ normalized to probabilities (rows sum toâ€¯1)<br>
</li>
<li>Simple neural net with a <strong>single linear layer</strong><br>
</li>
</ul></li>
<li><strong>Limitation:</strong> only one previous character â†’ poor â€œnameâ€‘likeâ€ predictions</li>
</ul></li>
<li><strong>Problem with extending the count table</strong>
<ul>
<li>Context length <em>k</em> â†’ table size grows <strong>exponentially</strong> (<code>27^k</code> for characters)<br>
</li>
<li>Too many rows â†’ insufficient counts â†’ model â€œexplodesâ€</li>
</ul></li>
</ul>
<hr>
</section>
<section id="moving-to-a-multilayer-perceptron-mlp" class="level2">
<h2 class="anchored" data-anchor-id="moving-to-a-multilayer-perceptron-mlp">2ï¸âƒ£ Moving to a Multiâ€‘Layer Perceptron (MLP)</h2>
<ul>
<li><strong>Goal:</strong> Predict next character using <strong>multiple previous characters</strong> as context.<br>
</li>
<li><strong>Reference paper:</strong> <em>Bengio etâ€¯al., 2003</em> (wordâ€‘level, but ideas transfer).</li>
</ul>
<section id="core-idea-from-the-paper" class="level3">
<h3 class="anchored" data-anchor-id="core-idea-from-the-paper">2.1 Core Idea from the Paper</h3>
<ul>
<li><strong>Word embeddings:</strong> each word â†’ 30â€‘dimensional vector (random init, learned).<br>
</li>
<li><strong>Neural net:</strong>
<ol type="1">
<li><strong>Embedding lookup</strong> â†’ concatenate embeddings of previous <em>n</em> words.<br>
</li>
<li><strong>Hidden layer</strong> (size = hyperâ€‘parameter).<br>
</li>
<li><strong>Linear output layer</strong> â†’ logits for all possible next tokens.<br>
</li>
<li><strong>Softmax</strong> â†’ probability distribution.<br>
</li>
</ol></li>
<li><strong>Training objective:</strong> Maximize logâ€‘likelihood (same as crossâ€‘entropy).</li>
</ul>
</section>
<section id="adapting-to-characters" class="level3">
<h3 class="anchored" data-anchor-id="adapting-to-characters">2.2 Adapting to Characters</h3>
<ul>
<li>Vocabulary = <strong>27 characters</strong> (aâ€‘z + â€œ.â€).<br>
</li>
<li>Embedding dimension initially <strong>2</strong> (for easy visualization).<br>
</li>
<li>Context length (block size) = <strong>3</strong> characters (can be changed).</li>
</ul>
<hr>
</section>
</section>
<section id="implementation-details-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="implementation-details-pytorch">3ï¸âƒ£ Implementation Details (PyTorch)</h2>
<section id="data-preparation-1" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation-1">3.1 Data Preparation</h3>
<ul>
<li><strong>Build dataset</strong> (<code>x</code>, <code>y</code>):
<ul>
<li><code>x</code> = list of <em>blockâ€‘size</em> integer indices (context).<br>
</li>
<li><code>y</code> = integer index of the next character.<br>
</li>
<li>Pad with zeros for the first <em>blockâ€‘size</em> positions.<br>
</li>
</ul></li>
<li>Example (word â€œemmaâ€):
<ul>
<li>Context <code>[0,0, e] â†’ label m</code>, <code>[0, e, m] â†’ label m</code>, â€¦</li>
</ul></li>
</ul>
</section>
<section id="embedding-lookup-c" class="level3">
<h3 class="anchored" data-anchor-id="embedding-lookup-c">3.2 Embedding Lookup (<code>C</code>)</h3>
<ul>
<li><code>C</code> shape: <strong>27 Ã— 2</strong> (rows = characters, cols = embedding dim).<br>
</li>
<li>Two equivalent ways to embed an index <code>i</code>:
<ol type="1">
<li>Direct indexing: <code>C[i]</code>.<br>
</li>
<li>Oneâ€‘hot â†’ matrix multiplication: <code>one_hot(i) @ C</code>.<br>
</li>
</ol></li>
<li>For a batch <code>x</code> (shape <code>B Ã— 3</code>): <code>C[x]</code> â†’ <strong>B Ã— 3 Ã— 2</strong> tensor.</li>
</ul>
</section>
<section id="flattening-the-context" class="level3">
<h3 class="anchored" data-anchor-id="flattening-the-context">3.3 Flattening the Context</h3>
<ul>
<li>Need shape <strong>B Ã— (3â€¯Ã—â€¯2) = B Ã— 6</strong> to feed the hidden layer.<br>
</li>
<li><strong>Methods:</strong>
<ul>
<li><code>torch.cat([c0, c1, c2], dim=1)</code> (naÃ¯ve, not generic).<br>
</li>
<li><code>torch.unbind(x, dim=1)</code> â†’ tuple of tensors â†’ <code>torch.cat(..., dim=1)</code>.<br>
</li>
<li><strong>Best:</strong> <code>C[x].view(B, -1)</code> (uses <code>view</code> â†’ no extra memory).</li>
</ul></li>
</ul>
</section>
<section id="hidden-layer" class="level3">
<h3 class="anchored" data-anchor-id="hidden-layer">3.4 Hidden Layer</h3>
<ul>
<li>Weight matrix <code>W1</code>: <strong>6 Ã— H</strong> (H = hidden size, e.g., 100).<br>
</li>
<li>Bias <code>b1</code>: <strong>H</strong>.<br>
</li>
<li>Activation: <strong>tanh</strong> (<code>torch.tanh</code>).</li>
</ul>
</section>
<section id="output-layer" class="level3">
<h3 class="anchored" data-anchor-id="output-layer">3.5 Output Layer</h3>
<ul>
<li>Weight matrix <code>W2</code>: <strong>H Ã— 27</strong>.<br>
</li>
<li>Bias <code>b2</code>: <strong>27</strong>.<br>
</li>
<li>Logits: <code>h @ W2 + b2</code> â†’ shape <strong>B Ã— 27</strong>.</li>
</ul>
</section>
<section id="loss-computation" class="level3">
<h3 class="anchored" data-anchor-id="loss-computation">3.6 Loss Computation</h3>
<ul>
<li><strong>Manual:</strong>
<ul>
<li><code>logits.exp()</code> â†’ â€œcountsâ€.<br>
</li>
<li>Normalize â†’ probabilities.<br>
</li>
<li>Pick probability of true class â†’ <code>-log(p_true)</code>.<br>
</li>
</ul></li>
<li><strong>Preferred:</strong> <code>torch.nn.functional.cross_entropy(logits, y)</code>
<ul>
<li>Faster (fused kernels).<br>
</li>
<li>Numerically stable (logâ€‘softmax internally).</li>
</ul></li>
</ul>
</section>
<section id="training-loop-core-steps" class="level3">
<h3 class="anchored" data-anchor-id="training-loop-core-steps">3.7 Training Loop (Core Steps)</h3>
<ol type="1">
<li>Zero grads: <code>p.grad = None</code> for each parameter.<br>
</li>
<li>Forward pass â†’ loss.<br>
</li>
<li><code>loss.backward()</code> â†’ gradients.<br>
</li>
<li>Parameter update: <code>p -= lr * p.grad</code>.</li>
</ol>
</section>
<section id="minibatch-training" class="level3">
<h3 class="anchored" data-anchor-id="minibatch-training">3.8 Miniâ€‘Batch Training</h3>
<ul>
<li><strong>Why:</strong> Full dataset (~228â€¯k examples) â†’ too slow.<br>
</li>
<li><strong>How:</strong>
<ul>
<li>Sample random indices <code>ix = torch.randint(0, N, (batch_size,))</code>.<br>
</li>
<li>Use <code>x[ix]</code>, <code>y[ix]</code> for each iteration.<br>
</li>
</ul></li>
<li><strong>Effect:</strong> Noisy gradient â†’ need more steps, but far faster.</li>
</ul>
<hr>
</section>
</section>
<section id="hyperparameter-exploration" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-exploration">4ï¸âƒ£ Hyperâ€‘Parameter Exploration</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 25%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Hyperâ€‘parameter</th>
<th>Description</th>
<th>Typical Values (used)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>block_size</code></td>
<td>Number of previous characters</td>
<td>3 (tried 4,â€¯5,â€¯10)</td>
</tr>
<tr class="even">
<td><code>embed_dim</code></td>
<td>Dimensionality of character embeddings</td>
<td>2 (visual), 10 (better)</td>
</tr>
<tr class="odd">
<td><code>hidden_size</code></td>
<td>Neurons in hidden layer</td>
<td>100 â†’ 200 â†’ 300</td>
</tr>
<tr class="even">
<td><code>lr</code> (learning rate)</td>
<td>Step size for SGD</td>
<td>0.1 (good), 0.01 (fineâ€‘tune), 0.001 (slow)</td>
</tr>
<tr class="odd">
<td><code>batch_size</code></td>
<td>Miniâ€‘batch size</td>
<td>32 (default), can increase</td>
</tr>
<tr class="even">
<td><code>num_steps</code></td>
<td>Training iterations</td>
<td>10â€¯k â†’ 200â€¯k (long runs)</td>
</tr>
<tr class="odd">
<td><code>lr_decay</code></td>
<td>Reduce LR after N steps</td>
<td>Ã—0.1 after 100â€¯k steps</td>
</tr>
</tbody>
</table>
<section id="learningrate-search-practical-trick" class="level3">
<h3 class="anchored" data-anchor-id="learningrate-search-practical-trick">4.1 Learningâ€‘Rate Search (Practical Trick)</h3>
<ul>
<li>Sweep <strong>logâ€‘space</strong>: <code>lr_exps = torch.linspace(-3, 0, steps=1000)</code> â†’ <code>lrs = 10**lr_exps</code>.<br>
</li>
<li>Run a few steps for each LR, record loss â†’ plot <strong>LR vs.&nbsp;loss</strong>.<br>
</li>
<li>Choose LR in the â€œvalleyâ€ (e.g., <code>10â»Â¹ = 0.1</code>).</li>
</ul>
</section>
<section id="overunderfitting-diagnosis" class="level3">
<h3 class="anchored" data-anchor-id="overunderfitting-diagnosis">4.2 Overâ€‘/Underâ€‘Fitting Diagnosis</h3>
<ul>
<li><strong>Training loss â‰ˆ validation loss</strong> â†’ <strong>underâ€‘fitting</strong> (model too small).<br>
</li>
<li><strong>Training loss &lt;&lt; validation loss</strong> â†’ <strong>overâ€‘fitting</strong> (model too large).<br>
</li>
<li>Adjust hidden size, embed dim, or regularization accordingly.</li>
</ul>
<hr>
</section>
</section>
<section id="data-splits-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="data-splits-evaluation">5ï¸âƒ£ Data Splits &amp; Evaluation</h2>
<ol type="1">
<li><strong>Training set</strong> â€“ ~80â€¯% of words (â‰ˆâ€¯25â€¯k examples).<br>
</li>
<li><strong>Dev/validation set</strong> â€“ ~10â€¯% (â‰ˆâ€¯3â€¯k examples).<br>
</li>
<li><strong>Test set</strong> â€“ remaining ~10â€¯% (â‰ˆâ€¯2â€¯k examples).</li>
</ol>
<ul>
<li><strong>Training</strong> uses only the training split.<br>
</li>
<li><strong>Hyperâ€‘parameter tuning</strong> uses the dev set.<br>
</li>
<li><strong>Final performance</strong> reported on the test set <strong>once</strong>.</li>
</ul>
<hr>
</section>
<section id="embedding-visualization-2d-case" class="level2">
<h2 class="anchored" data-anchor-id="embedding-visualization-2d-case">6ï¸âƒ£ Embedding Visualization (2â€‘D case)</h2>
<ul>
<li>After training with <code>embed_dim = 2</code>, plot each character:
<ul>
<li><code>x = C[:,0]</code>, <code>y = C[:,1]</code>.<br>
</li>
<li>Annotate with the character symbol.<br>
</li>
</ul></li>
<li>Observations:
<ul>
<li>Vowels cluster together â†’ network learns similarity.<br>
</li>
<li>Rare symbols (e.g., â€œqâ€, â€œ.â€) occupy distinct regions.</li>
</ul></li>
</ul>
<p><em>When <code>embed_dim</code> &gt;â€¯2, direct 2â€‘D plot isnâ€™t possible; consider PCA/tâ€‘SNE.</em></p>
<hr>
</section>
<section id="sampling-from-the-trained-model" class="level2">
<h2 class="anchored" data-anchor-id="sampling-from-the-trained-model">7ï¸âƒ£ Sampling from the Trained Model</h2>
<ol type="1">
<li><strong>Initialize context</strong> with three â€œ.â€ (or any start token).<br>
</li>
<li>Loop:
<ul>
<li>Embed current context â†’ hidden state â†’ logits.<br>
</li>
<li><code>prob = torch.softmax(logits, dim=-1)</code>.<br>
</li>
<li>Sample next token: <code>next_idx = torch.multinomial(prob, 1)</code>.<br>
</li>
<li>Shift context window, append <code>next_idx</code>.<br>
</li>
</ul></li>
<li>Convert indices back to characters â†’ generated string.</li>
</ol>
<ul>
<li>Generated examples look <strong>more nameâ€‘like</strong> (e.g., â€œham joesâ€, â€œemilyâ€).</li>
</ul>
<hr>
</section>
<section id="practical-tips-extras" class="level2">
<h2 class="anchored" data-anchor-id="practical-tips-extras">8ï¸âƒ£ Practical Tips &amp; Extras</h2>
<ul>
<li><strong>Tensor indexing tricks</strong> (list, 1â€‘D tensor, multiâ€‘dim tensor) â†’ <code>C[x]</code>.<br>
</li>
<li><strong><code>view</code> vs.&nbsp;<code>reshape</code></strong> â€“ <code>view</code> is a <em>noâ€‘copy</em> operation (fast).<br>
</li>
<li><strong>Broadcasting</strong> â€“ Adding bias <code>b1</code> to hidden activations works automatically (<code>BÃ—H</code> + <code>H</code>).<br>
</li>
<li><strong>Avoid hardâ€‘coding</strong> magic numbers; use variables (<code>block_size</code>, <code>embed_dim</code>).<br>
</li>
<li><strong>Googleâ€¯Colab</strong> â€“ Readyâ€‘toâ€‘run notebook, no local install needed (link provided in video).</li>
</ul>
<hr>
</section>
<section id="takeaways-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="takeaways-next-steps">9ï¸âƒ£ Takeâ€‘aways &amp; Next Steps</h2>
<ul>
<li><strong>Achieved</strong>: Loss â‰ˆâ€¯2.17 (better than bigram â‰ˆâ€¯2.45).<br>
</li>
<li><strong>Open knobs for improvement</strong>:
<ul>
<li>Increase hidden size / embedding dimension.<br>
</li>
<li>Use longer context (<code>block_size</code>).<br>
</li>
<li>Experiment with different optimizers (Adam, RMSprop).<br>
</li>
<li>Add regularization (weight decay, dropout).<br>
</li>
<li>Train longer with proper learningâ€‘rate schedule.<br>
</li>
</ul></li>
<li><strong>Read the paper</strong> (Bengioâ€¯etâ€¯al., 2003) for deeper insights &amp; advanced ideas.</li>
</ul>
<hr>
<section id="quick-reference-pseudocode" class="level3">
<h3 class="anchored" data-anchor-id="quick-reference-pseudocode">ğŸ“Œ Quick Reference (Pseudoâ€‘code)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Build dataset</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> build_dataset(words, block_size<span class="op">=</span><span class="dv">3</span>)   <span class="co"># x: (N,3), y: (N,)</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Model components</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>C   <span class="op">=</span> torch.randn(<span class="dv">27</span>, embed_dim, requires_grad<span class="op">=</span><span class="va">True</span>)   <span class="co"># embedding table</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>W1  <span class="op">=</span> torch.randn(<span class="dv">3</span><span class="op">*</span>embed_dim, hidden, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>b1  <span class="op">=</span> torch.randn(hidden, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>W2  <span class="op">=</span> torch.randn(hidden, <span class="dv">27</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>b2  <span class="op">=</span> torch.randn(<span class="dv">27</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Forward pass (batch)</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x_batch):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> C[x_batch]                <span class="co"># (B,3,embed_dim)</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> e.view(e.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)    <span class="co"># (B,3*embed_dim)</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(e <span class="op">@</span> W1 <span class="op">+</span> b1)   <span class="co"># (B,hidden)</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2          <span class="co"># (B,27)</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Training loop (miniâ€‘batch)</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, N, (batch_size,))</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> forward(x[ix])</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    loss   <span class="op">=</span> F.cross_entropy(logits, y[ix])</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> [C,W1,b1,W2,b2]:</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">-=</span> lr <span class="op">*</span> p.grad</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
<p><em>End of mindâ€‘map.</em><br>
# Lesson 4</p>
</section>
</section>
</section>
<section id="mindmap-of-the-lecture" class="level1">
<h1>ğŸ§  Mindâ€‘Map of the Lecture</h1>
<p><em>(All points are derived from the transcript. The hierarchy shows the logical flow and relationships between concepts.)</em></p>
<hr>
<section id="recap-multilayer-perceptron-mlp-for-characterlevel-language-modelling" class="level2">
<h2 class="anchored" data-anchor-id="recap-multilayer-perceptron-mlp-for-characterlevel-language-modelling">1ï¸âƒ£ Recap: Multiâ€‘Layer Perceptron (MLP) for characterâ€‘level language modelling</h2>
<ul>
<li>Implemented following <strong>Benj 2003</strong> (MLP â†’ nextâ€‘character prediction).<br>
</li>
<li><strong>Current status</strong>
<ul>
<li>11â€¯k parameters, 200â€¯k training steps, batchâ€‘sizeâ€¯=â€¯32.<br>
</li>
<li>Training/validation loss â‰ˆâ€¯2.16.<br>
</li>
<li>Sampling produces readable but imperfect words.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="why-look-deeper" class="level2">
<h2 class="anchored" data-anchor-id="why-look-deeper">2ï¸âƒ£ Why look deeper?</h2>
<ul>
<li>Goal: move to <strong>recurrent / LSTM / GRU</strong> networks.<br>
</li>
<li><strong>Prerequisite:</strong> solid intuition of <strong>activations</strong> &amp; <strong>gradients</strong> during training.<br>
</li>
<li>Understanding these dynamics explains why RNNs are <strong>hard to optimise</strong> with plain firstâ€‘order methods.</li>
</ul>
<hr>
</section>
<section id="problem-1-bad-initialisation-of-the-mlp" class="level2">
<h2 class="anchored" data-anchor-id="problem-1-bad-initialisation-of-the-mlp">3ï¸âƒ£ Problem #1 â€“ Bad Initialisation of the MLP</h2>
<section id="observed-symptom" class="level3">
<h3 class="anchored" data-anchor-id="observed-symptom">3.1 Observed symptom</h3>
<ul>
<li><strong>Loss at iterationâ€¯0:</strong>â€¯27â€¯â†’â€¯much higher than expected.</li>
</ul>
</section>
<section id="expected-loss-for-a-uniform-softmax" class="level3">
<h3 class="anchored" data-anchor-id="expected-loss-for-a-uniform-softmax">3.2 Expected loss for a uniform softmax</h3>
<ul>
<li>27 possible next characters â†’ uniform probability = 1/27.<br>
</li>
<li>Negativeâ€‘logâ€‘likelihood = <code>-log(1/27) â‰ˆ 3.29</code>.</li>
</ul>
</section>
<section id="what-went-wrong" class="level3">
<h3 class="anchored" data-anchor-id="what-went-wrong">3.3 What went wrong?</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 42%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th>Issue</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Logits far from 0</strong> (extreme values)</td>
<td>Softmax becomes <strong>overâ€‘confident</strong> â†’ huge loss.</td>
</tr>
<tr class="even">
<td><strong>Random bias <code>bâ‚‚</code></strong></td>
<td>Adds a constant offset â†’ pushes logits away from 0.</td>
</tr>
<tr class="odd">
<td><strong>Weight scale too large</strong> (<code>Wâ‚‚</code>)</td>
<td>Amplifies the offset, further saturating softmax.</td>
</tr>
</tbody>
</table>
</section>
<section id="fixes-applied" class="level3">
<h3 class="anchored" data-anchor-id="fixes-applied">3.4 Fixes applied</h3>
<ol type="1">
<li><strong>Zero the output bias</strong> (<code>bâ‚‚ = 0</code>).<br>
</li>
<li><strong>Scale down <code>Wâ‚‚</code></strong> (multiply by 0.1 â†’ 0.01).<br>
</li>
<li>Keep a tiny nonâ€‘zero variance (e.g., 0.01) for <strong>symmetry breaking</strong>.</li>
</ol>
<p>Result: loss curve loses the â€œhockeyâ€‘stickâ€ shape; training becomes more productive.</p>
<hr>
</section>
</section>
<section id="problem-2-saturated-tanh-ğ‘¡ğ‘ğ‘›â„-activations" class="level2">
<h2 class="anchored" data-anchor-id="problem-2-saturated-tanh-ğ‘¡ğ‘ğ‘›â„-activations">4ï¸âƒ£ Problem #2 â€“ Saturatedâ€¯<code>tanh</code> (ğ‘¡ğ‘ğ‘›â„) activations</h2>
<section id="observation" class="level3">
<h3 class="anchored" data-anchor-id="observation">4.1 Observation</h3>
<ul>
<li>Histogram of hiddenâ€‘state <code>H</code> after <code>tanh</code> shows <strong>most values at Â±1</strong>.<br>
</li>
<li>Preâ€‘activations (input to <code>tanh</code>) range roughly <strong>â€‘5 â€¦ 15</strong> â†’ many neurons in the <strong>flat tails</strong>.</li>
</ul>
</section>
<section id="consequence-for-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="consequence-for-backpropagation">4.2 Consequence for backâ€‘propagation</h3>
<ul>
<li>Derivative of <code>tanh</code> = <code>1 â€“ tÂ²</code>.<br>
</li>
<li>When <code>t â‰ˆ Â±1</code>, derivative â‰ˆâ€¯0 â†’ <strong>gradient vanishes</strong> for those neurons.<br>
</li>
<li>â€œDead neuronsâ€ (always saturated) never learn (gradient = 0).</li>
</ul>
</section>
<section id="diagnostic-check" class="level3">
<h3 class="anchored" data-anchor-id="diagnostic-check">4.3 Diagnostic check</h3>
<ul>
<li>Compute <strong>percentage of units with |t|â€¯&gt;â€¯0.99</strong> â†’ large white area in Boolean mask â†’ many dead neurons.</li>
</ul>
</section>
<section id="remedy" class="level3">
<h3 class="anchored" data-anchor-id="remedy">4.4 Remedy</h3>
<ul>
<li>Reduce magnitude of preâ€‘activations:
<ul>
<li><strong>Scale down the firstâ€‘layer weights</strong> (<code>Wâ‚</code>) (e.g., multiply by 0.1).<br>
</li>
<li>Optionally <strong>bias = 0</strong> (biases become useless after batchâ€‘norm, see Â§6).<br>
</li>
</ul></li>
<li>Result: hidden activations become <strong>roughly Gaussian (â‰ˆâ€¯ğ’©(0,1))</strong>, gradients stay alive.</li>
</ul>
<hr>
</section>
</section>
<section id="general-weightinitialisation-theory" class="level2">
<h2 class="anchored" data-anchor-id="general-weightinitialisation-theory">5ï¸âƒ£ General Weightâ€‘Initialisation Theory</h2>
<section id="fanin-fanout-concept" class="level3">
<h3 class="anchored" data-anchor-id="fanin-fanout-concept">5.1 Fanâ€‘in / Fanâ€‘out concept</h3>
<ul>
<li>For a layer with <code>fan_in</code> inputs, initialise weights with variance <code>1 / fan_in</code>.<br>
</li>
<li>Guarantees that <strong>output variance â‰ˆâ€¯input variance</strong> (preserves a unitâ€‘Gaussian flow).</li>
</ul>
</section>
<section id="gains-for-different-nonlinearities-he-xavier" class="level3">
<h3 class="anchored" data-anchor-id="gains-for-different-nonlinearities-he-xavier">5.2 Gains for different nonâ€‘linearities (He / Xavier)</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Nonâ€‘linearity</th>
<th>Recommended gain <code>g</code></th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear / Identity</td>
<td><code>1</code></td>
<td>No contraction.</td>
</tr>
<tr class="even">
<td>ReLU / Leakyâ€‘ReLU</td>
<td><code>âˆš2</code></td>
<td>Half the distribution is zeroed.</td>
</tr>
<tr class="odd">
<td>tanh</td>
<td><code>5/3</code> (â‰ˆâ€¯1.67)</td>
<td>Empirically balances contraction of tanh.</td>
</tr>
<tr class="even">
<td>(others)</td>
<td>derived from variance analysis</td>
<td>â€“</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Final weight scale</strong> = <code>gain / âˆšfan_in</code>.</li>
</ul>
<hr>
</section>
</section>
<section id="batch-normalisation-batchnorm-the-gamechanger" class="level2">
<h2 class="anchored" data-anchor-id="batch-normalisation-batchnorm-the-gamechanger">6ï¸âƒ£ Batch Normalisation (BatchNorm) â€“ The â€œGameâ€‘Changerâ€</h2>
<section id="core-idea" class="level3">
<h3 class="anchored" data-anchor-id="core-idea">6.1 Core idea</h3>
<ol type="1">
<li><strong>Collect batch statistics</strong> â†’ mean <code>Î¼_B</code> and variance <code>ÏƒÂ²_B</code>.<br>
</li>
<li><strong>Standardise</strong>: <code>xÌ‚ = (x â€“ Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)</code>.<br>
</li>
<li><strong>Learnable affine transform</strong>: <code>y = Î³Â·xÌ‚ + Î²</code>.</li>
</ol>
</section>
<section id="why-it-works" class="level3">
<h3 class="anchored" data-anchor-id="why-it-works">6.2 Why it works</h3>
<ul>
<li>Forces <strong>activations to stay unitâ€‘Gaussian</strong> throughout the network â†’ prevents saturation / vanishing gradients.<br>
</li>
<li>Acts as a <strong>regulariser</strong> (batchâ€‘wise noise).</li>
</ul>
</section>
<section id="training-vs.-inference" class="level3">
<h3 class="anchored" data-anchor-id="training-vs.-inference">6.3 Training vs.&nbsp;Inference</h3>
<ul>
<li><p><strong>Training:</strong> use batch statistics (<code>Î¼_B</code>, <code>ÏƒÂ²_B</code>).<br>
</p></li>
<li><p><strong>Inference:</strong> use <strong>running estimates</strong> (<code>Î¼Ì‚</code>, <code>ÏƒÌ‚Â²</code>) updated with exponential moving average:</p>
<p><code>running_mean = momentumÂ·running_mean + (1â€‘momentum)Â·Î¼_B</code></p>
<p><code>running_var  = momentumÂ·running_var  + (1â€‘momentum)Â·ÏƒÂ²_B</code></p></li>
<li><p><code>momentum</code> â‰ˆâ€¯0.1 for large batches; smaller batches may need <strong>lower momentum</strong> (e.g.,â€¯0.001).</p></li>
</ul>
</section>
<section id="practical-notes" class="level3">
<h3 class="anchored" data-anchor-id="practical-notes">6.4 Practical notes</h3>
<ul>
<li><strong>Bias before BatchNorm is useless</strong> â€“ BatchNorm already learns a bias (<code>Î²</code>).<br>
</li>
<li>Set <code>affine=True</code> (learnable <code>Î³</code>, <code>Î²</code>).<br>
</li>
<li><code>eps</code> (defaultâ€¯1eâ€‘5) avoids divisionâ€‘byâ€‘zero.<br>
</li>
<li><strong>No gradient tracking</strong> for running stats (<code>torch.no_grad()</code> context).</li>
</ul>
<hr>
</section>
</section>
<section id="diagnostic-toolbox-for-neuralnet-health" class="level2">
<h2 class="anchored" data-anchor-id="diagnostic-toolbox-for-neuralnet-health">7ï¸âƒ£ Diagnostic Toolbox for Neuralâ€‘Net Health</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 36%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Diagnostic</th>
<th>What it tells you</th>
<th>Typical â€œgoodâ€ range</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Activation histogram</strong> (per layer)</td>
<td>Distribution shape, saturation %</td>
<td>Stdâ€¯â‰ˆâ€¯1, saturationâ€¯&lt;â€¯5â€¯%</td>
</tr>
<tr class="even">
<td><strong>Gradient histogram</strong> (per layer)</td>
<td>Gradient magnitude, vanishing/exploding</td>
<td>Similar scale to activations</td>
</tr>
<tr class="odd">
<td><strong>Weight histogram</strong></td>
<td>Parameter spread, dead weights</td>
<td>Stdâ€¯â‰ˆâ€¯1 (or as per init)</td>
</tr>
<tr class="even">
<td><strong>Updateâ€‘toâ€‘Data ratio</strong> <code>â€–Î”Î¸â€– / â€–Î¸â€–</code> (logâ‚â‚€)</td>
<td>Relative step size per iteration</td>
<td>â‰ˆâ€¯â€‘3 (i.e., updates â‰ˆâ€¯0.001â€¯Ã—â€¯parameter)</td>
</tr>
<tr class="odd">
<td><strong>Learningâ€‘rate sanity check</strong></td>
<td>If ratio â‰ªâ€¯â€‘3 â†’ LR too low; â‰«â€¯â€‘2 â†’ LR too high</td>
<td>Aim for â€“3â€¯Â±â€¯0.5</td>
</tr>
<tr class="even">
<td><strong>Runningâ€‘mean / var convergence</strong></td>
<td>BatchNorm stats stabilise?</td>
<td>Small drift after a few epochs</td>
</tr>
</tbody>
</table>
<ul>
<li>Plot these <strong>over training time</strong> (not just a single snapshot) to see trends.</li>
</ul>
<hr>
</section>
<section id="putting-it-all-together-torchify-the-code" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together-torchify-the-code">8ï¸âƒ£ Putting It All Together â€“ â€œTorchâ€‘ifyâ€ the Code</h2>
<section id="modular-design-mirrors-torch.nn" class="level3">
<h3 class="anchored" data-anchor-id="modular-design-mirrors-torch.nn">8.1 Modular design (mirrors <code>torch.nn</code>)</h3>
<ul>
<li><strong>Linear layer</strong> â†’ <code>nn.Linear(in_features, out_features, bias=False)</code> (bias omitted when followed by BatchNorm).<br>
</li>
<li><strong>BatchNorm1d</strong> â†’ <code>nn.BatchNorm1d(num_features, eps=1eâ€‘5, momentum=0.001, affine=True)</code>.<br>
</li>
<li><strong>tanh activation</strong> â†’ custom wrapper (or <code>nn.Tanh</code>).</li>
</ul>
</section>
<section id="network-construction-pattern" class="level3">
<h3 class="anchored" data-anchor-id="network-construction-pattern">8.2 Network construction pattern</h3>
<pre class="text"><code>Embedding â†’ Linear â†’ BatchNorm â†’ tanh â†’ Linear â†’ BatchNorm â†’ tanh â†’ â€¦ â†’ Linear â†’ BatchNorm â†’ Softmax</code></pre>
<ul>
<li><strong>BatchNorm placed after each Linear, before tanh</strong> (standard practice).<br>
</li>
<li>Can also be placed after tanh â€“ results are similar.</li>
</ul>
</section>
<section id="training-loop-highlevel" class="level3">
<h3 class="anchored" data-anchor-id="training-loop-highlevel">8.3 Training loop (highâ€‘level)</h3>
<ol type="1">
<li><strong>Zero grads</strong>.<br>
</li>
<li><strong>Forward pass</strong> (collect activations, apply BatchNorm).<br>
</li>
<li><strong>Compute loss</strong> (crossâ€‘entropy).<br>
</li>
<li><strong>Backward</strong> (<code>loss.backward()</code>).<br>
</li>
<li><strong>Optimizer step</strong> (SGD / Adam).<br>
</li>
<li><strong>Update running stats</strong> (handled automatically by <code>nn.BatchNorm</code>).</li>
</ol>
<hr>
</section>
</section>
<section id="takeaways-outlook" class="level2">
<h2 class="anchored" data-anchor-id="takeaways-outlook">9ï¸âƒ£ Takeâ€‘aways &amp; Outlook</h2>
<ol type="1">
<li><strong>Initialisation matters</strong> â€“ scaling weights &amp; zeroing biases prevents early â€œoverâ€‘confidenceâ€.<br>
</li>
<li><strong>tanh saturation kills gradients</strong> â€“ keep preâ€‘activations near zero (via weight scaling).<br>
</li>
<li><strong>BatchNorm stabilises deep nets</strong> by constantly reâ€‘Gaussianising activations; it also reduces sensitivity to exact gain choices.<br>
</li>
<li><strong>Diagnostic visualisations</strong> (histograms, updateâ€‘toâ€‘data ratios) are essential for spotting dead neurons, exploding/vanishing gradients, and misâ€‘scaled learning rates.<br>
</li>
<li><strong>Future work</strong>
<ul>
<li>Move to <strong>recurrent architectures</strong> (RNN, LSTM, GRU) â€“ deeper unrolled graphs will amplify the issues we just mitigated.<br>
</li>
<li>Explore <strong>alternative normalisation</strong> (LayerNorm, GroupNorm) that avoid batch coupling.<br>
</li>
<li>Leverage <strong>advanced optimisers</strong> (Adam, RMSProp) and <strong>residual connections</strong> for even deeper models.</li>
</ul></li>
</ol>
<hr>
<section id="quickreference-cheatsheet" class="level3">
<h3 class="anchored" data-anchor-id="quickreference-cheatsheet">ğŸ“Œ Quickâ€‘Reference Cheatâ€‘Sheet</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 44%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula / Setting</th>
<th>Typical Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weight init variance</td>
<td><code>var = gainÂ² / fan_in</code></td>
<td><code>gain = 1</code> (linear), <code>âˆš2</code> (ReLU), <code>5/3</code> (tanh)</td>
</tr>
<tr class="even">
<td>Softmax uniform loss</td>
<td><code>-log(1/ğ‘˜)</code></td>
<td><code>k = vocab size</code></td>
</tr>
<tr class="odd">
<td>tanh derivative</td>
<td><code>1 â€“ tÂ²</code></td>
<td>â†’ 0 when <code>|t| â†’ 1</code></td>
</tr>
<tr class="even">
<td>BatchNorm scaling</td>
<td><code>Î³</code> (learned)</td>
<td>Initialise to <code>1</code></td>
</tr>
<tr class="odd">
<td>BatchNorm shift</td>
<td><code>Î²</code> (learned)</td>
<td>Initialise to <code>0</code></td>
</tr>
<tr class="even">
<td>Updateâ€‘toâ€‘Data logâ‚â‚€ target</td>
<td><code>â‰ˆâ€¯â€‘3</code></td>
<td>Adjust LR accordingly</td>
</tr>
<tr class="odd">
<td>Momentum for running stats</td>
<td><code>0.1</code> (large batch) / <code>0.001</code> (batchâ€¯=â€¯32)</td>
<td>â€“</td>
</tr>
<tr class="even">
<td>Îµ (epsilon) in BN</td>
<td><code>1eâ€‘5</code></td>
<td>â€“</td>
</tr>
</tbody>
</table>
<hr>
<p><em>End of mindâ€‘map.</em><br>
# Lesson 5</p>
</section>
</section>
</section>
<section id="mindmap-manual-backpropagation-for-a-twolayer-mlp" class="level1">
<h1>ğŸ§  Mindmap â€“ Manual Backâ€‘Propagation for a Twoâ€‘Layer MLP</h1>
<hr>
<section id="overview-1" class="level2">
<h2 class="anchored" data-anchor-id="overview-1">1ï¸âƒ£ Overview</h2>
<ul>
<li><strong>Goal:</strong> Replace <code>loss.backward()</code> with a fully manual backward pass (tensorâ€‘level).<br>
</li>
<li><strong>Why?</strong>
<ul>
<li>Understand the internals of autograd.<br>
</li>
<li>Debug subtle bugs (gradient clipping, dead neurons, exploding/vanishing gradients).<br>
</li>
<li>Gain intuition about how gradients flow through each operation.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="historical-context" class="level2">
<h2 class="anchored" data-anchor-id="historical-context">2ï¸âƒ£ Historical Context</h2>
<ul>
<li><strong>~2006 â€“ 2010:</strong>
<ul>
<li>Researchers (e.g., Hinton &amp; Salakhutdinov) wrote their own backâ€‘prop in MATLAB/NumPy.<br>
</li>
<li>Manual gradient computation was the norm.<br>
</li>
</ul></li>
<li><strong>2014:</strong>
<ul>
<li>Authorâ€™s â€œFragmented Embeddingsâ€ paper â€“ full manual forwardâ€¯+â€¯backward passes in NumPy.<br>
</li>
</ul></li>
<li><strong>Today:</strong>
<ul>
<li>Autograd is standard, but the exercise remains valuable for learning.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="network-architecture-forward-pass" class="level2">
<h2 class="anchored" data-anchor-id="network-architecture-forward-pass">3ï¸âƒ£ Network Architecture (forward pass)</h2>
<pre><code>Embedding â†’ Linear1 (W1, B1) â†’ 10â€‘H (tanh) â†’ BatchNorm (Î³, Î²) â†’ Linear2 (W2, B2) â†’ Logits â†’ Softmax â†’ Crossâ€‘Entropy</code></pre>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Layer</th>
<th>Shape (batchâ€¯=â€¯32)</th>
<th>Key tensors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Embedding</strong></td>
<td>32â€¯Ã—â€¯3â€¯Ã—â€¯10</td>
<td><code>C</code> (27â€¯Ã—â€¯10)</td>
</tr>
<tr class="even">
<td><strong>Linear1</strong></td>
<td>32â€¯Ã—â€¯64</td>
<td><code>W1</code> (64â€¯Ã—â€¯64), <code>B1</code> (1â€¯Ã—â€¯64)</td>
</tr>
<tr class="odd">
<td><strong>10â€‘H</strong></td>
<td>32â€¯Ã—â€¯64</td>
<td><code>H = tanh(preact)</code></td>
</tr>
<tr class="even">
<td><strong>BatchNorm</strong></td>
<td>32â€¯Ã—â€¯64</td>
<td><code>Î¼</code> (1â€¯Ã—â€¯64), <code>ÏƒÂ²</code> (1â€¯Ã—â€¯64), <code>Î³</code>, <code>Î²</code></td>
</tr>
<tr class="odd">
<td><strong>Linear2</strong></td>
<td>32â€¯Ã—â€¯27</td>
<td><code>W2</code> (27â€¯Ã—â€¯64), <code>B2</code> (1â€¯Ã—â€¯27)</td>
</tr>
<tr class="even">
<td><strong>Logits</strong></td>
<td>32â€¯Ã—â€¯27</td>
<td><code>logits</code></td>
</tr>
<tr class="odd">
<td><strong>Softmax</strong></td>
<td>32â€¯Ã—â€¯27</td>
<td><code>probs</code></td>
</tr>
<tr class="even">
<td><strong>Loss</strong></td>
<td>scalar</td>
<td><code>loss = -mean(log_probs[range(N), Y])</code></td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="manual-backpropagation-core-concepts" class="level2">
<h2 class="anchored" data-anchor-id="manual-backpropagation-core-concepts">4ï¸âƒ£ Manual Backâ€‘Propagation â€“ Core Concepts</h2>
<section id="gradient-of-the-loss-w.r.t.-log_probs-d_log_probs" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-w.r.t.-log_probs-d_log_probs">4.1 Gradient of the Loss w.r.t. <code>log_probs</code> (<code>d_log_probs</code>)</h3>
<ul>
<li><p><strong>Shape:</strong> 32â€¯Ã—â€¯27 (same as <code>log_probs</code>).<br>
</p></li>
<li><p><strong>Derivation:</strong></p>
<ul>
<li><code>loss = -(1/N) Î£_i log_probs[i, Y[i]]</code><br>
</li>
<li><code>âˆ‚loss/âˆ‚log_probs[i, j] = -1/N</code> if <code>j == Y[i]</code>, else <code>0</code>.<br>
</li>
</ul></li>
<li><p><strong>Implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>d_log_probs <span class="op">=</span> torch.zeros_like(log_probs)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>d_log_probs[torch.arange(N), Y] <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> N</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
</section>
<section id="backprop-through-log-probs" class="level3">
<h3 class="anchored" data-anchor-id="backprop-through-log-probs">4.2 Backâ€‘prop through <code>log</code> â†’ <code>probs</code></h3>
<ul>
<li><code>log_probs = torch.log(probs)</code><br>
</li>
<li>Local derivative: <code>âˆ‚log/âˆ‚probs = 1 / probs</code> (elementâ€‘wise).<br>
</li>
<li>Chain rule: <code>d_probs = d_log_probs / probs</code>.</li>
</ul>
</section>
<section id="backprop-through-softmax-logits-probs" class="level3">
<h3 class="anchored" data-anchor-id="backprop-through-softmax-logits-probs">4.3 Backâ€‘prop through Softmax (logits â†’ probs)</h3>
<ul>
<li><p><strong>Softmax formula:</strong> <code>p_i = exp(l_i) / Î£_j exp(l_j)</code>.<br>
</p></li>
<li><p><strong>Gradient (batch version):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> torch.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>d_logits <span class="op">=</span> probs.clone()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>d_logits[torch.arange(N), Y] <span class="op">-=</span> <span class="dv">1</span>   <span class="co"># subtract 1 at correct class</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>d_logits <span class="op">/=</span> N                        <span class="co"># average over batch</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Intuition:</strong></p>
<ul>
<li>Pull up probability of the correct class, push down all others.<br>
</li>
<li>Sum of each row of <code>d_logits</code> = 0 (conservation of probability).</li>
</ul></li>
</ul>
</section>
<section id="linear-layer-w2-b2" class="level3">
<h3 class="anchored" data-anchor-id="linear-layer-w2-b2">4.4 Linear Layer (W2, B2)</h3>
<ul>
<li>Forward: <code>logits = H @ W2.T + B2</code>.<br>
</li>
<li>Gradients:
<ul>
<li><code>d_W2 = d_logits.T @ H</code> (shape 27â€¯Ã—â€¯64).<br>
</li>
<li><code>d_B2 = d_logits.sum(dim=0, keepdim=True)</code>.<br>
</li>
<li><code>d_H  = d_logits @ W2</code>.</li>
</ul></li>
</ul>
</section>
<section id="batch-normalization" class="level3">
<h3 class="anchored" data-anchor-id="batch-normalization">4.5 Batch Normalization</h3>
<ul>
<li><p><strong>Forward (simplified, Î³â€¯=â€¯1, Î²â€¯=â€¯0):</strong></p>
<pre><code>Î¼   = mean(H, dim=0)                # 1Ã—64
ÏƒÂ²  = var(H, dim=0, unbiased=False) # 1Ã—64
HÌ‚   = (H - Î¼) / sqrt(ÏƒÂ² + Îµ)</code></pre></li>
<li><p><strong>Backward (key steps):</strong></p>
<ol type="1">
<li><code>d_HÌ‚ = d_H_pre</code> (gradient from next layer).<br>
</li>
<li><code>d_ÏƒÂ² = -0.5 * (d_HÌ‚ * (H-Î¼)) * (ÏƒÂ²+Îµ)^(-3/2)</code> â†’ sum over batch.<br>
</li>
<li><code>d_Î¼  = -d_HÌ‚ / sqrt(ÏƒÂ²+Îµ) - 2 * d_ÏƒÂ² * (H-Î¼) / N</code>.<br>
</li>
<li><code>d_H  = d_HÌ‚ / sqrt(ÏƒÂ²+Îµ) + d_ÏƒÂ² * 2*(H-Î¼)/N + d_Î¼ / N</code>.<br>
</li>
<li><code>d_Î³ = (d_H_pre * HÌ‚).sum(dim=0, keepdim=True)</code> (if Î³ kept).<br>
</li>
<li><code>d_Î² = d_H_pre.sum(dim=0, keepdim=True)</code> (if Î² kept).</li>
</ol></li>
<li><p><strong>Broadcasting rule:</strong></p>
<ul>
<li>When a scalar (e.g., <code>Î¼</code>) is broadcast to a matrix, the backward pass <strong>sums</strong> the incoming gradients over the broadcasted dimension.</li>
</ul></li>
</ul>
</section>
<section id="activation-10h-tanh" class="level3">
<h3 class="anchored" data-anchor-id="activation-10h-tanh">4.6 Activation <code>10â€‘H</code> (tanh)</h3>
<ul>
<li>Forward: <code>H = tanh(preact)</code>.<br>
</li>
<li>Local derivative: <code>1 - HÂ²</code>.<br>
</li>
<li>Backward: <code>d_preact = d_H * (1 - H**2)</code>.</li>
</ul>
</section>
<section id="linear-layer-w1-b1" class="level3">
<h3 class="anchored" data-anchor-id="linear-layer-w1-b1">4.7 Linear Layer (W1, B1)</h3>
<ul>
<li>Same pattern as W2/B2, but with <code>preact = X @ W1.T + B1</code>.<br>
</li>
<li>Gradients:
<ul>
<li><code>d_W1 = d_preact.T @ X</code>.<br>
</li>
<li><code>d_B1 = d_preact.sum(dim=0, keepdim=True)</code>.<br>
</li>
<li><code>d_X  = d_preact @ W1</code>.</li>
</ul></li>
</ul>
</section>
<section id="embedding-lookup-indexing" class="level3">
<h3 class="anchored" data-anchor-id="embedding-lookup-indexing">4.8 Embedding Lookup (indexing)</h3>
<ul>
<li><p>Forward: <code>M[i, k, :] = C[Y[i, k]]</code>.<br>
</p></li>
<li><p>Backward:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>d_C <span class="op">=</span> torch.zeros_like(C)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> Y[i, k]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        d_C[idx] <span class="op">+=</span> d_M[i, k]   <span class="co"># accumulate if same idx appears multiple times</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
<hr>
</section>
</section>
<section id="exercises-progressive-refactoring" class="level2">
<h2 class="anchored" data-anchor-id="exercises-progressive-refactoring">5ï¸âƒ£ Exercises (Progressive Refactoring)</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 40%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Exercise</th>
<th>Whatâ€™s changed</th>
<th>Key takeaway</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1</strong></td>
<td>Compute every intermediate <code>d_â€¦</code> tensor (as above). Verify with <code>torch.allclose</code>.</td>
<td>Manual gradients match autograd when shapes &amp; broadcasting are handled correctly.</td>
</tr>
<tr class="even">
<td><strong>2</strong></td>
<td>Derive a <strong>single</strong> analytic expression for <code>d_logits</code> (softmaxâ€¯+â€¯crossâ€‘entropy). Implement it in one line.</td>
<td>Much faster forwardâ€¯+â€¯backward; shows that many intermediate ops can be collapsed.</td>
</tr>
<tr class="odd">
<td><strong>3</strong></td>
<td>Derive a compact formula for <strong>batchâ€‘norm</strong> backward (see Â§4.5). Implement the whole layer in a few lines.</td>
<td>Highlights the â€œsumâ€‘overâ€‘broadcastâ€ pattern; avoids perâ€‘element code.</td>
</tr>
<tr class="even">
<td><strong>4</strong></td>
<td>Assemble all manual pieces into a full training loop (no <code>loss.backward()</code>).</td>
<td>Endâ€‘toâ€‘end manual training yields the same loss &amp; samples as the autograd version.</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="intuitive-insights" class="level2">
<h2 class="anchored" data-anchor-id="intuitive-insights">6ï¸âƒ£ Intuitive Insights</h2>
<ul>
<li><strong>Gradient â€œpushâ€‘pullâ€</strong> on logits:
<ul>
<li>Correct class gets a <strong>negative</strong> gradient (pull up).<br>
</li>
<li>Incorrect classes get a <strong>positive</strong> gradient (push down).<br>
</li>
<li>Rowâ€‘wise sum =â€¯0 â†’ probability mass conserved.</li>
</ul></li>
<li><strong>Batchâ€‘norm variance bias vs.&nbsp;unbiased:</strong>
<ul>
<li>Training often uses <strong>biased</strong> estimator (<code>1/N</code>).<br>
</li>
<li>Inference (running stats) should use <strong>unbiased</strong> (<code>1/(Nâ€‘1)</code>).<br>
</li>
<li>Mismatch can be a subtle bug; the author prefers the unbiased version throughout.</li>
</ul></li>
<li><strong>Broadcast â†”ï¸ Sum Duality:</strong>
<ul>
<li><strong>Forward:</strong> broadcasting replicates a smaller tensor across a larger one.<br>
</li>
<li><strong>Backward:</strong> the gradient w.r.t. the broadcasted tensor is the <strong>sum</strong> of the replicated gradients.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">7ï¸âƒ£ Next Steps</h2>
<ul>
<li><strong>Recurrent Neural Networks (RNNs) &amp; LSTMs</strong> â€“ extend manual backâ€‘prop to timeâ€‘unrolled architectures.<br>
</li>
<li>Explore <strong>gradient clipping</strong>, <strong>weight tying</strong>, and <strong>teacher forcing</strong> with manual gradients.</li>
</ul>
<hr>
<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">ğŸ“Œ TL;DR</h3>
<ol type="1">
<li>Replace <code>loss.backward()</code> with explicit tensorâ€‘level derivatives.<br>
</li>
<li>Derive and implement compact formulas for softmaxâ€‘crossâ€‘entropy and batchâ€‘norm.<br>
</li>
<li>Verify each step against PyTorchâ€™s autograd.<br>
</li>
<li>Assemble a full training loop that runs as fast as the autograd version while giving you full visibility into every gradient flow.</li>
</ol>
<p>Happy hacking! ğŸš€<br>
# Lesson 6</p>
</section>
</section>
<section id="mindmap-of-the-lecture-1" class="level2">
<h2 class="anchored" data-anchor-id="mindmap-of-the-lecture-1">ğŸ“š Mindâ€‘Map of the Lecture</h2>
<p><em>(Markdown + Mermaid diagram for quick visualisation)</em></p>
<hr>
<section id="overview-2" class="level3">
<h3 class="anchored" data-anchor-id="overview-2">1ï¸âƒ£ Overview</h3>
<ul>
<li><strong>Goal</strong> â€“ Extend a simple characterâ€‘level MLP language model into a deeper, hierarchical architecture (Wavenetâ€‘style).<br>
</li>
<li><strong>Context</strong> â€“ Lecture recorded in a hotel room in Kyoto; continuation of previous parts (3â€¯&amp;â€¯4).</li>
</ul>
</section>
<section id="data-baseline-model" class="level3">
<h3 class="anchored" data-anchor-id="data-baseline-model">2ï¸âƒ£ Data &amp; Baseline Model</h3>
<ul>
<li><strong>Dataset</strong> â€“ 182â€¯000 examples, each: 3â€‘character context â†’ predict 4th character.<br>
</li>
<li><strong>Baseline Architecture</strong>
<ul>
<li>Embedding table (<code>C</code>) â†’ Linear â†’ BatchNorm (named <em>bathroom</em>) â†’ 1Dâ€‘BatchNorm (<code>10h</code>) â†’ Linear output.<br>
</li>
<li>12â€¯000 parameters, validation loss â‰ˆâ€¯2.10.<br>
</li>
</ul></li>
<li><strong>Observations</strong>
<ul>
<li>Model already generates plausible â€œnameâ€‘likeâ€ strings.<br>
</li>
<li>Too much information is squashed in a single hidden layer.</li>
</ul></li>
</ul>
</section>
<section id="desired-architectural-changes" class="level3">
<h3 class="anchored" data-anchor-id="desired-architectural-changes">3ï¸âƒ£ Desired Architectural Changes</h3>
<ul>
<li><strong>Take more context</strong> â€“ increase block size from 3 â†’ 8 (later 16).<br>
</li>
<li><strong>Hierarchical fusion</strong> â€“ progressively combine neighboring characters (bigrams â†’ 4â€‘grams â†’ â€¦) instead of flattening all at once.<br>
</li>
<li><strong>Wavenet inspiration</strong> â€“ dilated causal convolutions â†’ treeâ€‘like receptive field growth.</li>
</ul>
</section>
<section id="refactoring-the-code" class="level3">
<h3 class="anchored" data-anchor-id="refactoring-the-code">4ï¸âƒ£ Refactoring the Code</h3>
<section id="layer-building-blocks" class="level4">
<h4 class="anchored" data-anchor-id="layer-building-blocks">4.1 Layer Building Blocks</h4>
<ul>
<li><strong>Linear layer</strong> â€“ simple matrix multiply (mirrors <code>torch.nn.Linear</code>).<br>
</li>
<li><strong>BatchNorm (bathroom)</strong> â€“ maintains running mean/variance, behaves differently in train vs.&nbsp;eval.<br>
</li>
<li><strong>Embedding layer</strong> â€“ lookup table (<code>nn.Embedding</code>).<br>
</li>
<li><strong>Flatten / â€œFlattenConsecutiveâ€</strong> â€“ custom module to reshape tensors, now able to group <em>n</em> consecutive embeddings.</li>
</ul>
</section>
<section id="containers" class="level4">
<h4 class="anchored" data-anchor-id="containers">4.2 Containers</h4>
<ul>
<li><strong>Sequential container</strong> â€“ custom implementation that stores a list of layers and forwards input through them.<br>
</li>
<li><strong>Model definition</strong> â€“ <code>model = Sequential([Embedding, FlattenConsecutive, Linear, BatchNorm, â€¦])</code>.</li>
</ul>
</section>
<section id="debugging-shapegymnastics" class="level4">
<h4 class="anchored" data-anchor-id="debugging-shapegymnastics">4.3 Debugging &amp; Shapeâ€‘Gymnastics</h4>
<ul>
<li>Inspected tensor shapes after each layer (e.g., <code>BÃ—TÃ—C â†’ BÃ—TÃ—E â†’ BÃ—(TÂ·E)</code>).<br>
</li>
<li>Realised flattening to <code>BÃ—â€‘1</code> was too aggressive; needed a 3â€‘D view (<code>BÃ—groupsÃ—(nÂ·E)</code>).</li>
</ul>
</section>
</section>
<section id="implementing-hierarchical-fusion" class="level3">
<h3 class="anchored" data-anchor-id="implementing-hierarchical-fusion">5ï¸âƒ£ Implementing Hierarchical Fusion</h3>
<ol type="1">
<li><strong>FlattenConsecutive(n=2)</strong> â€“ groups every 2 consecutive characters â†’ shape <code>BÃ—(T/2)Ã—(2Â·E)</code>.<br>
</li>
<li><strong>Stacked linear layers</strong> â€“ each layer processes the grouped embeddings, progressively increasing receptive field.<br>
</li>
<li><strong>Resulting network</strong> â€“ 3 hidden layers, each widening the context (2 â†’ 4 â†’ 8 characters).</li>
</ol>
</section>
<section id="batchnorm-bug-fix" class="level3">
<h3 class="anchored" data-anchor-id="batchnorm-bug-fix">6ï¸âƒ£ BatchNorm Bug &amp; Fix</h3>
<ul>
<li><strong>Problem</strong> â€“ <code>BatchNorm1D</code> computed statistics over only the first dimension (<code>B</code>) â†’ produced perâ€‘position means/variances.<br>
</li>
<li><strong>Fix</strong> â€“ Reduce over dimensions <code>(0,â€¯1)</code> when input is 3â€‘D, yielding a single mean/variance per channel (<code>1Ã—1Ã—C</code>).<br>
</li>
<li><strong>Outcome</strong> â€“ More stable statistics, slight validation loss improvement (2.029 â†’ 2.022).</li>
</ul>
</section>
<section id="training-results-hyperparameter-tweaks" class="level3">
<h3 class="anchored" data-anchor-id="training-results-hyperparameter-tweaks">7ï¸âƒ£ Training Results &amp; Hyperâ€‘parameter Tweaks</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Change</th>
<th>Params</th>
<th>Validation loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline (3â€‘char)</td>
<td>~12â€¯k</td>
<td>2.10</td>
</tr>
<tr class="even">
<td>â†‘ Context to 8 chars (flat)</td>
<td>+10â€¯k</td>
<td>2.02</td>
</tr>
<tr class="odd">
<td>Hierarchical (3â€‘layer)</td>
<td>~22â€¯k</td>
<td>2.029 â†’ 2.022</td>
</tr>
<tr class="even">
<td>â†‘ Embedding dim to 24, hidden units â†‘</td>
<td>~76â€¯k</td>
<td><strong>1.99</strong> (first subâ€‘2.0)</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Takeaway</strong> â€“ Bigger capacity helps, but training becomes slower; still no systematic hyperâ€‘parameter search.</li>
</ul>
</section>
<section id="relation-to-convolutional-networks-wavenet" class="level3">
<h3 class="anchored" data-anchor-id="relation-to-convolutional-networks-wavenet">8ï¸âƒ£ Relation to Convolutional Networks (Wavenet)</h3>
<ul>
<li><strong>Current implementation</strong> â€“ Explicit â€œforâ€‘loopâ€ over each position (inefficient).<br>
</li>
<li><strong>Convolutional view</strong> â€“ Same linear filters applied via dilated causal convolutions â†’ parallel GPU kernels, reuse of intermediate results.<br>
</li>
<li><strong>Future work</strong> â€“ Replace explicit loops with <code>nn.Conv1d</code> (dilated, causal), add gated activations, residual &amp; skip connections.</li>
</ul>
</section>
<section id="development-process-insights" class="level3">
<h3 class="anchored" data-anchor-id="development-process-insights">9ï¸âƒ£ Development Process Insights</h3>
<ul>
<li><strong>Documentation pain</strong> â€“ PyTorch docs are good; the courseâ€™s own â€œPatreonâ€ docs are sparse/inaccurate.<br>
</li>
<li><strong>Shape gymnastics</strong> â€“ Constantly checking NCL vs.&nbsp;NLC ordering, using <code>view</code>, <code>reshape</code>, <code>permute</code>.<br>
</li>
<li><strong>Prototyping workflow</strong>
<ol type="1">
<li><strong>Jupyter notebook</strong> â€“ rapid testing, shape inspection, debugging.<br>
</li>
<li><strong>Copyâ€‘paste to VSâ€¯Code repo</strong> â€“ clean module code.<br>
</li>
<li><strong>Run experiments</strong> via scripts (future: experiment harness).</li>
</ol></li>
</ul>
</section>
<section id="future-directions-open-topics" class="level3">
<h3 class="anchored" data-anchor-id="future-directions-open-topics">ğŸ”Ÿ Future Directions (Open Topics)</h3>
<ol type="1">
<li><strong>Implement true dilated causal convolutions</strong> (Wavenet).<br>
</li>
<li><strong>Add gated linear units, residual &amp; skip connections</strong>.<br>
</li>
<li><strong>Build an experimental harness</strong> â€“ systematic hyperâ€‘parameter sweeps, logging, early stopping.<br>
</li>
<li><strong>Explore other architectures</strong> â€“ RNNs, LSTMs, GRUs, Transformers.<br>
</li>
<li><strong>Beat the current best loss (â‰ˆâ€¯1.99)</strong> â€“ try different channel allocations, embedding sizes, initialization schemes, optimizers.</li>
</ol>
<hr>
</section>
</section>
<section id="mermaid-mindmap-copypaste-into-a-mermaidenabled-markdown-viewer" class="level2">
<h2 class="anchored" data-anchor-id="mermaid-mindmap-copypaste-into-a-mermaidenabled-markdown-viewer">ğŸ§­ Mermaid Mindâ€‘Map (copyâ€‘paste into a Mermaidâ€‘enabled markdown viewer)</h2>
<pre class="mermaid"><code>mindmap
  root((Characterâ€‘Level Language Model))
    Overview
      Goal
      Context
    Data &amp; Baseline
      Dataset
      Baseline Architecture
        Embedding
        Linear
        BatchNorm (bathroom)
        1Dâ€‘BatchNorm (10h)
        Linear Output
      Baseline Metrics
    Desired Changes
      Larger Context (3â†’8â†’16)
      Hierarchical Fusion
        Bigrams â†’ 4â€‘grams â†’ â€¦
      Wavenet Inspiration
    Refactoring
      Layer Building Blocks
        Linear
        BatchNorm
        Embedding
        FlattenConsecutive
      Containers
        Sequential
      Debugging Shapes
    Hierarchical Fusion Implementation
      FlattenConsecutive(n=2)
      Stacked Linear Layers
      Resulting 3â€‘layer Net
    BatchNorm Bug
      Problem (perâ€‘position stats)
      Fix (reduce over 0,1)
      Outcome
    Training Results
      Table of Changes â†’ Params â†’ Validation loss
      Observations
    Convolutional Relation
      Current explicit loops
      Convolutional view (dilated causal)
      Future: Conv1d + residual/skip
    Development Process
      Documentation challenges
      Shape gymnastics
      Prototyping workflow (Jupyter â†’ VSCode)
    Future Directions
      Dilated convolutions
      Gated units, residuals, skips
      Experiment harness
      RNN/LSTM/Transformer exploration
      Beat loss 1.99</code></pre>
<p><em>Render the diagram with any Mermaidâ€‘compatible markdown viewer (e.g., VSâ€¯Code, GitHub, HackMD).</em></p>
<hr>
<p><strong>TL;DR:</strong><br>
We started from a simple 3â€‘character MLP, expanded the context, introduced a hierarchical â€œflattenâ€‘consecutiveâ€ module, fixed a subtle BatchNorm bug, and built a deeper 3â€‘layer network that already beats the 2.0 validationâ€‘loss barrier. The next steps are to replace the explicit loops with true dilated causal convolutions, add residual/skip connections, and set up a proper experimental harness for systematic hyperâ€‘parameter search. Happy hacking! # Lesson 7</p>
</section>
</section>
<section id="mindmap-of-the-transcript" class="level1">
<h1>ğŸ§  Mindâ€‘Map of the Transcript</h1>
<p><em>(Markdown outline â€“ each level deeper = a more detailed subâ€‘topic)</em></p>
<hr>
<section id="introduction-motivation" class="level2">
<h2 class="anchored" data-anchor-id="introduction-motivation">1. Introduction &amp; Motivation</h2>
<ul>
<li><strong>ChatGPT</strong> â€“ a textâ€‘based AI that can perform many tasks (write poems, explain HTML, generate news, etc.)<br>
</li>
<li><strong>Probabilistic system</strong> â€“ same prompt â†’ different plausible outputs<br>
</li>
<li><strong>Goal of the talk</strong> â€“ understand whatâ€™s â€œunder the hoodâ€ of ChatGPT and build a tiny version ourselves</li>
</ul>
</section>
<section id="languagemodel-basics" class="level2">
<h2 class="anchored" data-anchor-id="languagemodel-basics">2. Languageâ€‘Model Basics</h2>
<ul>
<li><strong>Definition</strong> â€“ models the sequence of tokens (characters, subâ€‘words, words)<br>
</li>
<li><strong>Task</strong> â€“ given a prefix, predict the next token â†’ sequence completion<br>
</li>
<li><strong>Tokenization</strong>
<ul>
<li><em>Characterâ€‘level</em> (used in the demo) â†’ 65â€‘symbol vocab<br>
</li>
<li><em>Subâ€‘word / BPE</em> (used by OpenAI) â†’ ~50â€¯k vocab<br>
</li>
<li>Encoder â†”ï¸ Decoder maps between strings â†”ï¸ integer IDs</li>
</ul></li>
</ul>
</section>
<section id="data-set-tiny-shakespeare" class="level2">
<h2 class="anchored" data-anchor-id="data-set-tiny-shakespeare">3. Data Set â€“ â€œTiny Shakespeareâ€</h2>
<ul>
<li>Single ~1â€¯MiB file containing all Shakespeare works<br>
</li>
<li>Treated as a <strong>single long integer sequence</strong> after tokenization<br>
</li>
<li>Split: <strong>90â€¯% train</strong>, <strong>10â€¯% validation</strong></li>
</ul>
</section>
<section id="model-architecture-from-simple-to-full-transformer" class="level2">
<h2 class="anchored" data-anchor-id="model-architecture-from-simple-to-full-transformer">4. Model Architecture â€“ From Simple to Full Transformer</h2>
<section id="simple-baseline-bytelevel-byr-model" class="level3">
<h3 class="anchored" data-anchor-id="simple-baseline-bytelevel-byr-model">4.1. Simple Baseline: Byteâ€‘Level (BYR) Model</h3>
<ul>
<li>Embedding table â†’ directly produces logits for each position<br>
</li>
<li>Loss = <strong>Crossâ€‘Entropy</strong> (negative logâ€‘likelihood)</li>
</ul>
</section>
<section id="adding-positional-information" class="level3">
<h3 class="anchored" data-anchor-id="adding-positional-information">4.2. Adding Positional Information</h3>
<ul>
<li>Positionalâ€‘embedding matrix (blockâ€‘size Ã— embedâ€‘dim)<br>
</li>
<li>Token embedding + positional embedding â†’ input <strong>X</strong></li>
</ul>
</section>
<section id="selfattention-single-head" class="level3">
<h3 class="anchored" data-anchor-id="selfattention-single-head">4.3. Selfâ€‘Attention (single head)</h3>
<ul>
<li><strong>Queries (Q)</strong>, <strong>Keys (K)</strong>, <strong>Values (V)</strong> = linear projections of <strong>X</strong><br>
</li>
<li>Attention scores = <code>Q Â· Káµ€ / sqrt(head_dim)</code><br>
</li>
<li><strong>Masking</strong> â€“ lowerâ€‘triangular mask to prevent future tokens from attending (decoderâ€‘only)<br>
</li>
<li>Softmax â†’ weighted sum of <strong>V</strong> â†’ output of the head</li>
</ul>
</section>
<section id="multihead-attention" class="level3">
<h3 class="anchored" data-anchor-id="multihead-attention">4.4. Multiâ€‘Head Attention</h3>
<ul>
<li>Run several independent heads in parallel (e.g., 4 heads)<br>
</li>
<li>Concatenate their outputs â†’ same dimension as original embed size</li>
</ul>
</section>
<section id="feedforward-network-ffn" class="level3">
<h3 class="anchored" data-anchor-id="feedforward-network-ffn">4.5. Feedâ€‘Forward Network (FFN)</h3>
<ul>
<li>Linear â†’ GELU (or ReLU) â†’ Linear<br>
</li>
<li>Hidden dimension = 4â€¯Ã—â€¯embed_dim (as in the original paper)</li>
</ul>
</section>
<section id="residual-skip-connections" class="level3">
<h3 class="anchored" data-anchor-id="residual-skip-connections">4.6. Residual (Skip) Connections</h3>
<ul>
<li><code>X â†’ Selfâ€‘Attention â†’ +X</code><br>
</li>
<li><code>X â†’ FFN â†’ +X</code></li>
</ul>
</section>
<section id="layer-normalization" class="level3">
<h3 class="anchored" data-anchor-id="layer-normalization">4.7. Layer Normalization</h3>
<ul>
<li>Applied <strong>before</strong> each subâ€‘layer (preâ€‘norm formulation)<br>
</li>
<li>Normalizes across the embedding dimension per token</li>
</ul>
</section>
<section id="dropout" class="level3">
<h3 class="anchored" data-anchor-id="dropout">4.8. Dropout</h3>
<ul>
<li>Applied on attention weights, after attention output, and after FFN</li>
</ul>
</section>
<section id="full-decoderonly-block" class="level3">
<h3 class="anchored" data-anchor-id="full-decoderonly-block">4.9. Full Decoderâ€‘Only Block</h3>
<pre><code>X â”€â”€â–º LayerNorm â”€â”€â–º Multiâ€‘Head Selfâ€‘Attention â”€â”€â–º Dropout â”€â”€â–º +X
   â”‚                                            â”‚
   â””â”€â–º LayerNorm â”€â”€â–º Feedâ€‘Forward â”€â”€â–º Dropout â”€â”€â–º +X</code></pre>
</section>
<section id="stacking-blocks" class="level3">
<h3 class="anchored" data-anchor-id="stacking-blocks">4.10. Stacking Blocks</h3>
<ul>
<li>Stack <strong>N</strong> identical blocks (e.g., 6 layers) â†’ deep Transformer</li>
</ul>
</section>
<section id="final-projection" class="level3">
<h3 class="anchored" data-anchor-id="final-projection">4.11. Final Projection</h3>
<ul>
<li>LayerNorm â†’ Linear (embed_dim â†’ vocab_size) â†’ logits</li>
</ul>
</section>
</section>
<section id="training-procedure" class="level2">
<h2 class="anchored" data-anchor-id="training-procedure">5. Training Procedure</h2>
<ul>
<li><strong>Batching</strong> â€“ sample random chunks (blockâ€‘size) â†’ shape B Ã— T<br>
</li>
<li><strong>Optimizer</strong> â€“ Adam (often with weightâ€‘decay)<br>
</li>
<li><strong>Learning rate</strong> â€“ e.g., 3eâ€‘4 (scaled down for larger models)<br>
</li>
<li><strong>Training loop</strong> â€“ forward â†’ loss â†’ backward â†’ optimizer step<br>
</li>
<li><strong>Evaluation</strong> â€“ periodic â€œestimate_lossâ€ over several batches (train &amp; val)</li>
</ul>
</section>
<section id="scaling-experiments-results" class="level2">
<h2 class="anchored" data-anchor-id="scaling-experiments-results">6. Scaling Experiments &amp; Results</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 10%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Experiment</th>
<th>Model Size</th>
<th>Block Size</th>
<th>Heads</th>
<th>Embed Dim</th>
<th>Layers</th>
<th>Validation Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BYR (char)</td>
<td>~10â€¯M params</td>
<td>8</td>
<td>1</td>
<td>32</td>
<td>1</td>
<td>~4.8</td>
</tr>
<tr class="even">
<td>Add Selfâ€‘Attention (1 head)</td>
<td>~10â€¯M</td>
<td>8</td>
<td>1</td>
<td>32</td>
<td>1</td>
<td>~2.4</td>
</tr>
<tr class="odd">
<td>Multiâ€‘Head (4 heads)</td>
<td>~10â€¯M</td>
<td>8</td>
<td>4</td>
<td>8 each</td>
<td>1</td>
<td>~2.28</td>
</tr>
<tr class="even">
<td>+ Feedâ€‘Forward (4Ã—)</td>
<td>~10â€¯M</td>
<td>8</td>
<td>4</td>
<td>8 each</td>
<td>1</td>
<td>~2.24</td>
</tr>
<tr class="odd">
<td>Deep + Residual + LayerNorm</td>
<td>~10â€¯M</td>
<td>256</td>
<td>6</td>
<td>384</td>
<td>6</td>
<td>~2.08</td>
</tr>
<tr class="even">
<td>Deep + LayerNorm (preâ€‘norm)</td>
<td>~10â€¯M</td>
<td>256</td>
<td>6</td>
<td>384</td>
<td>6</td>
<td>~2.06</td>
</tr>
<tr class="odd">
<td>Fullâ€‘Scale (64â€‘batch, 256â€‘ctx, 6 heads, 6 layers, dropout 0.2)</td>
<td>~10â€¯M</td>
<td>256</td>
<td>6</td>
<td>384</td>
<td>6</td>
<td><strong>1.48</strong></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Observation:</strong> Adding attention, multiâ€‘heads, FFN, residuals, layerâ€‘norm, and scaling up context dramatically reduces loss.<br>
</li>
<li>Generated text becomes more â€œShakespeareâ€‘likeâ€ (still nonsensical at character level).</li>
</ul>
</section>
<section id="decoderonly-vs-encoderdecoder" class="level2">
<h2 class="anchored" data-anchor-id="decoderonly-vs-encoderdecoder">7. Decoderâ€‘Only vs Encoderâ€‘Decoder</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 21%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Decoderâ€‘Only (GPT)</th>
<th>Encoderâ€‘Decoder (e.g., original â€œAttention is All You Needâ€)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Masking</td>
<td>Causal (triangular) â†’ autoregressive generation</td>
<td>No causal mask in encoder; decoder still causal</td>
</tr>
<tr class="even">
<td>Crossâ€‘Attention</td>
<td><strong>Absent</strong> (only selfâ€‘attention)</td>
<td>Present â€“ decoder attends to encoder outputs</td>
</tr>
<tr class="odd">
<td>Useâ€‘case</td>
<td>Unconditioned language modeling / text generation</td>
<td>Conditional generation (e.g., translation)</td>
</tr>
<tr class="even">
<td>In this demo</td>
<td>Only decoder block â†’ generates Shakespeareâ€‘style text</td>
<td>Not implemented (no encoder, no crossâ€‘attention)</td>
</tr>
</tbody>
</table>
</section>
<section id="finetuning-alignment-chatgpt" class="level2">
<h2 class="anchored" data-anchor-id="finetuning-alignment-chatgpt">8. Fineâ€‘Tuning &amp; Alignment (ChatGPT)</h2>
<ol type="1">
<li><strong>Preâ€‘training</strong> â€“ massive corpus (â‰ˆ300â€¯B tokens) â†’ decoderâ€‘only Transformer (e.g., GPTâ€‘3 175â€¯B params)<br>
</li>
<li><strong>Supervised fineâ€‘tuning</strong> â€“ small dataset of <em>question â†’ answer</em> pairs (fewâ€‘k examples) to make the model an â€œassistantâ€<br>
</li>
<li><strong>Reward Modeling</strong> â€“ collect multiple model outputs, rank them, train a reward model to predict human preference<br>
</li>
<li><strong>RLHF (Reinforcement Learning from Human Feedback)</strong> â€“ use Proximal Policy Optimization (PPO) to fineâ€‘tune the policy so generated answers score high on the reward model</li>
</ol>
<ul>
<li>The fineâ€‘tuning stages are <strong>not</strong> publicly released; they require largeâ€‘scale infrastructure.</li>
</ul>
</section>
<section id="nanogpt-repository-by-the-presenter" class="level2">
<h2 class="anchored" data-anchor-id="nanogpt-repository-by-the-presenter">9. nanogpt Repository (by the presenter)</h2>
<ul>
<li><strong>Two files</strong>: <code>model.py</code> (definition of the Transformer) and <code>train.py</code> (training loop, checkpointing, distributed support)<br>
</li>
<li>Mirrors the notebook implementation:
<ul>
<li>Tokenizer (characterâ€‘level)<br>
</li>
<li>Embedding + positional embedding<br>
</li>
<li>Multiâ€‘head selfâ€‘attention (batched)<br>
</li>
<li>Feedâ€‘forward, residuals, layerâ€‘norm, dropout<br>
</li>
<li>Optimizer, learningâ€‘rate schedule, evaluation utilities</li>
</ul></li>
</ul>
</section>
<section id="takeaways-next-steps-1" class="level2">
<h2 class="anchored" data-anchor-id="takeaways-next-steps-1">10. Takeâ€‘aways &amp; Next Steps</h2>
<ul>
<li><strong>Core idea:</strong> â€œAttention is all you needâ€ â†’ a stack of selfâ€‘attention + feedâ€‘forward blocks is enough for powerful language models.<br>
</li>
<li><strong>Building a GPTâ€‘like model</strong> can be done in ~200 lines of PyTorch code when using a tiny dataset.<br>
</li>
<li><strong>Scaling</strong> (larger context, more heads, deeper stacks, regularization) yields dramatic loss improvements.<br>
</li>
<li><strong>Realâ€‘world ChatGPT</strong> adds two major phases beyond preâ€‘training: supervised fineâ€‘tuning and RLHF.<br>
</li>
<li><strong>Further work:</strong>
<ul>
<li>Experiment with subâ€‘word tokenizers (BPE, SentencePiece).<br>
</li>
<li>Train larger models on bigger corpora (e.g., Wikipedia, OpenWebText).<br>
</li>
<li>Implement encoderâ€‘decoder architecture for conditional tasks (translation, summarization).<br>
</li>
<li>Explore RLHF pipelines to align models with human preferences.</li>
</ul></li>
</ul>
<hr>
<p><em>End of mindâ€‘map.</em><br>
# Lesson 8</p>
</section>
<section id="tokenization-mindmap-markdown-mermaid" class="level2">
<h2 class="anchored" data-anchor-id="tokenization-mindmap-markdown-mermaid">ğŸ§  Tokenization Mindâ€‘Map (Markdownâ€¯+â€¯Mermaid)</h2>
<p>Below is a <strong>comprehensive mindâ€‘map</strong> that captures every major idea, subâ€‘idea and detail from the transcript.<br>
The map is written in <strong>Mermaid</strong> syntax (supported by most Markdown viewers) and is followed by a plainâ€‘text outline for quick reference.</p>
<hr>
<section id="mermaid-diagram" class="level3">
<h3 class="anchored" data-anchor-id="mermaid-diagram">Mermaid Diagram</h3>
<pre class="mermaid"><code>mindmap
  root((Tokenization in Large Language Models))

    subgraph Overview
      Overview[What is tokenization?]
      WhyItMatters[Why tokenization is the â€œatomâ€ of LLMs]
      HiddenIssues[Hidden footâ€‘guns &amp; odd behaviours]
    end

    subgraph Naive_Char_Level
      CharTokenizer[Characterâ€‘level tokenizer (65 chars)]
      EmbeddingTable[Embedding table = vocab size rows]
      Limitations[Too coarse â†’ real models use chunkâ€‘level]
    end

    subgraph BPE_Concept
      BPE[Byteâ€‘Pair Encoding (BPE)]
      InputEncoding[UTFâ€‘8 â†’ bytes (0â€‘255)]
      InitialVocab[256 raw byte tokens]
      MergeProcess[Iteratively merge mostâ€‘frequent byte pairs]
      VocabularyGrowth[New token IDs appended (256,257,â€¦)]
      Example[Example: â€œAAâ€ â†’ token 256, then â€œABâ€ â†’ token 257 â€¦]
    end

    subgraph Tokenizer_Implementation
      GetStats[Function: get_stats(list of ints)]
      MergeStep[Function: replace_pair(ids, pair, new_id)]
      YLoop[Iterate merges â†’ target vocab size]
      Compression[Sequence length shrinks, vocab grows]
      CodeRepo[MBP repo â€“ reference implementation]
    end

    subgraph Real_World_Tokenizers
      Tiktoken[Tiktoken (OpenAI)]
        TiktokenApp[Web UI â€“ live tokenisation]
        GPT2_Tokenizer[~50â€¯k vocab, 1.24â€‘token context]
        GPT4_Tokenizer[~100â€¯k vocab, denser, better whitespace handling]
        SpecialTokens[&lt;eos&gt;, &lt;pad&gt;, &lt;bos&gt;, &lt;fim&gt; prefixes]
        TokenSizeEffect[More tokens â†’ denser context, but larger embedding &amp; LM head]
      SentencePiece[Google SentencePiece]
        SP_Encoding[Can train &amp; infer]
        SP_BPE[Runs BPE on Unicode codeâ€‘points]
        ByteFallback[Rare codeâ€‘points â†’ UTFâ€‘8 bytes â†’ extra tokens]
        ConfigComplexity[Many hyperâ€‘params, â€œshrinkâ€‘factorâ€, etc.]
        RegexChunking[Regex rules to prevent bad merges (punctuation, numbers, etc.)]
    end

    subgraph Tokenization_Issues
      Spelling[LLMs struggle with spelling (long tokens like â€œdefaultstyleâ€)]
      Arithmetic[Numbers split arbitrarily â†’ poor arithmetic]
      NonEnglish[More tokens for same sentence â†’ context waste]
      Python_Code[Spaces become separate tokens â†’ context loss]
      TrailingSpace[Warning: trailing space adds a token â†’ hurts performance]
      UnstableTokens[â€œunstableâ€ token handling in tiktoken source]
      SolidGoldMagikarp[Rare Redditâ€‘user token never seen in LM training â†’ undefined behaviour]
    end

    subgraph Model_Surgery
      ExtendVocab[Add new special tokens â†’ resize embedding rows]
      LMHeadResize[Resize final linear layer (logits) accordingly]
      FreezeBase[Freeze original weights, train only new token embeddings]
      GistTokens[Compress long prompts into a few learned tokens (distillation)]
    end

    subgraph Multimodal_Tokenization
      VisionTokens[Image patches â†’ tokens]
      AudioTokens[Audio frames â†’ tokens]
      SoftTokens[Continuous embeddings (autoâ€‘encoders) vs hard tokens]
      UnifiedTransformer[Same architecture, different token vocabularies]
    end

    subgraph Efficiency_Considerations
      ContextLength[Longer vocab â†’ shorter sequences â†’ more context per token]
      EmbeddingCost[More rows â†’ more parameters &amp; compute]
      DataFormats[JSON vs YAML token count (JSON 116â€¯tokens, YAML 99â€¯tokens)]
      TokenEconomy[Payâ€‘perâ€‘token APIs â†’ choose dense encodings]
    end

    subgraph Recommendations
      UseGPT4_Tiktoken[Prefer GPTâ€‘4 tokeniser (dense, good whitespace handling)]
      IfTrainingNeeded[Use SentencePiece BPE (but copyâ€‘paste Metaâ€™s config)]
      AvoidDIY[Donâ€™t handâ€‘tune many SP hyperâ€‘params â€“ easy to misâ€‘configure]
      WaitForMâ€‘BPE[Future: a trainingâ€‘ready version of tiktokenâ€™s BPE]
      BewareSpecialTokens[Know specialâ€‘token IDs when fineâ€‘tuning]
    end

    %% Connections
    Overview --&gt; WhyItMatters
    Overview --&gt; HiddenIssues
    Naive_Char_Level --&gt; CharTokenizer
    Naive_Char_Level --&gt; EmbeddingTable
    Naive_Char_Level --&gt; Limitations
    BPE_Concept --&gt; InputEncoding
    BPE_Concept --&gt; InitialVocab
    BPE_Concept --&gt; MergeProcess
    BPE_Concept --&gt; VocabularyGrowth
    BPE_Concept --&gt; Example
    Tokenizer_Implementation --&gt; GetStats
    Tokenizer_Implementation --&gt; MergeStep
    Tokenizer_Implementation --&gt; YLoop
    Tokenizer_Implementation --&gt; Compression
    Tokenizer_Implementation --&gt; CodeRepo
    Real_World_Tokenizers --&gt; Tiktoken
    Real_World_Tokenizers --&gt; SentencePiece
    Tokenization_Issues --&gt; Spelling
    Tokenization_Issues --&gt; Arithmetic
    Tokenization_Issues --&gt; NonEnglish
    Tokenization_Issues --&gt; Python_Code
    Tokenization_Issues --&gt; TrailingSpace
    Tokenization_Issues --&gt; UnstableTokens
    Tokenization_Issues --&gt; SolidGoldMagikarp
    Model_Surgery --&gt; ExtendVocab
    Model_Surgery --&gt; LMHeadResize
    Model_Surgery --&gt; FreezeBase
    Model_Surgery --&gt; GistTokens
    Multimodal_Tokenization --&gt; VisionTokens
    Multimodal_Tokenization --&gt; AudioTokens
    Multimodal_Tokenization --&gt; SoftTokens
    Multimodal_Tokenization --&gt; UnifiedTransformer
    Efficiency_Considerations --&gt; ContextLength
    Efficiency_Considerations --&gt; EmbeddingCost
    Efficiency_Considerations --&gt; DataFormats
    Efficiency_Considerations --&gt; TokenEconomy
    Recommendations --&gt; UseGPT4_Tiktoken
    Recommendations --&gt; IfTrainingNeeded
    Recommendations --&gt; AvoidDIY
    Recommendations --&gt; WaitForMâ€‘BPE
    Recommendations --&gt; BewareSpecialTokens</code></pre>
<hr>
</section>
<section id="plaintext-outline-for-quick-skim" class="level3">
<h3 class="anchored" data-anchor-id="plaintext-outline-for-quick-skim">Plainâ€‘Text Outline (for quick skim)</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 42%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Section</strong></th>
<th><strong>Key Points</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Tokenization Overview</strong></td>
<td>Converts raw text â†’ sequence of integer tokens. Tokens are the atomic unit of LLMs; many hidden quirks stem from this step.</td>
</tr>
<tr class="even">
<td><strong>NaÃ¯ve Characterâ€‘Level Tokenizer</strong></td>
<td>65â€‘character vocab from Shakespeare data; each char â†’ token; embedding table size = vocab size. Works but far from stateâ€‘ofâ€‘theâ€‘art.</td>
</tr>
<tr class="odd">
<td><strong>Byteâ€‘Pair Encoding (BPE)</strong></td>
<td>Start with 256 byte tokens (UTFâ€‘8). Repeatedly find most frequent adjacent byte pair, replace with a new token (ID 256, 257, â€¦). Reduces sequence length while growing vocab.</td>
</tr>
<tr class="even">
<td><strong>Implementation Details</strong></td>
<td><code>get_stats</code> counts consecutive pairs; <code>merge</code> replaces a pair with a new ID; a Yâ€‘loop repeats until target vocab size (e.g., 276 â†’ 20 merges). Compression ratio â‰ˆ 1.27 on example text.</td>
</tr>
<tr class="odd">
<td><strong>Realâ€‘World Tokenizers</strong></td>
<td><strong>Tiktoken</strong> (OpenAI): fast inference, preâ€‘trained vocab (GPTâ€‘2 â‰ˆâ€¯50â€¯k, GPTâ€‘4 â‰ˆâ€¯100â€¯k). Handles special tokens (<code>&lt;eos&gt;</code>, <code>&lt;pad&gt;</code>, <code>&lt;fim&gt;</code>). <strong>SentencePiece</strong>: can train &amp; infer, runs BPE on Unicode codeâ€‘points, falls back to byte tokens for rare chars, many configurable options, regexâ€‘based chunking to avoid bad merges.</td>
</tr>
<tr class="even">
<td><strong>Tokenization Issues</strong></td>
<td>â€¢ Spelling: long tokens (e.g., â€œdefaultstyleâ€) make the model treat whole words as single atoms â†’ poor spelling. <br>â€¢ Arithmetic: numbers split arbitrarily (e.g., â€œ127â€ â†’ two tokens) â†’ bad math. <br>â€¢ Nonâ€‘English: same sentence uses many more tokens â†’ context waste. <br>â€¢ Python code: each space becomes a token â†’ huge context consumption. <br>â€¢ Trailing spaces add a token â†’ API warns of degraded performance. <br>â€¢ â€œUnstableâ€ tokens in tiktoken source cause edgeâ€‘case failures. <br>â€¢ â€œSolid Gold Magikarpâ€ â€“ a Redditâ€‘user token never seen during LM training â†’ untrained embedding â†’ undefined behaviour.</td>
</tr>
<tr class="odd">
<td><strong>Model Surgery for New Tokens</strong></td>
<td>To add special tokens: enlarge embedding matrix (new rows) and LM head (new columns). Usually freeze original weights, train only new token embeddings. â€œGist tokensâ€ compress long prompts into a few learned tokens via distillation.</td>
</tr>
<tr class="even">
<td><strong>Multimodal Tokenization</strong></td>
<td>Same Transformer can process image patches, audio frames, or soft continuous embeddings as tokens. No architectural change needed â€“ just different vocabularies.</td>
</tr>
<tr class="odd">
<td><strong>Efficiency &amp; Token Economy</strong></td>
<td>Larger vocab â†’ shorter sequences â†’ more context per token, but larger embedding &amp; final linear layer â†’ more compute. Choose dense data formats (YAML &lt; JSON) to save tokens; token cost matters for API pricing.</td>
</tr>
<tr class="even">
<td><strong>Practical Recommendations</strong></td>
<td>1. Use <strong>tiktoken (GPTâ€‘4)</strong> for inference â€“ most efficient. <br>2. If you must train a tokenizer, copy Metaâ€™s SentencePiece config (avoid fiddling with many hyperâ€‘params). <br>3. Donâ€™t reinvent SentencePiece; itâ€™s easy to misâ€‘configure. <br>4. Await a trainingâ€‘ready version of tiktokenâ€™s BPE (Mâ€‘BPE). <br>5. When fineâ€‘tuning, remember to resize embeddings &amp; LM head for any new special tokens.</td>
</tr>
</tbody>
</table>
<hr>
<section id="how-to-view-the-diagram" class="level4">
<h4 class="anchored" data-anchor-id="how-to-view-the-diagram">How to view the diagram</h4>
<ul>
<li>If youâ€™re using <strong>GitHub</strong>, <strong>GitLab</strong>, <strong>VSâ€¯Code</strong>, <strong>Obsidian</strong>, or any Markdown editor that supports Mermaid, simply copy the Mermaid block above into a <code>.md</code> file and it will render automatically.<br>
</li>
<li>For plainâ€‘text viewers, refer to the outline table.</li>
</ul>
<p>Feel free to expand any node (e.g., dive deeper into the regex used by SentencePiece, or explore the exact token IDs for special tokens) by adding subâ€‘branches in the Mermaid code. Happy tokenizing! ğŸš€</p>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/Rahuketu86\.github\.io\/solveit_z2h");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/Rahuketu86/solveit_z2h/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>