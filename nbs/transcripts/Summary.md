# Lesson 1

## ğŸ§  Mindmap of the Lecture â€œBuilding & Understanding Microgradâ€

```mermaid
mindmap
  root((Micrograd Lecture â€“ Neuralâ€‘Net Training Under the Hood))

  %% -------------------------------------------------
  %% 1. Introduction & Goals
  %% -------------------------------------------------
  Introduction
    What weâ€™ll do
      â€¢ Start from a blank Jupyter notebook
      â€¢ Define & train a neural net stepâ€‘byâ€‘step
      â€¢ Peek â€œunder the hoodâ€ of backâ€‘propagation
    Why it matters
      â€¢ Intuitive grasp of gradientâ€‘based learning
      â€¢ Build a tiny autograd engine (micrograd)

  %% -------------------------------------------------
  %% 2. Micrograd â€“ The Core Idea
  %% -------------------------------------------------
  Micrograd
    Definition
      â€¢ A **scalarâ€‘valued autograd engine**
      â€¢ Implements **automatic differentiation** (backâ€‘prop)
    Key Concepts
      â€¢ **Value object** â€“ wraps a scalar & tracks graph info
      â€¢ **Expression graph** â€“ nodes = operations, edges = data flow
      â€¢ **Forward pass** â€“ compute output (e.g. g = â€¦)
      â€¢ **Backward pass** â€“ propagate gradients via chain rule
    Operations supported
      â€¢ add (+)          â€“ local derivative = 1
      â€¢ mul (Ã—)          â€“ local derivative = other operand
      â€¢ pow (^)          â€“ local derivative = nÂ·xâ¿â»Â¹
      â€¢ tanh (10h)       â€“ local derivative = 1 â€“ tanhÂ²(x)
      â€¢ neg, sqrt, div, â€¦ (can be added similarly)

  %% -------------------------------------------------
  %% 3. Derivative Intuition
  %% -------------------------------------------------
  Derivative Basics
    â€¢ Definition: limâ‚•â†’0 (f(x+h) â€“ f(x))/h
    â€¢ Numerical approximation with tiny h (e.g. 0.001)
    â€¢ Example: f(x)=3xÂ²â€“4x+5 â†’ fâ€²(3)=14
    â€¢ Sign tells direction of change (positive â†’ increase)

  %% -------------------------------------------------
  %% 4. Building the Value Object
  %% -------------------------------------------------
  Value Object
    Attributes
      â€¢ data  â€“ raw scalar
      â€¢ grad  â€“ âˆ‚output/âˆ‚self (init 0)
      â€¢ _prev â€“ set of child nodes (for graph traversal)
      â€¢ _op   â€“ string name of operation that created it
    Operator Overloads
      â€¢ __add__(self, other) â†’ Value
      â€¢ __radd__(self, other) â†’ Value (handles const + Value)
      â€¢ __mul__(self, other) â†’ Value
      â€¢ __rmul__(self, other) â†’ Value (handles const Ã— Value)
      â€¢ __pow__(self, exponent) â†’ Value
      â€¢ __neg__, __sub__, __truediv__ (via mul & pow)
    Local Backward Functions
      â€¢ add:   self.grad += out.grad ; other.grad += out.grad
      â€¢ mul:   self.grad += other.data * out.grad
               other.grad += self.data * out.grad
      â€¢ pow:   self.grad += exponent * (self.data**(exponent-1)) * out.grad
      â€¢ tanh:  self.grad += (1 - self.data**2) * out.grad

  %% -------------------------------------------------
  %% 5. Graph Visualization
  %% -------------------------------------------------
  Graph Drawing (drawdot)
    â€¢ Uses Graphviz (graphvizâ€‘dot) to render nodes & edges
    â€¢ Shows operation nodes (e.g. â€œ+â€, â€œ*â€) and value nodes
    â€¢ Helpful for debugging & teaching

  %% -------------------------------------------------
  %% 6. Backâ€‘Propagation Mechanics
  %% -------------------------------------------------
  Backâ€‘Propagation
    â€¢ Start at output node â†’ set grad = 1
    â€¢ **Topological sort** (DFS) to order nodes so children processed first
    â€¢ Walk nodes in reverse topological order, calling each nodeâ€™s _backward
    â€¢ **Chain rule**: local derivative Ã— upstream gradient
    â€¢ Gradient accumulation (`+=`) â€“ crucial when a node has multiple parents
      â€“ Fixed bug: previously used `=` causing overwrites (e.g. a + a)

  %% -------------------------------------------------
  %% 7. Numerical Gradient Checks
  %% -------------------------------------------------
  Gradient Check
    â€¢ Perturb a leaf (e.g. a += h) â†’ recompute output
    â€¢ Approximate âˆ‚output/âˆ‚a â‰ˆ (f(a+h) â€“ f(a))/h
    â€¢ Verify against analytically computed grads

  %% -------------------------------------------------
  %% 8. Building Neural Networks with Micrograd
  %% -------------------------------------------------
  Neuralâ€‘Net Construction
    Neuron
      â€¢ Parameters: weights `w_i` (list of Value) + bias `b`
      â€¢ Forward: Î£ w_iÂ·x_i + b â†’ activation (tanh)
    Layer
      â€¢ List of Neurons (fullyâ€‘connected to same inputs)
      â€¢ Forward: apply each neuron, collect outputs
    MLP (Multiâ€‘Layer Perceptron)
      â€¢ Sequence of Layers
      â€¢ Forward: feed output of one layer as input to next
      â€¢ Example architecture: 3â€‘input â†’ [4,4] hidden â†’ 1â€‘output

  %% -------------------------------------------------
  %% 9. Loss Functions & Training Loop
  %% -------------------------------------------------
  Loss & Optimization
    Loss (Meanâ€‘Squaredâ€‘Error)
      â€¢ L = Î£ (y_pred â€“ y_true)Â²
      â€¢ Gradient w.r.t. each prediction = 2Â·(y_pred â€“ y_true)
    Training Loop (Stochastic Gradient Descent)
      1. Zero grads (`p.grad = 0` for all params)
      2. Forward pass â†’ compute loss
      3. Backward pass (`loss.backward()`)
      4. Parameter update: `p.data -= lr * p.grad`
      5. Repeat (adjust learningâ€‘rate, optionally decay)
    Common Pitfalls
      â€¢ Forgetting to zero grads â†’ gradient accumulation bug
      â€¢ Too large learning rate â†’ overshoot / divergence
      â€¢ Too small â†’ slow convergence

  %% -------------------------------------------------
  %% 10. Comparison with PyTorch
  %% -------------------------------------------------
  PyTorch Parallel
    â€¢ Same API: `torch.tensor`, `requires_grad=True`
    â€¢ Autograd automatically builds the same graph (but with tensors)
    â€¢ Forward & backward behave identically for scalar case
    â€¢ Example: replicate micrograd network using `torch.nn.Module`
    â€¢ Extending PyTorch
      â€“ Register custom ops (forward + backward) via `torch.autograd.Function`
      â€“ Example shown for a cubic polynomial

  %% -------------------------------------------------
  â”‚ 11. Advanced Topics (Briefly Mentioned)
  %% -------------------------------------------------
  Advanced Topics
    â€¢ Batching & miniâ€‘batch SGD (process subsets of data)
    â€¢ Crossâ€‘entropy loss for classification
    â€¢ L2 regularization (weight decay) for better generalization
    â€¢ Learningâ€‘rate schedules (decay, momentum, Adam, etc.)
    â€¢ Scaling to billions of parameters (e.g., GPTâ€‘style models)

  %% -------------------------------------------------
  %% 12. Summary & Takeâ€‘aways
  %% -------------------------------------------------
  Summary
    â€¢ Neural nets = **compositional mathematical expressions**
    â€¢ **Backâ€‘prop = recursive application of the chain rule**
    â€¢ Micrograd shows the **minimal code** needed (â‰ˆ150â€¯lines)
    â€¢ Real libraries (PyTorch) add **tensor efficiency** but same math
    â€¢ Understanding the core mechanics helps debug & extend models
```

### How to Read the Mindmap
- **Indentation** â†’ hierarchy (main topic â†’ subâ€‘topic â†’ details).  
- **Bold headings** indicate the most important concepts.  
- **Arrows (â†’)** show data flow or process steps (e.g., forward â†’ backward).  
- **Bullet points** under each node give concrete examples, code snippets, or key takeâ€‘aways.

Feel free to copy the Mermaid block into any Markdown viewer that supports Mermaid (e.g., GitHub, VSâ€¯Code, Jupyter with `%%mermaid`) to see the visual mindmap. Happy learning! ğŸš€
# Lesson 2

# ğŸ§  Makeâ€‘More: Characterâ€‘Level Language Modeling Mindâ€‘Map  

*(All concepts are derived from the transcript.  Indentation = hierarchy.)*  

---  

## 1ï¸âƒ£ Overview  
- **Goal** â€“ Reâ€‘implement *micrograd*â€‘style learning on a new repo **makeâ€‘more**.  
- **Dataset** â€“ `names.txt` (~32â€¯000 unique names scraped from a government site).  
- **Useâ€‘case** â€“ Generate novel, nameâ€‘like strings (e.g., babyâ€‘name suggestions).  

---  

## 2ï¸âƒ£ Core Idea: Characterâ€‘Level Language Model  
- **Treat each name as a sequence of characters** (including start & end tokens).  
- **Model task** â€“ Predict the *next* character given the previous one(s).  

---  

## 3ï¸âƒ£ Bigram (2â€‘gram) Model â€“ The Simple Baseline  
### 3.1 Data Preparation  
- Load `names.txt` â†’ massive string â†’ `splitlines()` â†’ list `words`.  
- Compute:  
  - `num_words â‰ˆ 32â€¯000`  
  - `min_len = 2`, `max_len = 15`  

### 3.2 Extracting Bigrams  
- For each word `w`:  
  ```python
  for c1, c2 in zip(w, w[1:]):   # sliding window of size 2
      bigram = (c1, c2)
  ```  
- Add **special start token** `.` before the first char and **end token** `.` after the last char.  

### 3.3 Counting Frequencies (Dictionary â†’ 2â€‘D Tensor)  
- `counts[prev_char, next_char] += 1` (default 0).  
- Convert to a **28â€¯Ã—â€¯28** integer tensor (`torch.int32`).  
  - 26 letters + **start** (`.`) + **end** (`.`).  

### 3.4 Visualisation (matplotlib)  
- Heatâ€‘map of the count matrix.  
- Observations:  
  - Rows/columns for start/end tokens are mostly zeros (they never appear in the â€œwrongâ€ position).  

### 3.5 Refinement of Token Set  
- Collapse to **27â€¯Ã—â€¯27** matrix (single special token).  
- Reâ€‘index:  
  - `.` â†’ indexâ€¯0, `a` â†’ 1, â€¦, `z` â†’ 26.  

### 3.6 From Counts â†’ Probabilities  
- Rowâ€‘wise normalization:  
  ```python
  probs = counts.float() / counts.sum(dim=1, keepdim=True)
  ```  
- Each row now sums to **1** â†’ a categorical distribution for the next character.  

### 3.7 Sampling Names (using the bigram table)  
- Start at indexâ€¯0 (`.`).  
- Loop:  
  1. Grab current row `p = probs[current_idx]`.  
  2. Sample `next_idx = torch.multinomial(p, 1, replacement=True, generator=g)`.  
  3. Break if `next_idx == 0` (end token).  
  4. Append the decoded character.  

### 3.8 Model Evaluation â€“ Likelihood  
- **Likelihood** = product of probabilities assigned to the true bigrams.  
- **Logâ€‘likelihood** = sum of `log(p_i)`.  
- **Negative Logâ€‘Likelihood (NLL)** = `â€‘log_likelihood` â†’ standard loss (lower is better).  
- Example: NLL â‰ˆ **2.4â€“2.5** on the full training set.  

### 3.9 Smoothing (to avoid zero probabilities)  
- Add a small constant (e.g., `+1`) to every count before normalisation.  
- Guarantees nonâ€‘zero probabilities â†’ prevents infinite NLL for unseen bigrams.  

---  

## 4ï¸âƒ£ Neuralâ€‘Network Reâ€‘Implementation (Gradientâ€‘Based)  
### 4.1 Why Switch?  
- Counting works for bigrams but **doesnâ€™t scale** to longer contexts (e.g., 10â€‘grams).  
- Neural nets can learn **compact, differentiable** representations for arbitrary context lengths.  

### 4.2 Data Encoding â€“ Oneâ€‘Hot Vectors  
- Map each character index `i` â†’ 27â€‘dimensional oneâ€‘hot vector `x_i`.  
- Use `torch.nn.functional.one_hot(indices, num_classes=27)`.  
- Cast to `float32` for NN input.  

### 4.3 Model Architecture (initially)  
- **Linear layer** (no bias): `logits = x @ W`  
  - `W` shape **27â€¯Ã—â€¯27** (each row = logâ€‘counts for a given previous character).  
- **Softmax** â†’ probabilities:  
  ```python
  probs = torch.softmax(logits, dim=1)   # exponentiate + normalise internally
  ```  

### 4.4 Forward Pass (vectorised)  
1. Encode all inputs â†’ `X` (Nâ€¯Ã—â€¯27).  
2. Compute `logits = X @ W`.  
3. `probs = softmax(logits)`.  
4. Gather the probability of the *true* next character:  
   ```python
   true_probs = probs[torch.arange(N), targets]   # targets = nextâ€‘char indices
   ```  
5. Compute **NLL loss**:  
   ```python
   loss = -torch.log(true_probs).mean()
   ```  

### 4.5 Backâ€‘Propagation & Parameter Update  
- Zero grads: `W.grad = None`.  
- `loss.backward()` â†’ fills `W.grad`.  
- Gradient descent step (e.g., SGD):  
  ```python
  W.data -= lr * W.grad
  ```  
- Repeat for many epochs â†’ loss drops from ~3.8 â†’ ~2.4 (matches counting approach).  

### 4.6 Practical PyTorch Tips (from the transcript)  
- `torch.tensor` vs `torch.Tensor` â†’ prefer lowercase `torch.tensor` for float tensors.  
- **Broadcasting**: dividing a (27â€¯Ã—â€¯27) matrix by a (27â€¯Ã—â€¯1) column works because the column is broadcast across rows.  
- `requires_grad=True` on parameters to enable autograd.  
- Use `torch.Generator` with a fixed seed for deterministic sampling.  

---  

## 5ï¸âƒ£ Regularisation & Implicit Smoothing  
- **L2 regularisation** (weight decay) on `W`:  
  ```python
  reg = Î» * (W**2).mean()
  loss_total = loss + reg
  ```  
- When `W` â†’ 0, logits become uniform â†’ equivalent to **label smoothing**.  
- Adjust Î» to control the tradeâ€‘off between fitting data and keeping probabilities smooth.  

---  

## 6ï¸âƒ£ Scaling Beyond Bigrams  
### 6.1 Wordâ€‘Level Modeling  
- Extend the same pipeline to **tokens = words** (instead of characters).  
- Larger vocab â†’ larger embedding/linear layers.  

### 6.2 Longer Contexts (nâ€‘grams, RNNs, Transformers)  
- Feed **multiple previous characters** (or embeddings) into deeper networks:  
  - **RNN / LSTM** â†’ hidden state carries history.  
  - **Transformer** â†’ selfâ€‘attention over the whole context.  
- Output layer always produces **logits â†’ softmax â†’ probability distribution** for the next token.  

### 6.3 Why Neural Nets Scale  
- Counting tables would explode (`|V|^k` entries for kâ€‘gram).  
- Parameter sharing in NN (weights) keeps model size **linear** in vocabulary size, not exponential in context length.  

---  

## 7ï¸âƒ£ Future Roadmap (as hinted in the talk)  
1. **Wordâ€‘level language model** â€“ generate full sentences.  
2. **Imageâ€‘text models** â€“ e.g., DALLÂ·E, Stable Diffusion.  
3. **Full transformer implementation** â€“ equivalent to GPTâ€‘2 at character level, then scale up.  

---  

## 8ï¸âƒ£ Quick Reference Cheatâ€‘Sheet  

| Concept | Symbol / Code | Key Insight |
|--------|---------------|-------------|
| **Start token** | `.` (indexâ€¯0) | Marks beginning of a name |
| **End token** | `.` (indexâ€¯0 after collapse) | Marks termination |
| **Bigram count matrix** | `N` (28â€¯Ã—â€¯28) | Raw frequencies |
| **Probability matrix** | `P = N / N.sum(dim=1, keepdim=True)` | Rowâ€‘wise categorical distribution |
| **Oneâ€‘hot encoding** | `x_i = F.one_hot(i, 27).float()` | Turns integer index into NN input |
| **Weight matrix** | `W` (27â€¯Ã—â€¯27) | Learns logâ€‘counts (logits) |
| **Softmax** | `torch.softmax(logits, dim=1)` | Turns logits â†’ probabilities |
| **Negative Logâ€‘Likelihood** | `loss = -log(p_true).mean()` | Optimisation objective |
| **Gradient step** | `W.data -= lr * W.grad` | Simple SGD update |
| **L2 regularisation** | `Î» * (W**2).mean()` | Encourages smoother (more uniform) predictions |
| **Sampling loop** | `while idx != 0: idx = torch.multinomial(P[idx], 1)` | Generates a new name |

---  

### ğŸ‰ Takeâ€‘away  
- **Counting bigrams** gives a perfect baseline (NLL â‰ˆâ€¯2.4).  
- **Training the same model with gradient descent** reproduces the baseline *and* provides a flexible foundation for more powerful architectures (RNNs, Transformers).  
- Understanding **tensor shapes, broadcasting, and autograd** is essential for scaling up.  

*Happy modeling!* ğŸš€
# Lesson 3

# ğŸ§  Comprehensive Mindâ€‘Map of the â€œMakemoreâ€ Lecture  

*(All points are derived from the transcript.  The hierarchy reflects the logical flow of ideas, concepts, and implementation details.)*  

---  

## 1ï¸âƒ£ Introduction & Recap  
- **Previous lecture**  
  - Implemented a **bigram language model**  
    - Countâ€‘based version â†’ normalized to probabilities (rows sum toâ€¯1)  
    - Simple neural net with a **single linear layer**  
  - **Limitation:** only one previous character â†’ poor â€œnameâ€‘likeâ€ predictions  

- **Problem with extending the count table**  
  - Context length *k* â†’ table size grows **exponentially** (`27^k` for characters)  
  - Too many rows â†’ insufficient counts â†’ model â€œexplodesâ€  

---  

## 2ï¸âƒ£ Moving to a Multiâ€‘Layer Perceptron (MLP)  
- **Goal:** Predict next character using **multiple previous characters** as context.  
- **Reference paper:** *Bengio etâ€¯al., 2003* (wordâ€‘level, but ideas transfer).  

### 2.1 Core Idea from the Paper  
- **Word embeddings:** each word â†’ 30â€‘dimensional vector (random init, learned).  
- **Neural net:**  
  1. **Embedding lookup** â†’ concatenate embeddings of previous *n* words.  
  2. **Hidden layer** (size = hyperâ€‘parameter).  
  3. **Linear output layer** â†’ logits for all possible next tokens.  
  4. **Softmax** â†’ probability distribution.  
- **Training objective:** Maximize logâ€‘likelihood (same as crossâ€‘entropy).  

### 2.2 Adapting to Characters  
- Vocabulary = **27 characters** (aâ€‘z + â€œ.â€).  
- Embedding dimension initially **2** (for easy visualization).  
- Context length (block size) = **3** characters (can be changed).  

---  

## 3ï¸âƒ£ Implementation Details (PyTorch)  

### 3.1 Data Preparation  
- **Build dataset** (`x`, `y`):  
  - `x` = list of *blockâ€‘size* integer indices (context).  
  - `y` = integer index of the next character.  
  - Pad with zeros for the first *blockâ€‘size* positions.  
- Example (word â€œemmaâ€):  
  - Context `[0,0, e] â†’ label m`, `[0, e, m] â†’ label m`, â€¦  

### 3.2 Embedding Lookup (`C`)  
- `C` shape: **27 Ã— 2** (rows = characters, cols = embedding dim).  
- Two equivalent ways to embed an index `i`:  
  1. Direct indexing: `C[i]`.  
  2. Oneâ€‘hot â†’ matrix multiplication: `one_hot(i) @ C`.  
- For a batch `x` (shape `B Ã— 3`): `C[x]` â†’ **B Ã— 3 Ã— 2** tensor.  

### 3.3 Flattening the Context  
- Need shape **B Ã— (3â€¯Ã—â€¯2) = B Ã— 6** to feed the hidden layer.  
- **Methods:**  
  - `torch.cat([c0, c1, c2], dim=1)` (naÃ¯ve, not generic).  
  - `torch.unbind(x, dim=1)` â†’ tuple of tensors â†’ `torch.cat(..., dim=1)`.  
  - **Best:** `C[x].view(B, -1)` (uses `view` â†’ no extra memory).  

### 3.4 Hidden Layer  
- Weight matrix `W1`: **6 Ã— H** (H = hidden size, e.g., 100).  
- Bias `b1`: **H**.  
- Activation: **tanh** (`torch.tanh`).  

### 3.5 Output Layer  
- Weight matrix `W2`: **H Ã— 27**.  
- Bias `b2`: **27**.  
- Logits: `h @ W2 + b2` â†’ shape **B Ã— 27**.  

### 3.6 Loss Computation  
- **Manual:**  
  - `logits.exp()` â†’ â€œcountsâ€.  
  - Normalize â†’ probabilities.  
  - Pick probability of true class â†’ `-log(p_true)`.  
- **Preferred:** `torch.nn.functional.cross_entropy(logits, y)`  
  - Faster (fused kernels).  
  - Numerically stable (logâ€‘softmax internally).  

### 3.7 Training Loop (Core Steps)  
1. Zero grads: `p.grad = None` for each parameter.  
2. Forward pass â†’ loss.  
3. `loss.backward()` â†’ gradients.  
4. Parameter update: `p -= lr * p.grad`.  

### 3.8 Miniâ€‘Batch Training  
- **Why:** Full dataset (~228â€¯k examples) â†’ too slow.  
- **How:**  
  - Sample random indices `ix = torch.randint(0, N, (batch_size,))`.  
  - Use `x[ix]`, `y[ix]` for each iteration.  
- **Effect:** Noisy gradient â†’ need more steps, but far faster.  

---  

## 4ï¸âƒ£ Hyperâ€‘Parameter Exploration  

| Hyperâ€‘parameter | Description | Typical Values (used) |
|----------------|-------------|----------------------|
| `block_size`   | Number of previous characters | 3 (tried 4,â€¯5,â€¯10) |
| `embed_dim`    | Dimensionality of character embeddings | 2 (visual), 10 (better) |
| `hidden_size`  | Neurons in hidden layer | 100 â†’ 200 â†’ 300 |
| `lr` (learning rate) | Step size for SGD | 0.1 (good), 0.01 (fineâ€‘tune), 0.001 (slow) |
| `batch_size`   | Miniâ€‘batch size | 32 (default), can increase |
| `num_steps`    | Training iterations | 10â€¯k â†’ 200â€¯k (long runs) |
| `lr_decay`     | Reduce LR after N steps | Ã—0.1 after 100â€¯k steps |

### 4.1 Learningâ€‘Rate Search (Practical Trick)  
- Sweep **logâ€‘space**: `lr_exps = torch.linspace(-3, 0, steps=1000)` â†’ `lrs = 10**lr_exps`.  
- Run a few steps for each LR, record loss â†’ plot **LR vs. loss**.  
- Choose LR in the â€œvalleyâ€ (e.g., `10â»Â¹ = 0.1`).  

### 4.2 Overâ€‘/Underâ€‘Fitting Diagnosis  
- **Training loss â‰ˆ validation loss** â†’ **underâ€‘fitting** (model too small).  
- **Training loss << validation loss** â†’ **overâ€‘fitting** (model too large).  
- Adjust hidden size, embed dim, or regularization accordingly.  

---  

## 5ï¸âƒ£ Data Splits & Evaluation  

1. **Training set** â€“ ~80â€¯% of words (â‰ˆâ€¯25â€¯k examples).  
2. **Dev/validation set** â€“ ~10â€¯% (â‰ˆâ€¯3â€¯k examples).  
3. **Test set** â€“ remaining ~10â€¯% (â‰ˆâ€¯2â€¯k examples).  

- **Training** uses only the training split.  
- **Hyperâ€‘parameter tuning** uses the dev set.  
- **Final performance** reported on the test set **once**.  

---  

## 6ï¸âƒ£ Embedding Visualization (2â€‘D case)  

- After training with `embed_dim = 2`, plot each character:  
  - `x = C[:,0]`, `y = C[:,1]`.  
  - Annotate with the character symbol.  
- Observations:  
  - Vowels cluster together â†’ network learns similarity.  
  - Rare symbols (e.g., â€œqâ€, â€œ.â€) occupy distinct regions.  

*When `embed_dim` >â€¯2, direct 2â€‘D plot isnâ€™t possible; consider PCA/tâ€‘SNE.*  

---  

## 7ï¸âƒ£ Sampling from the Trained Model  

1. **Initialize context** with three â€œ.â€ (or any start token).  
2. Loop:  
   - Embed current context â†’ hidden state â†’ logits.  
   - `prob = torch.softmax(logits, dim=-1)`.  
   - Sample next token: `next_idx = torch.multinomial(prob, 1)`.  
   - Shift context window, append `next_idx`.  
3. Convert indices back to characters â†’ generated string.  

- Generated examples look **more nameâ€‘like** (e.g., â€œham joesâ€, â€œemilyâ€).  

---  

## 8ï¸âƒ£ Practical Tips & Extras  

- **Tensor indexing tricks** (list, 1â€‘D tensor, multiâ€‘dim tensor) â†’ `C[x]`.  
- **`view` vs. `reshape`** â€“ `view` is a *noâ€‘copy* operation (fast).  
- **Broadcasting** â€“ Adding bias `b1` to hidden activations works automatically (`BÃ—H` + `H`).  
- **Avoid hardâ€‘coding** magic numbers; use variables (`block_size`, `embed_dim`).  
- **Googleâ€¯Colab** â€“ Readyâ€‘toâ€‘run notebook, no local install needed (link provided in video).  

---  

## 9ï¸âƒ£ Takeâ€‘aways & Next Steps  

- **Achieved**: Loss â‰ˆâ€¯2.17 (better than bigram â‰ˆâ€¯2.45).  
- **Open knobs for improvement**:  
  - Increase hidden size / embedding dimension.  
  - Use longer context (`block_size`).  
  - Experiment with different optimizers (Adam, RMSprop).  
  - Add regularization (weight decay, dropout).  
  - Train longer with proper learningâ€‘rate schedule.  
- **Read the paper** (Bengioâ€¯etâ€¯al., 2003) for deeper insights & advanced ideas.  

---  

### ğŸ“Œ Quick Reference (Pseudoâ€‘code)

```python
# 1. Build dataset
x, y = build_dataset(words, block_size=3)   # x: (N,3), y: (N,)

# 2. Model components
C   = torch.randn(27, embed_dim, requires_grad=True)   # embedding table
W1  = torch.randn(3*embed_dim, hidden, requires_grad=True)
b1  = torch.randn(hidden, requires_grad=True)
W2  = torch.randn(hidden, 27, requires_grad=True)
b2  = torch.randn(27, requires_grad=True)

# 3. Forward pass (batch)
def forward(x_batch):
    e = C[x_batch]                # (B,3,embed_dim)
    e = e.view(e.shape[0], -1)    # (B,3*embed_dim)
    h = torch.tanh(e @ W1 + b1)   # (B,hidden)
    logits = h @ W2 + b2          # (B,27)
    return logits

# 4. Training loop (miniâ€‘batch)
for step in range(num_steps):
    ix = torch.randint(0, N, (batch_size,))
    logits = forward(x[ix])
    loss   = F.cross_entropy(logits, y[ix])
    loss.backward()
    for p in [C,W1,b1,W2,b2]:
        p.data -= lr * p.grad
        p.grad.zero_()
```

---  

*End of mindâ€‘map.*  
# Lesson 4

# ğŸ§  Mindâ€‘Map of the Lecture  

*(All points are derived from the transcript.  The hierarchy shows the logical flow and relationships between concepts.)*  

---  

## 1ï¸âƒ£  Recap:  Multiâ€‘Layer Perceptron (MLP) for characterâ€‘level language modelling  
- Implemented following **Benj 2003** (MLP â†’ nextâ€‘character prediction).  
- **Current status**  
  - 11â€¯k parameters, 200â€¯k training steps, batchâ€‘sizeâ€¯=â€¯32.  
  - Training/validation loss â‰ˆâ€¯2.16.  
  - Sampling produces readable but imperfect words.  

---  

## 2ï¸âƒ£  Why look deeper?  
- Goal: move to **recurrent / LSTM / GRU** networks.  
- **Prerequisite:** solid intuition of **activations** & **gradients** during training.  
- Understanding these dynamics explains why RNNs are **hard to optimise** with plain firstâ€‘order methods.  

---  

## 3ï¸âƒ£  Problem #1 â€“ Bad Initialisation of the MLP  

### 3.1  Observed symptom  
- **Loss at iterationâ€¯0:**â€¯27â€¯â†’â€¯much higher than expected.  

### 3.2  Expected loss for a uniform softmax  
- 27 possible next characters â†’ uniform probability = 1/27.  
- Negativeâ€‘logâ€‘likelihood = `-log(1/27) â‰ˆ 3.29`.  

### 3.3  What went wrong?  
| Issue | Effect |
|------|--------|
| **Logits far from 0** (extreme values) | Softmax becomes **overâ€‘confident** â†’ huge loss. |
| **Random bias `bâ‚‚`** | Adds a constant offset â†’ pushes logits away from 0. |
| **Weight scale too large** (`Wâ‚‚`) | Amplifies the offset, further saturating softmax. |

### 3.4  Fixes applied  
1. **Zero the output bias** (`bâ‚‚ = 0`).  
2. **Scale down `Wâ‚‚`** (multiply by 0.1 â†’ 0.01).  
3. Keep a tiny nonâ€‘zero variance (e.g., 0.01) for **symmetry breaking**.  

Result: loss curve loses the â€œhockeyâ€‘stickâ€ shape; training becomes more productive.  

---  

## 4ï¸âƒ£  Problem #2 â€“ Saturatedâ€¯`tanh` (ğ‘¡ğ‘ğ‘›â„) activations  

### 4.1  Observation  
- Histogram of hiddenâ€‘state `H` after `tanh` shows **most values at Â±1**.  
- Preâ€‘activations (input to `tanh`) range roughly **â€‘5 â€¦ 15** â†’ many neurons in the **flat tails**.  

### 4.2  Consequence for backâ€‘propagation  
- Derivative of `tanh` = `1 â€“ tÂ²`.  
- When `t â‰ˆ Â±1`, derivative â‰ˆâ€¯0 â†’ **gradient vanishes** for those neurons.  
- â€œDead neuronsâ€ (always saturated) never learn (gradient = 0).  

### 4.3  Diagnostic check  
- Compute **percentage of units with |t|â€¯>â€¯0.99** â†’ large white area in Boolean mask â†’ many dead neurons.  

### 4.4  Remedy  
- Reduce magnitude of preâ€‘activations:  
  - **Scale down the firstâ€‘layer weights** (`Wâ‚`) (e.g., multiply by 0.1).  
  - Optionally **bias = 0** (biases become useless after batchâ€‘norm, see Â§6).  
- Result: hidden activations become **roughly Gaussian (â‰ˆâ€¯ğ’©(0,1))**, gradients stay alive.  

---  

## 5ï¸âƒ£  General Weightâ€‘Initialisation Theory  

### 5.1  Fanâ€‘in / Fanâ€‘out concept  
- For a layer with `fan_in` inputs, initialise weights with variance `1 / fan_in`.  
- Guarantees that **output variance â‰ˆâ€¯input variance** (preserves a unitâ€‘Gaussian flow).  

### 5.2  Gains for different nonâ€‘linearities (He / Xavier)  
| Nonâ€‘linearity | Recommended gain `g` | Reason |
|---------------|----------------------|--------|
| Linear / Identity | `1` | No contraction. |
| ReLU / Leakyâ€‘ReLU | `âˆš2` | Half the distribution is zeroed. |
| tanh | `5/3` (â‰ˆâ€¯1.67) | Empirically balances contraction of tanh. |
| (others) | derived from variance analysis | â€“ |

- **Final weight scale** = `gain / âˆšfan_in`.  

---  

## 6ï¸âƒ£  Batch Normalisation (BatchNorm) â€“ The â€œGameâ€‘Changerâ€  

### 6.1  Core idea  
1. **Collect batch statistics** â†’ mean `Î¼_B` and variance `ÏƒÂ²_B`.  
2. **Standardise**: `xÌ‚ = (x â€“ Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)`.  
3. **Learnable affine transform**: `y = Î³Â·xÌ‚ + Î²`.  

### 6.2  Why it works  
- Forces **activations to stay unitâ€‘Gaussian** throughout the network â†’ prevents saturation / vanishing gradients.  
- Acts as a **regulariser** (batchâ€‘wise noise).  

### 6.3  Training vs. Inference  
- **Training:** use batch statistics (`Î¼_B`, `ÏƒÂ²_B`).  
- **Inference:** use **running estimates** (`Î¼Ì‚`, `ÏƒÌ‚Â²`) updated with exponential moving average:  

  `running_mean = momentumÂ·running_mean + (1â€‘momentum)Â·Î¼_B`  

  `running_var  = momentumÂ·running_var  + (1â€‘momentum)Â·ÏƒÂ²_B`  

- `momentum` â‰ˆâ€¯0.1 for large batches; smaller batches may need **lower momentum** (e.g.,â€¯0.001).  

### 6.4  Practical notes  
- **Bias before BatchNorm is useless** â€“ BatchNorm already learns a bias (`Î²`).  
- Set `affine=True` (learnable `Î³`, `Î²`).  
- `eps` (defaultâ€¯1eâ€‘5) avoids divisionâ€‘byâ€‘zero.  
- **No gradient tracking** for running stats (`torch.no_grad()` context).  

---  

## 7ï¸âƒ£  Diagnostic Toolbox for Neuralâ€‘Net Health  

| Diagnostic | What it tells you | Typical â€œgoodâ€ range |
|------------|-------------------|---------------------|
| **Activation histogram** (per layer) | Distribution shape, saturation % | Stdâ€¯â‰ˆâ€¯1, saturationâ€¯<â€¯5â€¯% |
| **Gradient histogram** (per layer) | Gradient magnitude, vanishing/exploding | Similar scale to activations |
| **Weight histogram** | Parameter spread, dead weights | Stdâ€¯â‰ˆâ€¯1 (or as per init) |
| **Updateâ€‘toâ€‘Data ratio** `â€–Î”Î¸â€– / â€–Î¸â€–` (logâ‚â‚€) | Relative step size per iteration | â‰ˆâ€¯â€‘3 (i.e., updates â‰ˆâ€¯0.001â€¯Ã—â€¯parameter) |
| **Learningâ€‘rate sanity check** | If ratio â‰ªâ€¯â€‘3 â†’ LR too low; â‰«â€¯â€‘2 â†’ LR too high | Aim for â€“3â€¯Â±â€¯0.5 |
| **Runningâ€‘mean / var convergence** | BatchNorm stats stabilise? | Small drift after a few epochs |

- Plot these **over training time** (not just a single snapshot) to see trends.  

---  

## 8ï¸âƒ£  Putting It All Together â€“ â€œTorchâ€‘ifyâ€ the Code  

### 8.1  Modular design (mirrors `torch.nn`)  
- **Linear layer** â†’ `nn.Linear(in_features, out_features, bias=False)` (bias omitted when followed by BatchNorm).  
- **BatchNorm1d** â†’ `nn.BatchNorm1d(num_features, eps=1eâ€‘5, momentum=0.001, affine=True)`.  
- **tanh activation** â†’ custom wrapper (or `nn.Tanh`).  

### 8.2  Network construction pattern  

```text
Embedding â†’ Linear â†’ BatchNorm â†’ tanh â†’ Linear â†’ BatchNorm â†’ tanh â†’ â€¦ â†’ Linear â†’ BatchNorm â†’ Softmax
```

- **BatchNorm placed after each Linear, before tanh** (standard practice).  
- Can also be placed after tanh â€“ results are similar.  

### 8.3  Training loop (highâ€‘level)  

1. **Zero grads**.  
2. **Forward pass** (collect activations, apply BatchNorm).  
3. **Compute loss** (crossâ€‘entropy).  
4. **Backward** (`loss.backward()`).  
5. **Optimizer step** (SGD / Adam).  
6. **Update running stats** (handled automatically by `nn.BatchNorm`).  

---  

## 9ï¸âƒ£  Takeâ€‘aways & Outlook  

1. **Initialisation matters** â€“ scaling weights & zeroing biases prevents early â€œoverâ€‘confidenceâ€.  
2. **tanh saturation kills gradients** â€“ keep preâ€‘activations near zero (via weight scaling).  
3. **BatchNorm stabilises deep nets** by constantly reâ€‘Gaussianising activations; it also reduces sensitivity to exact gain choices.  
4. **Diagnostic visualisations** (histograms, updateâ€‘toâ€‘data ratios) are essential for spotting dead neurons, exploding/vanishing gradients, and misâ€‘scaled learning rates.  
5. **Future work**  
   - Move to **recurrent architectures** (RNN, LSTM, GRU) â€“ deeper unrolled graphs will amplify the issues we just mitigated.  
   - Explore **alternative normalisation** (LayerNorm, GroupNorm) that avoid batch coupling.  
   - Leverage **advanced optimisers** (Adam, RMSProp) and **residual connections** for even deeper models.  

---  

### ğŸ“Œ  Quickâ€‘Reference Cheatâ€‘Sheet  

| Concept | Formula / Setting | Typical Value |
|---------|-------------------|---------------|
| Weight init variance | `var = gainÂ² / fan_in` | `gain = 1` (linear), `âˆš2` (ReLU), `5/3` (tanh) |
| Softmax uniform loss | `-log(1/ğ‘˜)` | `k = vocab size` |
| tanh derivative | `1 â€“ tÂ²` | â†’ 0 when `|t| â†’ 1` |
| BatchNorm scaling | `Î³` (learned) | Initialise to `1` |
| BatchNorm shift | `Î²` (learned) | Initialise to `0` |
| Updateâ€‘toâ€‘Data logâ‚â‚€ target | `â‰ˆâ€¯â€‘3` | Adjust LR accordingly |
| Momentum for running stats | `0.1` (large batch) / `0.001` (batchâ€¯=â€¯32) | â€“ |
| Îµ (epsilon) in BN | `1eâ€‘5` | â€“ |

---  

*End of mindâ€‘map.*  
# Lesson 5

# ğŸ§  Mindmap â€“ Manual Backâ€‘Propagation for a Twoâ€‘Layer MLP  

---  

## 1ï¸âƒ£ Overview  
- **Goal:** Replace `loss.backward()` with a fully manual backward pass (tensorâ€‘level).  
- **Why?**  
  - Understand the internals of autograd.  
  - Debug subtle bugs (gradient clipping, dead neurons, exploding/vanishing gradients).  
  - Gain intuition about how gradients flow through each operation.  

---  

## 2ï¸âƒ£ Historical Context  
- **~2006 â€“ 2010:**  
  - Researchers (e.g., Hinton & Salakhutdinov) wrote their own backâ€‘prop in MATLAB/NumPy.  
  - Manual gradient computation was the norm.  
- **2014:**  
  - Authorâ€™s â€œFragmented Embeddingsâ€ paper â€“ full manual forwardâ€¯+â€¯backward passes in NumPy.  
- **Today:**  
  - Autograd is standard, but the exercise remains valuable for learning.  

---  

## 3ï¸âƒ£ Network Architecture (forward pass)  

```
Embedding â†’ Linear1 (W1, B1) â†’ 10â€‘H (tanh) â†’ BatchNorm (Î³, Î²) â†’ Linear2 (W2, B2) â†’ Logits â†’ Softmax â†’ Crossâ€‘Entropy
```

| Layer | Shape (batchâ€¯=â€¯32) | Key tensors |
|------|-------------------|-------------|
| **Embedding** | 32â€¯Ã—â€¯3â€¯Ã—â€¯10 | `C` (27â€¯Ã—â€¯10) |
| **Linear1** | 32â€¯Ã—â€¯64 | `W1` (64â€¯Ã—â€¯64), `B1` (1â€¯Ã—â€¯64) |
| **10â€‘H** | 32â€¯Ã—â€¯64 | `H = tanh(preact)` |
| **BatchNorm** | 32â€¯Ã—â€¯64 | `Î¼` (1â€¯Ã—â€¯64), `ÏƒÂ²` (1â€¯Ã—â€¯64), `Î³`, `Î²` |
| **Linear2** | 32â€¯Ã—â€¯27 | `W2` (27â€¯Ã—â€¯64), `B2` (1â€¯Ã—â€¯27) |
| **Logits** | 32â€¯Ã—â€¯27 | `logits` |
| **Softmax** | 32â€¯Ã—â€¯27 | `probs` |
| **Loss** | scalar | `loss = -mean(log_probs[range(N), Y])` |

---  

## 4ï¸âƒ£ Manual Backâ€‘Propagation â€“ Core Concepts  

### 4.1 Gradient of the Loss w.r.t. `log_probs` (`d_log_probs`)  
- **Shape:** 32â€¯Ã—â€¯27 (same as `log_probs`).  
- **Derivation:**  
  - `loss = -(1/N) Î£_i log_probs[i, Y[i]]`  
  - `âˆ‚loss/âˆ‚log_probs[i, j] = -1/N` if `j == Y[i]`, else `0`.  
- **Implementation:**  
  ```python
  d_log_probs = torch.zeros_like(log_probs)
  d_log_probs[torch.arange(N), Y] = -1.0 / N
  ```

### 4.2 Backâ€‘prop through `log` â†’ `probs`  
- `log_probs = torch.log(probs)`  
- Local derivative: `âˆ‚log/âˆ‚probs = 1 / probs` (elementâ€‘wise).  
- Chain rule: `d_probs = d_log_probs / probs`.

### 4.3 Backâ€‘prop through Softmax (logits â†’ probs)  
- **Softmax formula:** `p_i = exp(l_i) / Î£_j exp(l_j)`.  
- **Gradient (batch version):**  
  ```python
  probs = torch.softmax(logits, dim=1)
  d_logits = probs.clone()
  d_logits[torch.arange(N), Y] -= 1   # subtract 1 at correct class
  d_logits /= N                        # average over batch
  ```
- **Intuition:**  
  - Pull up probability of the correct class, push down all others.  
  - Sum of each row of `d_logits` = 0 (conservation of probability).

### 4.4 Linear Layer (W2, B2)  
- Forward: `logits = H @ W2.T + B2`.  
- Gradients:  
  - `d_W2 = d_logits.T @ H` (shape 27â€¯Ã—â€¯64).  
  - `d_B2 = d_logits.sum(dim=0, keepdim=True)`.  
  - `d_H  = d_logits @ W2`.

### 4.5 Batch Normalization  
- **Forward (simplified, Î³â€¯=â€¯1, Î²â€¯=â€¯0):**  
  ```
  Î¼   = mean(H, dim=0)                # 1Ã—64
  ÏƒÂ²  = var(H, dim=0, unbiased=False) # 1Ã—64
  HÌ‚   = (H - Î¼) / sqrt(ÏƒÂ² + Îµ)
  ```
- **Backward (key steps):**  
  1. `d_HÌ‚ = d_H_pre` (gradient from next layer).  
  2. `d_ÏƒÂ² = -0.5 * (d_HÌ‚ * (H-Î¼)) * (ÏƒÂ²+Îµ)^(-3/2)` â†’ sum over batch.  
  3. `d_Î¼  = -d_HÌ‚ / sqrt(ÏƒÂ²+Îµ) - 2 * d_ÏƒÂ² * (H-Î¼) / N`.  
  4. `d_H  = d_HÌ‚ / sqrt(ÏƒÂ²+Îµ) + d_ÏƒÂ² * 2*(H-Î¼)/N + d_Î¼ / N`.  
  5. `d_Î³ = (d_H_pre * HÌ‚).sum(dim=0, keepdim=True)` (if Î³ kept).  
  6. `d_Î² = d_H_pre.sum(dim=0, keepdim=True)` (if Î² kept).  

- **Broadcasting rule:**  
  - When a scalar (e.g., `Î¼`) is broadcast to a matrix, the backward pass **sums** the incoming gradients over the broadcasted dimension.

### 4.6 Activation `10â€‘H` (tanh)  
- Forward: `H = tanh(preact)`.  
- Local derivative: `1 - HÂ²`.  
- Backward: `d_preact = d_H * (1 - H**2)`.

### 4.7 Linear Layer (W1, B1)  
- Same pattern as W2/B2, but with `preact = X @ W1.T + B1`.  
- Gradients:  
  - `d_W1 = d_preact.T @ X`.  
  - `d_B1 = d_preact.sum(dim=0, keepdim=True)`.  
  - `d_X  = d_preact @ W1`.

### 4.8 Embedding Lookup (indexing)  
- Forward: `M[i, k, :] = C[Y[i, k]]`.  
- Backward:  
  ```python
  d_C = torch.zeros_like(C)
  for i in range(N):
      for k in range(3):
          idx = Y[i, k]
          d_C[idx] += d_M[i, k]   # accumulate if same idx appears multiple times
  ```

---  

## 5ï¸âƒ£ Exercises (Progressive Refactoring)  

| Exercise | Whatâ€™s changed | Key takeaway |
|----------|----------------|--------------|
| **1** | Compute every intermediate `d_â€¦` tensor (as above). Verify with `torch.allclose`. | Manual gradients match autograd when shapes & broadcasting are handled correctly. |
| **2** | Derive a **single** analytic expression for `d_logits` (softmaxâ€¯+â€¯crossâ€‘entropy). Implement it in one line. | Much faster forwardâ€¯+â€¯backward; shows that many intermediate ops can be collapsed. |
| **3** | Derive a compact formula for **batchâ€‘norm** backward (see Â§4.5). Implement the whole layer in a few lines. | Highlights the â€œsumâ€‘overâ€‘broadcastâ€ pattern; avoids perâ€‘element code. |
| **4** | Assemble all manual pieces into a full training loop (no `loss.backward()`). | Endâ€‘toâ€‘end manual training yields the same loss & samples as the autograd version. |

---  

## 6ï¸âƒ£ Intuitive Insights  

- **Gradient â€œpushâ€‘pullâ€** on logits:  
  - Correct class gets a **negative** gradient (pull up).  
  - Incorrect classes get a **positive** gradient (push down).  
  - Rowâ€‘wise sum =â€¯0 â†’ probability mass conserved.  

- **Batchâ€‘norm variance bias vs. unbiased:**  
  - Training often uses **biased** estimator (`1/N`).  
  - Inference (running stats) should use **unbiased** (`1/(Nâ€‘1)`).  
  - Mismatch can be a subtle bug; the author prefers the unbiased version throughout.  

- **Broadcast â†” Sum Duality:**  
  - **Forward:** broadcasting replicates a smaller tensor across a larger one.  
  - **Backward:** the gradient w.r.t. the broadcasted tensor is the **sum** of the replicated gradients.  

---  

## 7ï¸âƒ£ Next Steps  

- **Recurrent Neural Networks (RNNs) & LSTMs** â€“ extend manual backâ€‘prop to timeâ€‘unrolled architectures.  
- Explore **gradient clipping**, **weight tying**, and **teacher forcing** with manual gradients.  

---  

### ğŸ“Œ TL;DR  

1. Replace `loss.backward()` with explicit tensorâ€‘level derivatives.  
2. Derive and implement compact formulas for softmaxâ€‘crossâ€‘entropy and batchâ€‘norm.  
3. Verify each step against PyTorchâ€™s autograd.  
4. Assemble a full training loop that runs as fast as the autograd version while giving you full visibility into every gradient flow.  

Happy hacking! ğŸš€  
# Lesson 6

## ğŸ“š Mindâ€‘Map of the Lecture  
*(Markdown + Mermaid diagram for quick visualisation)*  

---  

### 1ï¸âƒ£  Overview  
- **Goal** â€“ Extend a simple characterâ€‘level MLP language model into a deeper, hierarchical architecture (Wavenetâ€‘style).  
- **Context** â€“ Lecture recorded in a hotel room in Kyoto; continuation of previous parts (3â€¯&â€¯4).  

### 2ï¸âƒ£  Data & Baseline Model  
- **Dataset** â€“ 182â€¯000 examples, each: 3â€‘character context â†’ predict 4th character.  
- **Baseline Architecture**  
  - Embedding table (`C`) â†’ Linear â†’ BatchNorm (named *bathroom*) â†’ 1Dâ€‘BatchNorm (`10h`) â†’ Linear output.  
  - 12â€¯000 parameters, validation loss â‰ˆâ€¯2.10.  
- **Observations**  
  - Model already generates plausible â€œnameâ€‘likeâ€ strings.  
  - Too much information is squashed in a single hidden layer.  

### 3ï¸âƒ£  Desired Architectural Changes  
- **Take more context** â€“ increase block size from 3 â†’ 8 (later 16).  
- **Hierarchical fusion** â€“ progressively combine neighboring characters (bigrams â†’ 4â€‘grams â†’ â€¦) instead of flattening all at once.  
- **Wavenet inspiration** â€“ dilated causal convolutions â†’ treeâ€‘like receptive field growth.  

### 4ï¸âƒ£  Refactoring the Code  
#### 4.1  Layer Building Blocks  
- **Linear layer** â€“ simple matrix multiply (mirrors `torch.nn.Linear`).  
- **BatchNorm (bathroom)** â€“ maintains running mean/variance, behaves differently in train vs. eval.  
- **Embedding layer** â€“ lookup table (`nn.Embedding`).  
- **Flatten / â€œFlattenConsecutiveâ€** â€“ custom module to reshape tensors, now able to group *n* consecutive embeddings.  

#### 4.2  Containers  
- **Sequential container** â€“ custom implementation that stores a list of layers and forwards input through them.  
- **Model definition** â€“ `model = Sequential([Embedding, FlattenConsecutive, Linear, BatchNorm, â€¦])`.  

#### 4.3  Debugging & Shapeâ€‘Gymnastics  
- Inspected tensor shapes after each layer (e.g., `BÃ—TÃ—C â†’ BÃ—TÃ—E â†’ BÃ—(TÂ·E)`).  
- Realised flattening to `BÃ—â€‘1` was too aggressive; needed a 3â€‘D view (`BÃ—groupsÃ—(nÂ·E)`).  

### 5ï¸âƒ£  Implementing Hierarchical Fusion  
1. **FlattenConsecutive(n=2)** â€“ groups every 2 consecutive characters â†’ shape `BÃ—(T/2)Ã—(2Â·E)`.  
2. **Stacked linear layers** â€“ each layer processes the grouped embeddings, progressively increasing receptive field.  
3. **Resulting network** â€“ 3 hidden layers, each widening the context (2 â†’ 4 â†’ 8 characters).  

### 6ï¸âƒ£  BatchNorm Bug & Fix  
- **Problem** â€“ `BatchNorm1D` computed statistics over only the first dimension (`B`) â†’ produced perâ€‘position means/variances.  
- **Fix** â€“ Reduce over dimensions `(0,â€¯1)` when input is 3â€‘D, yielding a single mean/variance per channel (`1Ã—1Ã—C`).  
- **Outcome** â€“ More stable statistics, slight validation loss improvement (2.029 â†’ 2.022).  

### 7ï¸âƒ£  Training Results & Hyperâ€‘parameter Tweaks  
| Change | Params | Validation loss |
|--------|--------|-----------------|
| Baseline (3â€‘char) | ~12â€¯k | 2.10 |
| â†‘ Context to 8 chars (flat) | +10â€¯k | 2.02 |
| Hierarchical (3â€‘layer) | ~22â€¯k | 2.029 â†’ 2.022 |
| â†‘ Embedding dim to 24, hidden units â†‘ | ~76â€¯k | **1.99** (first subâ€‘2.0) |
- **Takeaway** â€“ Bigger capacity helps, but training becomes slower; still no systematic hyperâ€‘parameter search.  

### 8ï¸âƒ£  Relation to Convolutional Networks (Wavenet)  
- **Current implementation** â€“ Explicit â€œforâ€‘loopâ€ over each position (inefficient).  
- **Convolutional view** â€“ Same linear filters applied via dilated causal convolutions â†’ parallel GPU kernels, reuse of intermediate results.  
- **Future work** â€“ Replace explicit loops with `nn.Conv1d` (dilated, causal), add gated activations, residual & skip connections.  

### 9ï¸âƒ£  Development Process Insights  
- **Documentation pain** â€“ PyTorch docs are good; the courseâ€™s own â€œPatreonâ€ docs are sparse/inaccurate.  
- **Shape gymnastics** â€“ Constantly checking NCL vs. NLC ordering, using `view`, `reshape`, `permute`.  
- **Prototyping workflow**  
  1. **Jupyter notebook** â€“ rapid testing, shape inspection, debugging.  
  2. **Copyâ€‘paste to VSâ€¯Code repo** â€“ clean module code.  
  3. **Run experiments** via scripts (future: experiment harness).  

### ğŸ”Ÿ  Future Directions (Open Topics)  
1. **Implement true dilated causal convolutions** (Wavenet).  
2. **Add gated linear units, residual & skip connections**.  
3. **Build an experimental harness** â€“ systematic hyperâ€‘parameter sweeps, logging, early stopping.  
4. **Explore other architectures** â€“ RNNs, LSTMs, GRUs, Transformers.  
5. **Beat the current best loss (â‰ˆâ€¯1.99)** â€“ try different channel allocations, embedding sizes, initialization schemes, optimizers.  

---  

## ğŸ§­ Mermaid Mindâ€‘Map (copyâ€‘paste into a Mermaidâ€‘enabled markdown viewer)

```mermaid
mindmap
  root((Characterâ€‘Level Language Model))
    Overview
      Goal
      Context
    Data & Baseline
      Dataset
      Baseline Architecture
        Embedding
        Linear
        BatchNorm (bathroom)
        1Dâ€‘BatchNorm (10h)
        Linear Output
      Baseline Metrics
    Desired Changes
      Larger Context (3â†’8â†’16)
      Hierarchical Fusion
        Bigrams â†’ 4â€‘grams â†’ â€¦
      Wavenet Inspiration
    Refactoring
      Layer Building Blocks
        Linear
        BatchNorm
        Embedding
        FlattenConsecutive
      Containers
        Sequential
      Debugging Shapes
    Hierarchical Fusion Implementation
      FlattenConsecutive(n=2)
      Stacked Linear Layers
      Resulting 3â€‘layer Net
    BatchNorm Bug
      Problem (perâ€‘position stats)
      Fix (reduce over 0,1)
      Outcome
    Training Results
      Table of Changes â†’ Params â†’ Validation loss
      Observations
    Convolutional Relation
      Current explicit loops
      Convolutional view (dilated causal)
      Future: Conv1d + residual/skip
    Development Process
      Documentation challenges
      Shape gymnastics
      Prototyping workflow (Jupyter â†’ VSCode)
    Future Directions
      Dilated convolutions
      Gated units, residuals, skips
      Experiment harness
      RNN/LSTM/Transformer exploration
      Beat loss 1.99
```

*Render the diagram with any Mermaidâ€‘compatible markdown viewer (e.g., VSâ€¯Code, GitHub, HackMD).*  

---  

**TL;DR:**  
We started from a simple 3â€‘character MLP, expanded the context, introduced a hierarchical â€œflattenâ€‘consecutiveâ€ module, fixed a subtle BatchNorm bug, and built a deeper 3â€‘layer network that already beats the 2.0 validationâ€‘loss barrier. The next steps are to replace the explicit loops with true dilated causal convolutions, add residual/skip connections, and set up a proper experimental harness for systematic hyperâ€‘parameter search. Happy hacking!
# Lesson 7

# ğŸ§  Mindâ€‘Map of the Transcript  

*(Markdown outline â€“ each level deeper = a more detailed subâ€‘topic)*  

---  

## 1. Introduction & Motivation  
- **ChatGPT** â€“ a textâ€‘based AI that can perform many tasks (write poems, explain HTML, generate news, etc.)  
- **Probabilistic system** â€“ same prompt â†’ different plausible outputs  
- **Goal of the talk** â€“ understand whatâ€™s â€œunder the hoodâ€ of ChatGPT and build a tiny version ourselves  

## 2. Languageâ€‘Model Basics  
- **Definition** â€“ models the sequence of tokens (characters, subâ€‘words, words)  
- **Task** â€“ given a prefix, predict the next token â†’ sequence completion  
- **Tokenization**  
  - *Characterâ€‘level* (used in the demo) â†’ 65â€‘symbol vocab  
  - *Subâ€‘word / BPE* (used by OpenAI) â†’ ~50â€¯k vocab  
  - Encoder â†” Decoder maps between strings â†” integer IDs  

## 3. Data Set â€“ â€œTiny Shakespeareâ€  
- Single ~1â€¯MiB file containing all Shakespeare works  
- Treated as a **single long integer sequence** after tokenization  
- Split: **90â€¯% train**, **10â€¯% validation**  

## 4. Model Architecture â€“ From Simple to Full Transformer  

### 4.1. Simple Baseline: Byteâ€‘Level (BYR) Model  
- Embedding table â†’ directly produces logits for each position  
- Loss = **Crossâ€‘Entropy** (negative logâ€‘likelihood)  

### 4.2. Adding Positional Information  
- Positionalâ€‘embedding matrix (blockâ€‘size Ã— embedâ€‘dim)  
- Token embedding + positional embedding â†’ input **X**  

### 4.3. Selfâ€‘Attention (single head)  
- **Queries (Q)**, **Keys (K)**, **Values (V)** = linear projections of **X**  
- Attention scores = `Q Â· Káµ€ / sqrt(head_dim)`  
- **Masking** â€“ lowerâ€‘triangular mask to prevent future tokens from attending (decoderâ€‘only)  
- Softmax â†’ weighted sum of **V** â†’ output of the head  

### 4.4. Multiâ€‘Head Attention  
- Run several independent heads in parallel (e.g., 4 heads)  
- Concatenate their outputs â†’ same dimension as original embed size  

### 4.5. Feedâ€‘Forward Network (FFN)  
- Linear â†’ GELU (or ReLU) â†’ Linear  
- Hidden dimension = 4â€¯Ã—â€¯embed_dim (as in the original paper)  

### 4.6. Residual (Skip) Connections  
- `X â†’ Selfâ€‘Attention â†’ +X`  
- `X â†’ FFN â†’ +X`  

### 4.7. Layer Normalization  
- Applied **before** each subâ€‘layer (preâ€‘norm formulation)  
- Normalizes across the embedding dimension per token  

### 4.8. Dropout  
- Applied on attention weights, after attention output, and after FFN  

### 4.9. Full Decoderâ€‘Only Block  
```
X â”€â”€â–º LayerNorm â”€â”€â–º Multiâ€‘Head Selfâ€‘Attention â”€â”€â–º Dropout â”€â”€â–º +X
   â”‚                                            â”‚
   â””â”€â–º LayerNorm â”€â”€â–º Feedâ€‘Forward â”€â”€â–º Dropout â”€â”€â–º +X
```

### 4.10. Stacking Blocks  
- Stack **N** identical blocks (e.g., 6 layers) â†’ deep Transformer  

### 4.11. Final Projection  
- LayerNorm â†’ Linear (embed_dim â†’ vocab_size) â†’ logits  

## 5. Training Procedure  
- **Batching** â€“ sample random chunks (blockâ€‘size) â†’ shape B Ã— T  
- **Optimizer** â€“ Adam (often with weightâ€‘decay)  
- **Learning rate** â€“ e.g., 3eâ€‘4 (scaled down for larger models)  
- **Training loop** â€“ forward â†’ loss â†’ backward â†’ optimizer step  
- **Evaluation** â€“ periodic â€œestimate_lossâ€ over several batches (train & val)  

## 6. Scaling Experiments & Results  

| Experiment | Model Size | Block Size | Heads | Embed Dim | Layers | Validation Loss |
|------------|------------|------------|-------|-----------|--------|-----------------|
| BYR (char) | ~10â€¯M params | 8 | 1 | 32 | 1 | ~4.8 |
| Add Selfâ€‘Attention (1 head) | ~10â€¯M | 8 | 1 | 32 | 1 | ~2.4 |
| Multiâ€‘Head (4 heads) | ~10â€¯M | 8 | 4 | 8 each | 1 | ~2.28 |
| + Feedâ€‘Forward (4Ã—) | ~10â€¯M | 8 | 4 | 8 each | 1 | ~2.24 |
| Deep + Residual + LayerNorm | ~10â€¯M | 256 | 6 | 384 | 6 | ~2.08 |
| Deep + LayerNorm (preâ€‘norm) | ~10â€¯M | 256 | 6 | 384 | 6 | ~2.06 |
| Fullâ€‘Scale (64â€‘batch, 256â€‘ctx, 6 heads, 6 layers, dropout 0.2) | ~10â€¯M | 256 | 6 | 384 | 6 | **1.48** |

- **Observation:** Adding attention, multiâ€‘heads, FFN, residuals, layerâ€‘norm, and scaling up context dramatically reduces loss.  
- Generated text becomes more â€œShakespeareâ€‘likeâ€ (still nonsensical at character level).  

## 7. Decoderâ€‘Only vs Encoderâ€‘Decoder  

| Component | Decoderâ€‘Only (GPT) | Encoderâ€‘Decoder (e.g., original â€œAttention is All You Needâ€) |
|-----------|-------------------|-----------------------------------------------------------|
| Masking   | Causal (triangular) â†’ autoregressive generation | No causal mask in encoder; decoder still causal |
| Crossâ€‘Attention | **Absent** (only selfâ€‘attention) | Present â€“ decoder attends to encoder outputs |
| Useâ€‘case  | Unconditioned language modeling / text generation | Conditional generation (e.g., translation) |
| In this demo | Only decoder block â†’ generates Shakespeareâ€‘style text | Not implemented (no encoder, no crossâ€‘attention) |

## 8. Fineâ€‘Tuning & Alignment (ChatGPT)  

1. **Preâ€‘training** â€“ massive corpus (â‰ˆ300â€¯B tokens) â†’ decoderâ€‘only Transformer (e.g., GPTâ€‘3 175â€¯B params)  
2. **Supervised fineâ€‘tuning** â€“ small dataset of *question â†’ answer* pairs (fewâ€‘k examples) to make the model an â€œassistantâ€  
3. **Reward Modeling** â€“ collect multiple model outputs, rank them, train a reward model to predict human preference  
4. **RLHF (Reinforcement Learning from Human Feedback)** â€“ use Proximal Policy Optimization (PPO) to fineâ€‘tune the policy so generated answers score high on the reward model  

- The fineâ€‘tuning stages are **not** publicly released; they require largeâ€‘scale infrastructure.  

## 9. nanogpt Repository (by the presenter)  

- **Two files**: `model.py` (definition of the Transformer) and `train.py` (training loop, checkpointing, distributed support)  
- Mirrors the notebook implementation:  
  - Tokenizer (characterâ€‘level)  
  - Embedding + positional embedding  
  - Multiâ€‘head selfâ€‘attention (batched)  
  - Feedâ€‘forward, residuals, layerâ€‘norm, dropout  
  - Optimizer, learningâ€‘rate schedule, evaluation utilities  

## 10. Takeâ€‘aways & Next Steps  

- **Core idea:** â€œAttention is all you needâ€ â†’ a stack of selfâ€‘attention + feedâ€‘forward blocks is enough for powerful language models.  
- **Building a GPTâ€‘like model** can be done in ~200 lines of PyTorch code when using a tiny dataset.  
- **Scaling** (larger context, more heads, deeper stacks, regularization) yields dramatic loss improvements.  
- **Realâ€‘world ChatGPT** adds two major phases beyond preâ€‘training: supervised fineâ€‘tuning and RLHF.  
- **Further work:**  
  - Experiment with subâ€‘word tokenizers (BPE, SentencePiece).  
  - Train larger models on bigger corpora (e.g., Wikipedia, OpenWebText).  
  - Implement encoderâ€‘decoder architecture for conditional tasks (translation, summarization).  
  - Explore RLHF pipelines to align models with human preferences.  

---  

*End of mindâ€‘map.*  
# Lesson 8

## ğŸ§  Tokenization Mindâ€‘Map (Markdownâ€¯+â€¯Mermaid)

Below is a **comprehensive mindâ€‘map** that captures every major idea, subâ€‘idea and detail from the transcript.  
The map is written in **Mermaid** syntax (supported by most Markdown viewers) and is followed by a plainâ€‘text outline for quick reference.

---  

### Mermaid Diagram  

```mermaid
mindmap
  root((Tokenization in Large Language Models))

    subgraph Overview
      Overview[What is tokenization?]
      WhyItMatters[Why tokenization is the â€œatomâ€ of LLMs]
      HiddenIssues[Hidden footâ€‘guns & odd behaviours]
    end

    subgraph Naive_Char_Level
      CharTokenizer[Characterâ€‘level tokenizer (65 chars)]
      EmbeddingTable[Embedding table = vocab size rows]
      Limitations[Too coarse â†’ real models use chunkâ€‘level]
    end

    subgraph BPE_Concept
      BPE[Byteâ€‘Pair Encoding (BPE)]
      InputEncoding[UTFâ€‘8 â†’ bytes (0â€‘255)]
      InitialVocab[256 raw byte tokens]
      MergeProcess[Iteratively merge mostâ€‘frequent byte pairs]
      VocabularyGrowth[New token IDs appended (256,257,â€¦)]
      Example[Example: â€œAAâ€ â†’ token 256, then â€œABâ€ â†’ token 257 â€¦]
    end

    subgraph Tokenizer_Implementation
      GetStats[Function: get_stats(list of ints)]
      MergeStep[Function: replace_pair(ids, pair, new_id)]
      YLoop[Iterate merges â†’ target vocab size]
      Compression[Sequence length shrinks, vocab grows]
      CodeRepo[MBP repo â€“ reference implementation]
    end

    subgraph Real_World_Tokenizers
      Tiktoken[Tiktoken (OpenAI)]
        TiktokenApp[Web UI â€“ live tokenisation]
        GPT2_Tokenizer[~50â€¯k vocab, 1.24â€‘token context]
        GPT4_Tokenizer[~100â€¯k vocab, denser, better whitespace handling]
        SpecialTokens[<eos>, <pad>, <bos>, <fim> prefixes]
        TokenSizeEffect[More tokens â†’ denser context, but larger embedding & LM head]
      SentencePiece[Google SentencePiece]
        SP_Encoding[Can train & infer]
        SP_BPE[Runs BPE on Unicode codeâ€‘points]
        ByteFallback[Rare codeâ€‘points â†’ UTFâ€‘8 bytes â†’ extra tokens]
        ConfigComplexity[Many hyperâ€‘params, â€œshrinkâ€‘factorâ€, etc.]
        RegexChunking[Regex rules to prevent bad merges (punctuation, numbers, etc.)]
    end

    subgraph Tokenization_Issues
      Spelling[LLMs struggle with spelling (long tokens like â€œdefaultstyleâ€)]
      Arithmetic[Numbers split arbitrarily â†’ poor arithmetic]
      NonEnglish[More tokens for same sentence â†’ context waste]
      Python_Code[Spaces become separate tokens â†’ context loss]
      TrailingSpace[Warning: trailing space adds a token â†’ hurts performance]
      UnstableTokens[â€œunstableâ€ token handling in tiktoken source]
      SolidGoldMagikarp[Rare Redditâ€‘user token never seen in LM training â†’ undefined behaviour]
    end

    subgraph Model_Surgery
      ExtendVocab[Add new special tokens â†’ resize embedding rows]
      LMHeadResize[Resize final linear layer (logits) accordingly]
      FreezeBase[Freeze original weights, train only new token embeddings]
      GistTokens[Compress long prompts into a few learned tokens (distillation)]
    end

    subgraph Multimodal_Tokenization
      VisionTokens[Image patches â†’ tokens]
      AudioTokens[Audio frames â†’ tokens]
      SoftTokens[Continuous embeddings (autoâ€‘encoders) vs hard tokens]
      UnifiedTransformer[Same architecture, different token vocabularies]
    end

    subgraph Efficiency_Considerations
      ContextLength[Longer vocab â†’ shorter sequences â†’ more context per token]
      EmbeddingCost[More rows â†’ more parameters & compute]
      DataFormats[JSON vs YAML token count (JSON 116â€¯tokens, YAML 99â€¯tokens)]
      TokenEconomy[Payâ€‘perâ€‘token APIs â†’ choose dense encodings]
    end

    subgraph Recommendations
      UseGPT4_Tiktoken[Prefer GPTâ€‘4 tokeniser (dense, good whitespace handling)]
      IfTrainingNeeded[Use SentencePiece BPE (but copyâ€‘paste Metaâ€™s config)]
      AvoidDIY[Donâ€™t handâ€‘tune many SP hyperâ€‘params â€“ easy to misâ€‘configure]
      WaitForMâ€‘BPE[Future: a trainingâ€‘ready version of tiktokenâ€™s BPE]
      BewareSpecialTokens[Know specialâ€‘token IDs when fineâ€‘tuning]
    end

    %% Connections
    Overview --> WhyItMatters
    Overview --> HiddenIssues
    Naive_Char_Level --> CharTokenizer
    Naive_Char_Level --> EmbeddingTable
    Naive_Char_Level --> Limitations
    BPE_Concept --> InputEncoding
    BPE_Concept --> InitialVocab
    BPE_Concept --> MergeProcess
    BPE_Concept --> VocabularyGrowth
    BPE_Concept --> Example
    Tokenizer_Implementation --> GetStats
    Tokenizer_Implementation --> MergeStep
    Tokenizer_Implementation --> YLoop
    Tokenizer_Implementation --> Compression
    Tokenizer_Implementation --> CodeRepo
    Real_World_Tokenizers --> Tiktoken
    Real_World_Tokenizers --> SentencePiece
    Tokenization_Issues --> Spelling
    Tokenization_Issues --> Arithmetic
    Tokenization_Issues --> NonEnglish
    Tokenization_Issues --> Python_Code
    Tokenization_Issues --> TrailingSpace
    Tokenization_Issues --> UnstableTokens
    Tokenization_Issues --> SolidGoldMagikarp
    Model_Surgery --> ExtendVocab
    Model_Surgery --> LMHeadResize
    Model_Surgery --> FreezeBase
    Model_Surgery --> GistTokens
    Multimodal_Tokenization --> VisionTokens
    Multimodal_Tokenization --> AudioTokens
    Multimodal_Tokenization --> SoftTokens
    Multimodal_Tokenization --> UnifiedTransformer
    Efficiency_Considerations --> ContextLength
    Efficiency_Considerations --> EmbeddingCost
    Efficiency_Considerations --> DataFormats
    Efficiency_Considerations --> TokenEconomy
    Recommendations --> UseGPT4_Tiktoken
    Recommendations --> IfTrainingNeeded
    Recommendations --> AvoidDIY
    Recommendations --> WaitForMâ€‘BPE
    Recommendations --> BewareSpecialTokens
```

---  

### Plainâ€‘Text Outline (for quick skim)

| **Section** | **Key Points** |
|------------|----------------|
| **Tokenization Overview** | Converts raw text â†’ sequence of integer tokens. Tokens are the atomic unit of LLMs; many hidden quirks stem from this step. |
| **NaÃ¯ve Characterâ€‘Level Tokenizer** | 65â€‘character vocab from Shakespeare data; each char â†’ token; embedding table size = vocab size. Works but far from stateâ€‘ofâ€‘theâ€‘art. |
| **Byteâ€‘Pair Encoding (BPE)** | Start with 256 byte tokens (UTFâ€‘8). Repeatedly find most frequent adjacent byte pair, replace with a new token (ID 256, 257, â€¦). Reduces sequence length while growing vocab. |
| **Implementation Details** | `get_stats` counts consecutive pairs; `merge` replaces a pair with a new ID; a Yâ€‘loop repeats until target vocab size (e.g., 276 â†’ 20 merges). Compression ratio â‰ˆ 1.27 on example text. |
| **Realâ€‘World Tokenizers** | **Tiktoken** (OpenAI): fast inference, preâ€‘trained vocab (GPTâ€‘2 â‰ˆâ€¯50â€¯k, GPTâ€‘4 â‰ˆâ€¯100â€¯k). Handles special tokens (`<eos>`, `<pad>`, `<fim>`). **SentencePiece**: can train & infer, runs BPE on Unicode codeâ€‘points, falls back to byte tokens for rare chars, many configurable options, regexâ€‘based chunking to avoid bad merges. |
| **Tokenization Issues** | â€¢ Spelling: long tokens (e.g., â€œdefaultstyleâ€) make the model treat whole words as single atoms â†’ poor spelling. <br>â€¢ Arithmetic: numbers split arbitrarily (e.g., â€œ127â€ â†’ two tokens) â†’ bad math. <br>â€¢ Nonâ€‘English: same sentence uses many more tokens â†’ context waste. <br>â€¢ Python code: each space becomes a token â†’ huge context consumption. <br>â€¢ Trailing spaces add a token â†’ API warns of degraded performance. <br>â€¢ â€œUnstableâ€ tokens in tiktoken source cause edgeâ€‘case failures. <br>â€¢ â€œSolid Gold Magikarpâ€ â€“ a Redditâ€‘user token never seen during LM training â†’ untrained embedding â†’ undefined behaviour. |
| **Model Surgery for New Tokens** | To add special tokens: enlarge embedding matrix (new rows) and LM head (new columns). Usually freeze original weights, train only new token embeddings. â€œGist tokensâ€ compress long prompts into a few learned tokens via distillation. |
| **Multimodal Tokenization** | Same Transformer can process image patches, audio frames, or soft continuous embeddings as tokens. No architectural change needed â€“ just different vocabularies. |
| **Efficiency & Token Economy** | Larger vocab â†’ shorter sequences â†’ more context per token, but larger embedding & final linear layer â†’ more compute. Choose dense data formats (YAML < JSON) to save tokens; token cost matters for API pricing. |
| **Practical Recommendations** | 1. Use **tiktoken (GPTâ€‘4)** for inference â€“ most efficient. <br>2. If you must train a tokenizer, copy Metaâ€™s SentencePiece config (avoid fiddling with many hyperâ€‘params). <br>3. Donâ€™t reinvent SentencePiece; itâ€™s easy to misâ€‘configure. <br>4. Await a trainingâ€‘ready version of tiktokenâ€™s BPE (Mâ€‘BPE). <br>5. When fineâ€‘tuning, remember to resize embeddings & LM head for any new special tokens. |

---  

#### How to view the diagram
- If youâ€™re using **GitHub**, **GitLab**, **VSâ€¯Code**, **Obsidian**, or any Markdown editor that supports Mermaid, simply copy the Mermaid block above into a `.md` file and it will render automatically.  
- For plainâ€‘text viewers, refer to the outline table.

Feel free to expand any node (e.g., dive deeper into the regex used by SentencePiece, or explore the exact token IDs for special tokens) by adding subâ€‘branches in the Mermaid code. Happy tokenizing! ğŸš€