# ğŸ§  Comprehensive Mindâ€‘Map of the â€œMakemoreâ€ Lecture  

*(All points are derived from the transcript.  The hierarchy reflects the logical flow of ideas, concepts, and implementation details.)*  

---  

## 1ï¸âƒ£ Introduction & Recap  
- **Previous lecture**  
  - Implemented a **bigram language model**  
    - Countâ€‘based version â†’ normalized to probabilities (rows sum toâ€¯1)  
    - Simple neural net with a **single linear layer**  
  - **Limitation:** only one previous character â†’ poor â€œnameâ€‘likeâ€ predictions  

- **Problem with extending the count table**  
  - Context length *k* â†’ table size grows **exponentially** (`27^k` for characters)  
  - Too many rows â†’ insufficient counts â†’ model â€œexplodesâ€  

---  

## 2ï¸âƒ£ Moving to a Multiâ€‘Layer Perceptron (MLP)  
- **Goal:** Predict next character using **multiple previous characters** as context.  
- **Reference paper:** *Bengio etâ€¯al., 2003* (wordâ€‘level, but ideas transfer).  

### 2.1 Core Idea from the Paper  
- **Word embeddings:** each word â†’ 30â€‘dimensional vector (random init, learned).  
- **Neural net:**  
  1. **Embedding lookup** â†’ concatenate embeddings of previous *n* words.  
  2. **Hidden layer** (size = hyperâ€‘parameter).  
  3. **Linear output layer** â†’ logits for all possible next tokens.  
  4. **Softmax** â†’ probability distribution.  
- **Training objective:** Maximize logâ€‘likelihood (same as crossâ€‘entropy).  

### 2.2 Adapting to Characters  
- Vocabulary = **27 characters** (aâ€‘z + â€œ.â€).  
- Embedding dimension initially **2** (for easy visualization).  
- Context length (block size) = **3** characters (can be changed).  

---  

## 3ï¸âƒ£ Implementation Details (PyTorch)  

### 3.1 Data Preparation  
- **Build dataset** (`x`, `y`):  
  - `x` = list of *blockâ€‘size* integer indices (context).  
  - `y` = integer index of the next character.  
  - Pad with zeros for the first *blockâ€‘size* positions.  
- Example (word â€œemmaâ€):  
  - Context `[0,0, e] â†’ label m`, `[0, e, m] â†’ label m`, â€¦  

### 3.2 Embedding Lookup (`C`)  
- `C` shape: **27 Ã— 2** (rows = characters, cols = embedding dim).  
- Two equivalent ways to embed an index `i`:  
  1. Direct indexing: `C[i]`.  
  2. Oneâ€‘hot â†’ matrix multiplication: `one_hot(i) @ C`.  
- For a batch `x` (shape `B Ã— 3`): `C[x]` â†’ **B Ã— 3 Ã— 2** tensor.  

### 3.3 Flattening the Context  
- Need shape **B Ã— (3â€¯Ã—â€¯2) = B Ã— 6** to feed the hidden layer.  
- **Methods:**  
  - `torch.cat([c0, c1, c2], dim=1)` (naÃ¯ve, not generic).  
  - `torch.unbind(x, dim=1)` â†’ tuple of tensors â†’ `torch.cat(..., dim=1)`.  
  - **Best:** `C[x].view(B, -1)` (uses `view` â†’ no extra memory).  

### 3.4 Hidden Layer  
- Weight matrix `W1`: **6 Ã— H** (H = hidden size, e.g., 100).  
- Bias `b1`: **H**.  
- Activation: **tanh** (`torch.tanh`).  

### 3.5 Output Layer  
- Weight matrix `W2`: **H Ã— 27**.  
- Bias `b2`: **27**.  
- Logits: `h @ W2 + b2` â†’ shape **B Ã— 27**.  

### 3.6 Loss Computation  
- **Manual:**  
  - `logits.exp()` â†’ â€œcountsâ€.  
  - Normalize â†’ probabilities.  
  - Pick probability of true class â†’ `-log(p_true)`.  
- **Preferred:** `torch.nn.functional.cross_entropy(logits, y)`  
  - Faster (fused kernels).  
  - Numerically stable (logâ€‘softmax internally).  

### 3.7 Training Loop (Core Steps)  
1. Zero grads: `p.grad = None` for each parameter.  
2. Forward pass â†’ loss.  
3. `loss.backward()` â†’ gradients.  
4. Parameter update: `p -= lr * p.grad`.  

### 3.8 Miniâ€‘Batch Training  
- **Why:** Full dataset (~228â€¯k examples) â†’ too slow.  
- **How:**  
  - Sample random indices `ix = torch.randint(0, N, (batch_size,))`.  
  - Use `x[ix]`, `y[ix]` for each iteration.  
- **Effect:** Noisy gradient â†’ need more steps, but far faster.  

---  

## 4ï¸âƒ£ Hyperâ€‘Parameter Exploration  

| Hyperâ€‘parameter | Description | Typical Values (used) |
|----------------|-------------|----------------------|
| `block_size`   | Number of previous characters | 3 (tried 4,â€¯5,â€¯10) |
| `embed_dim`    | Dimensionality of character embeddings | 2 (visual), 10 (better) |
| `hidden_size`  | Neurons in hidden layer | 100 â†’ 200 â†’ 300 |
| `lr` (learning rate) | Step size for SGD | 0.1 (good), 0.01 (fineâ€‘tune), 0.001 (slow) |
| `batch_size`   | Miniâ€‘batch size | 32 (default), can increase |
| `num_steps`    | Training iterations | 10â€¯k â†’ 200â€¯k (long runs) |
| `lr_decay`     | Reduce LR after N steps | Ã—0.1 after 100â€¯k steps |

### 4.1 Learningâ€‘Rate Search (Practical Trick)  
- Sweep **logâ€‘space**: `lr_exps = torch.linspace(-3, 0, steps=1000)` â†’ `lrs = 10**lr_exps`.  
- Run a few steps for each LR, record loss â†’ plot **LR vs. loss**.  
- Choose LR in the â€œvalleyâ€ (e.g., `10â»Â¹ = 0.1`).  

### 4.2 Overâ€‘/Underâ€‘Fitting Diagnosis  
- **Training loss â‰ˆ validation loss** â†’ **underâ€‘fitting** (model too small).  
- **Training loss << validation loss** â†’ **overâ€‘fitting** (model too large).  
- Adjust hidden size, embed dim, or regularization accordingly.  

---  

## 5ï¸âƒ£ Data Splits & Evaluation  

1. **Training set** â€“ ~80â€¯% of words (â‰ˆâ€¯25â€¯k examples).  
2. **Dev/validation set** â€“ ~10â€¯% (â‰ˆâ€¯3â€¯k examples).  
3. **Test set** â€“ remaining ~10â€¯% (â‰ˆâ€¯2â€¯k examples).  

- **Training** uses only the training split.  
- **Hyperâ€‘parameter tuning** uses the dev set.  
- **Final performance** reported on the test set **once**.  

---  

## 6ï¸âƒ£ Embedding Visualization (2â€‘D case)  

- After training with `embed_dim = 2`, plot each character:  
  - `x = C[:,0]`, `y = C[:,1]`.  
  - Annotate with the character symbol.  
- Observations:  
  - Vowels cluster together â†’ network learns similarity.  
  - Rare symbols (e.g., â€œqâ€, â€œ.â€) occupy distinct regions.  

*When `embed_dim` >â€¯2, direct 2â€‘D plot isnâ€™t possible; consider PCA/tâ€‘SNE.*  

---  

## 7ï¸âƒ£ Sampling from the Trained Model  

1. **Initialize context** with three â€œ.â€ (or any start token).  
2. Loop:  
   - Embed current context â†’ hidden state â†’ logits.  
   - `prob = torch.softmax(logits, dim=-1)`.  
   - Sample next token: `next_idx = torch.multinomial(prob, 1)`.  
   - Shift context window, append `next_idx`.  
3. Convert indices back to characters â†’ generated string.  

- Generated examples look **more nameâ€‘like** (e.g., â€œham joesâ€, â€œemilyâ€).  

---  

## 8ï¸âƒ£ Practical Tips & Extras  

- **Tensor indexing tricks** (list, 1â€‘D tensor, multiâ€‘dim tensor) â†’ `C[x]`.  
- **`view` vs. `reshape`** â€“ `view` is a *noâ€‘copy* operation (fast).  
- **Broadcasting** â€“ Adding bias `b1` to hidden activations works automatically (`BÃ—H` + `H`).  
- **Avoid hardâ€‘coding** magic numbers; use variables (`block_size`, `embed_dim`).  
- **Googleâ€¯Colab** â€“ Readyâ€‘toâ€‘run notebook, no local install needed (link provided in video).  

---  

## 9ï¸âƒ£ Takeâ€‘aways & Next Steps  

- **Achieved**: Loss â‰ˆâ€¯2.17 (better than bigram â‰ˆâ€¯2.45).  
- **Open knobs for improvement**:  
  - Increase hidden size / embedding dimension.  
  - Use longer context (`block_size`).  
  - Experiment with different optimizers (Adam, RMSprop).  
  - Add regularization (weight decay, dropout).  
  - Train longer with proper learningâ€‘rate schedule.  
- **Read the paper** (Bengioâ€¯etâ€¯al., 2003) for deeper insights & advanced ideas.  

---  

### ğŸ“Œ Quick Reference (Pseudoâ€‘code)

```python
# 1. Build dataset
x, y = build_dataset(words, block_size=3)   # x: (N,3), y: (N,)

# 2. Model components
C   = torch.randn(27, embed_dim, requires_grad=True)   # embedding table
W1  = torch.randn(3*embed_dim, hidden, requires_grad=True)
b1  = torch.randn(hidden, requires_grad=True)
W2  = torch.randn(hidden, 27, requires_grad=True)
b2  = torch.randn(27, requires_grad=True)

# 3. Forward pass (batch)
def forward(x_batch):
    e = C[x_batch]                # (B,3,embed_dim)
    e = e.view(e.shape[0], -1)    # (B,3*embed_dim)
    h = torch.tanh(e @ W1 + b1)   # (B,hidden)
    logits = h @ W2 + b2          # (B,27)
    return logits

# 4. Training loop (miniâ€‘batch)
for step in range(num_steps):
    ix = torch.randint(0, N, (batch_size,))
    logits = forward(x[ix])
    loss   = F.cross_entropy(logits, y[ix])
    loss.backward()
    for p in [C,W1,b1,W2,b2]:
        p.data -= lr * p.grad
        p.grad.zero_()
```

---  

*End of mindâ€‘map.*  