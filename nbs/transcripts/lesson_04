# ğŸ§  Mindâ€‘Map of the Lecture  

*(All points are derived from the transcript.  The hierarchy shows the logical flow and relationships between concepts.)*  

---  

## 1ï¸âƒ£  Recap:  Multiâ€‘Layer Perceptron (MLP) for characterâ€‘level language modelling  
- Implemented following **Benj 2003** (MLP â†’ nextâ€‘character prediction).  
- **Current status**  
  - 11â€¯k parameters, 200â€¯k training steps, batchâ€‘sizeâ€¯=â€¯32.  
  - Training/validation loss â‰ˆâ€¯2.16.  
  - Sampling produces readable but imperfect words.  

---  

## 2ï¸âƒ£  Why look deeper?  
- Goal: move to **recurrent / LSTM / GRU** networks.  
- **Prerequisite:** solid intuition of **activations** & **gradients** during training.  
- Understanding these dynamics explains why RNNs are **hard to optimise** with plain firstâ€‘order methods.  

---  

## 3ï¸âƒ£  Problem #1 â€“ Bad Initialisation of the MLP  

### 3.1  Observed symptom  
- **Loss at iterationâ€¯0:**â€¯27â€¯â†’â€¯much higher than expected.  

### 3.2  Expected loss for a uniform softmax  
- 27 possible next characters â†’ uniform probability = 1/27.  
- Negativeâ€‘logâ€‘likelihood = `-log(1/27) â‰ˆ 3.29`.  

### 3.3  What went wrong?  
| Issue | Effect |
|------|--------|
| **Logits far from 0** (extreme values) | Softmax becomes **overâ€‘confident** â†’ huge loss. |
| **Random bias `bâ‚‚`** | Adds a constant offset â†’ pushes logits away from 0. |
| **Weight scale too large** (`Wâ‚‚`) | Amplifies the offset, further saturating softmax. |

### 3.4  Fixes applied  
1. **Zero the output bias** (`bâ‚‚ = 0`).  
2. **Scale down `Wâ‚‚`** (multiply by 0.1 â†’ 0.01).  
3. Keep a tiny nonâ€‘zero variance (e.g., 0.01) for **symmetry breaking**.  

Result: loss curve loses the â€œhockeyâ€‘stickâ€ shape; training becomes more productive.  

---  

## 4ï¸âƒ£  Problem #2 â€“ Saturatedâ€¯`tanh` (ğ‘¡ğ‘ğ‘›â„) activations  

### 4.1  Observation  
- Histogram of hiddenâ€‘state `H` after `tanh` shows **most values at Â±1**.  
- Preâ€‘activations (input to `tanh`) range roughly **â€‘5 â€¦ 15** â†’ many neurons in the **flat tails**.  

### 4.2  Consequence for backâ€‘propagation  
- Derivative of `tanh` = `1 â€“ tÂ²`.  
- When `t â‰ˆ Â±1`, derivative â‰ˆâ€¯0 â†’ **gradient vanishes** for those neurons.  
- â€œDead neuronsâ€ (always saturated) never learn (gradient = 0).  

### 4.3  Diagnostic check  
- Compute **percentage of units with |t|â€¯>â€¯0.99** â†’ large white area in Boolean mask â†’ many dead neurons.  

### 4.4  Remedy  
- Reduce magnitude of preâ€‘activations:  
  - **Scale down the firstâ€‘layer weights** (`Wâ‚`) (e.g., multiply by 0.1).  
  - Optionally **bias = 0** (biases become useless after batchâ€‘norm, see Â§6).  
- Result: hidden activations become **roughly Gaussian (â‰ˆâ€¯ğ’©(0,1))**, gradients stay alive.  

---  

## 5ï¸âƒ£  General Weightâ€‘Initialisation Theory  

### 5.1  Fanâ€‘in / Fanâ€‘out concept  
- For a layer with `fan_in` inputs, initialise weights with variance `1 / fan_in`.  
- Guarantees that **output variance â‰ˆâ€¯input variance** (preserves a unitâ€‘Gaussian flow).  

### 5.2  Gains for different nonâ€‘linearities (He / Xavier)  
| Nonâ€‘linearity | Recommended gain `g` | Reason |
|---------------|----------------------|--------|
| Linear / Identity | `1` | No contraction. |
| ReLU / Leakyâ€‘ReLU | `âˆš2` | Half the distribution is zeroed. |
| tanh | `5/3` (â‰ˆâ€¯1.67) | Empirically balances contraction of tanh. |
| (others) | derived from variance analysis | â€“ |

- **Final weight scale** = `gain / âˆšfan_in`.  

---  

## 6ï¸âƒ£  Batch Normalisation (BatchNorm) â€“ The â€œGameâ€‘Changerâ€  

### 6.1  Core idea  
1. **Collect batch statistics** â†’ mean `Î¼_B` and variance `ÏƒÂ²_B`.  
2. **Standardise**: `xÌ‚ = (x â€“ Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)`.  
3. **Learnable affine transform**: `y = Î³Â·xÌ‚ + Î²`.  

### 6.2  Why it works  
- Forces **activations to stay unitâ€‘Gaussian** throughout the network â†’ prevents saturation / vanishing gradients.  
- Acts as a **regulariser** (batchâ€‘wise noise).  

### 6.3  Training vs. Inference  
- **Training:** use batch statistics (`Î¼_B`, `ÏƒÂ²_B`).  
- **Inference:** use **running estimates** (`Î¼Ì‚`, `ÏƒÌ‚Â²`) updated with exponential moving average:  

  `running_mean = momentumÂ·running_mean + (1â€‘momentum)Â·Î¼_B`  

  `running_var  = momentumÂ·running_var  + (1â€‘momentum)Â·ÏƒÂ²_B`  

- `momentum` â‰ˆâ€¯0.1 for large batches; smaller batches may need **lower momentum** (e.g.,â€¯0.001).  

### 6.4  Practical notes  
- **Bias before BatchNorm is useless** â€“ BatchNorm already learns a bias (`Î²`).  
- Set `affine=True` (learnable `Î³`, `Î²`).  
- `eps` (defaultâ€¯1eâ€‘5) avoids divisionâ€‘byâ€‘zero.  
- **No gradient tracking** for running stats (`torch.no_grad()` context).  

---  

## 7ï¸âƒ£  Diagnostic Toolbox for Neuralâ€‘Net Health  

| Diagnostic | What it tells you | Typical â€œgoodâ€ range |
|------------|-------------------|---------------------|
| **Activation histogram** (per layer) | Distribution shape, saturation % | Stdâ€¯â‰ˆâ€¯1, saturationâ€¯<â€¯5â€¯% |
| **Gradient histogram** (per layer) | Gradient magnitude, vanishing/exploding | Similar scale to activations |
| **Weight histogram** | Parameter spread, dead weights | Stdâ€¯â‰ˆâ€¯1 (or as per init) |
| **Updateâ€‘toâ€‘Data ratio** `â€–Î”Î¸â€– / â€–Î¸â€–` (logâ‚â‚€) | Relative step size per iteration | â‰ˆâ€¯â€‘3 (i.e., updates â‰ˆâ€¯0.001â€¯Ã—â€¯parameter) |
| **Learningâ€‘rate sanity check** | If ratio â‰ªâ€¯â€‘3 â†’ LR too low; â‰«â€¯â€‘2 â†’ LR too high | Aim for â€“3â€¯Â±â€¯0.5 |
| **Runningâ€‘mean / var convergence** | BatchNorm stats stabilise? | Small drift after a few epochs |

- Plot these **over training time** (not just a single snapshot) to see trends.  

---  

## 8ï¸âƒ£  Putting It All Together â€“ â€œTorchâ€‘ifyâ€ the Code  

### 8.1  Modular design (mirrors `torch.nn`)  
- **Linear layer** â†’ `nn.Linear(in_features, out_features, bias=False)` (bias omitted when followed by BatchNorm).  
- **BatchNorm1d** â†’ `nn.BatchNorm1d(num_features, eps=1eâ€‘5, momentum=0.001, affine=True)`.  
- **tanh activation** â†’ custom wrapper (or `nn.Tanh`).  

### 8.2  Network construction pattern  

```text
Embedding â†’ Linear â†’ BatchNorm â†’ tanh â†’ Linear â†’ BatchNorm â†’ tanh â†’ â€¦ â†’ Linear â†’ BatchNorm â†’ Softmax
```

- **BatchNorm placed after each Linear, before tanh** (standard practice).  
- Can also be placed after tanh â€“ results are similar.  

### 8.3  Training loop (highâ€‘level)  

1. **Zero grads**.  
2. **Forward pass** (collect activations, apply BatchNorm).  
3. **Compute loss** (crossâ€‘entropy).  
4. **Backward** (`loss.backward()`).  
5. **Optimizer step** (SGD / Adam).  
6. **Update running stats** (handled automatically by `nn.BatchNorm`).  

---  

## 9ï¸âƒ£  Takeâ€‘aways & Outlook  

1. **Initialisation matters** â€“ scaling weights & zeroing biases prevents early â€œoverâ€‘confidenceâ€.  
2. **tanh saturation kills gradients** â€“ keep preâ€‘activations near zero (via weight scaling).  
3. **BatchNorm stabilises deep nets** by constantly reâ€‘Gaussianising activations; it also reduces sensitivity to exact gain choices.  
4. **Diagnostic visualisations** (histograms, updateâ€‘toâ€‘data ratios) are essential for spotting dead neurons, exploding/vanishing gradients, and misâ€‘scaled learning rates.  
5. **Future work**  
   - Move to **recurrent architectures** (RNN, LSTM, GRU) â€“ deeper unrolled graphs will amplify the issues we just mitigated.  
   - Explore **alternative normalisation** (LayerNorm, GroupNorm) that avoid batch coupling.  
   - Leverage **advanced optimisers** (Adam, RMSProp) and **residual connections** for even deeper models.  

---  

### ğŸ“Œ  Quickâ€‘Reference Cheatâ€‘Sheet  

| Concept | Formula / Setting | Typical Value |
|---------|-------------------|---------------|
| Weight init variance | `var = gainÂ² / fan_in` | `gain = 1` (linear), `âˆš2` (ReLU), `5/3` (tanh) |
| Softmax uniform loss | `-log(1/ğ‘˜)` | `k = vocab size` |
| tanh derivative | `1 â€“ tÂ²` | â†’ 0 when `|t| â†’ 1` |
| BatchNorm scaling | `Î³` (learned) | Initialise to `1` |
| BatchNorm shift | `Î²` (learned) | Initialise to `0` |
| Updateâ€‘toâ€‘Data logâ‚â‚€ target | `â‰ˆâ€¯â€‘3` | Adjust LR accordingly |
| Momentum for running stats | `0.1` (large batch) / `0.001` (batchâ€¯=â€¯32) | â€“ |
| Îµ (epsilon) in BN | `1eâ€‘5` | â€“ |

---  

*End of mindâ€‘map.*  