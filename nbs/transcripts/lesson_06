## ğŸ“š Mindâ€‘Map of the Lecture  
*(Markdown + Mermaid diagram for quick visualisation)*  

---  

### 1ï¸âƒ£  Overview  
- **Goal** â€“ Extend a simple characterâ€‘level MLP language model into a deeper, hierarchical architecture (Wavenetâ€‘style).  
- **Context** â€“ Lecture recorded in a hotel room in Kyoto; continuation of previous parts (3â€¯&â€¯4).  

### 2ï¸âƒ£  Data & Baseline Model  
- **Dataset** â€“ 182â€¯000 examples, each: 3â€‘character context â†’ predict 4th character.  
- **Baseline Architecture**  
  - Embedding table (`C`) â†’ Linear â†’ BatchNorm (named *bathroom*) â†’ 1Dâ€‘BatchNorm (`10h`) â†’ Linear output.  
  - 12â€¯000 parameters, validation loss â‰ˆâ€¯2.10.  
- **Observations**  
  - Model already generates plausible â€œnameâ€‘likeâ€ strings.  
  - Too much information is squashed in a single hidden layer.  

### 3ï¸âƒ£  Desired Architectural Changes  
- **Take more context** â€“ increase block size from 3 â†’ 8 (later 16).  
- **Hierarchical fusion** â€“ progressively combine neighboring characters (bigrams â†’ 4â€‘grams â†’ â€¦) instead of flattening all at once.  
- **Wavenet inspiration** â€“ dilated causal convolutions â†’ treeâ€‘like receptive field growth.  

### 4ï¸âƒ£  Refactoring the Code  
#### 4.1  Layer Building Blocks  
- **Linear layer** â€“ simple matrix multiply (mirrors `torch.nn.Linear`).  
- **BatchNorm (bathroom)** â€“ maintains running mean/variance, behaves differently in train vs. eval.  
- **Embedding layer** â€“ lookup table (`nn.Embedding`).  
- **Flatten / â€œFlattenConsecutiveâ€** â€“ custom module to reshape tensors, now able to group *n* consecutive embeddings.  

#### 4.2  Containers  
- **Sequential container** â€“ custom implementation that stores a list of layers and forwards input through them.  
- **Model definition** â€“ `model = Sequential([Embedding, FlattenConsecutive, Linear, BatchNorm, â€¦])`.  

#### 4.3  Debugging & Shapeâ€‘Gymnastics  
- Inspected tensor shapes after each layer (e.g., `BÃ—TÃ—C â†’ BÃ—TÃ—E â†’ BÃ—(TÂ·E)`).  
- Realised flattening to `BÃ—â€‘1` was too aggressive; needed a 3â€‘D view (`BÃ—groupsÃ—(nÂ·E)`).  

### 5ï¸âƒ£  Implementing Hierarchical Fusion  
1. **FlattenConsecutive(n=2)** â€“ groups every 2 consecutive characters â†’ shape `BÃ—(T/2)Ã—(2Â·E)`.  
2. **Stacked linear layers** â€“ each layer processes the grouped embeddings, progressively increasing receptive field.  
3. **Resulting network** â€“ 3 hidden layers, each widening the context (2 â†’ 4 â†’ 8 characters).  

### 6ï¸âƒ£  BatchNorm Bug & Fix  
- **Problem** â€“ `BatchNorm1D` computed statistics over only the first dimension (`B`) â†’ produced perâ€‘position means/variances.  
- **Fix** â€“ Reduce over dimensions `(0,â€¯1)` when input is 3â€‘D, yielding a single mean/variance per channel (`1Ã—1Ã—C`).  
- **Outcome** â€“ More stable statistics, slight validation loss improvement (2.029 â†’ 2.022).  

### 7ï¸âƒ£  Training Results & Hyperâ€‘parameter Tweaks  
| Change | Params | Validation loss |
|--------|--------|-----------------|
| Baseline (3â€‘char) | ~12â€¯k | 2.10 |
| â†‘ Context to 8 chars (flat) | +10â€¯k | 2.02 |
| Hierarchical (3â€‘layer) | ~22â€¯k | 2.029 â†’ 2.022 |
| â†‘ Embedding dim to 24, hidden units â†‘ | ~76â€¯k | **1.99** (first subâ€‘2.0) |
- **Takeaway** â€“ Bigger capacity helps, but training becomes slower; still no systematic hyperâ€‘parameter search.  

### 8ï¸âƒ£  Relation to Convolutional Networks (Wavenet)  
- **Current implementation** â€“ Explicit â€œforâ€‘loopâ€ over each position (inefficient).  
- **Convolutional view** â€“ Same linear filters applied via dilated causal convolutions â†’ parallel GPU kernels, reuse of intermediate results.  
- **Future work** â€“ Replace explicit loops with `nn.Conv1d` (dilated, causal), add gated activations, residual & skip connections.  

### 9ï¸âƒ£  Development Process Insights  
- **Documentation pain** â€“ PyTorch docs are good; the courseâ€™s own â€œPatreonâ€ docs are sparse/inaccurate.  
- **Shape gymnastics** â€“ Constantly checking NCL vs. NLC ordering, using `view`, `reshape`, `permute`.  
- **Prototyping workflow**  
  1. **Jupyter notebook** â€“ rapid testing, shape inspection, debugging.  
  2. **Copyâ€‘paste to VSâ€¯Code repo** â€“ clean module code.  
  3. **Run experiments** via scripts (future: experiment harness).  

### ğŸ”Ÿ  Future Directions (Open Topics)  
1. **Implement true dilated causal convolutions** (Wavenet).  
2. **Add gated linear units, residual & skip connections**.  
3. **Build an experimental harness** â€“ systematic hyperâ€‘parameter sweeps, logging, early stopping.  
4. **Explore other architectures** â€“ RNNs, LSTMs, GRUs, Transformers.  
5. **Beat the current best loss (â‰ˆâ€¯1.99)** â€“ try different channel allocations, embedding sizes, initialization schemes, optimizers.  

---  

## ğŸ§­ Mermaid Mindâ€‘Map (copyâ€‘paste into a Mermaidâ€‘enabled markdown viewer)

```mermaid
mindmap
  root((Characterâ€‘Level Language Model))
    Overview
      Goal
      Context
    Data & Baseline
      Dataset
      Baseline Architecture
        Embedding
        Linear
        BatchNorm (bathroom)
        1Dâ€‘BatchNorm (10h)
        Linear Output
      Baseline Metrics
    Desired Changes
      Larger Context (3â†’8â†’16)
      Hierarchical Fusion
        Bigrams â†’ 4â€‘grams â†’ â€¦
      Wavenet Inspiration
    Refactoring
      Layer Building Blocks
        Linear
        BatchNorm
        Embedding
        FlattenConsecutive
      Containers
        Sequential
      Debugging Shapes
    Hierarchical Fusion Implementation
      FlattenConsecutive(n=2)
      Stacked Linear Layers
      Resulting 3â€‘layer Net
    BatchNorm Bug
      Problem (perâ€‘position stats)
      Fix (reduce over 0,1)
      Outcome
    Training Results
      Table of Changes â†’ Params â†’ Validation loss
      Observations
    Convolutional Relation
      Current explicit loops
      Convolutional view (dilated causal)
      Future: Conv1d + residual/skip
    Development Process
      Documentation challenges
      Shape gymnastics
      Prototyping workflow (Jupyter â†’ VSCode)
    Future Directions
      Dilated convolutions
      Gated units, residuals, skips
      Experiment harness
      RNN/LSTM/Transformer exploration
      Beat loss 1.99
```

*Render the diagram with any Mermaidâ€‘compatible markdown viewer (e.g., VSâ€¯Code, GitHub, HackMD).*  

---  

**TL;DR:**  
We started from a simple 3â€‘character MLP, expanded the context, introduced a hierarchical â€œflattenâ€‘consecutiveâ€ module, fixed a subtle BatchNorm bug, and built a deeper 3â€‘layer network that already beats the 2.0 validationâ€‘loss barrier. The next steps are to replace the explicit loops with true dilated causal convolutions, add residual/skip connections, and set up a proper experimental harness for systematic hyperâ€‘parameter search. Happy hacking!