# ğŸ§  Mindmap â€“ Manual Backâ€‘Propagation for a Twoâ€‘Layer MLP  

---  

## 1ï¸âƒ£ Overview  
- **Goal:** Replace `loss.backward()` with a fully manual backward pass (tensorâ€‘level).  
- **Why?**  
  - Understand the internals of autograd.  
  - Debug subtle bugs (gradient clipping, dead neurons, exploding/vanishing gradients).  
  - Gain intuition about how gradients flow through each operation.  

---  

## 2ï¸âƒ£ Historical Context  
- **~2006 â€“ 2010:**  
  - Researchers (e.g., Hinton & Salakhutdinov) wrote their own backâ€‘prop in MATLAB/NumPy.  
  - Manual gradient computation was the norm.  
- **2014:**  
  - Authorâ€™s â€œFragmented Embeddingsâ€ paper â€“ full manual forwardâ€¯+â€¯backward passes in NumPy.  
- **Today:**  
  - Autograd is standard, but the exercise remains valuable for learning.  

---  

## 3ï¸âƒ£ Network Architecture (forward pass)  

```
Embedding â†’ Linear1 (W1, B1) â†’ 10â€‘H (tanh) â†’ BatchNorm (Î³, Î²) â†’ Linear2 (W2, B2) â†’ Logits â†’ Softmax â†’ Crossâ€‘Entropy
```

| Layer | Shape (batchâ€¯=â€¯32) | Key tensors |
|------|-------------------|-------------|
| **Embedding** | 32â€¯Ã—â€¯3â€¯Ã—â€¯10 | `C` (27â€¯Ã—â€¯10) |
| **Linear1** | 32â€¯Ã—â€¯64 | `W1` (64â€¯Ã—â€¯64), `B1` (1â€¯Ã—â€¯64) |
| **10â€‘H** | 32â€¯Ã—â€¯64 | `H = tanh(preact)` |
| **BatchNorm** | 32â€¯Ã—â€¯64 | `Î¼` (1â€¯Ã—â€¯64), `ÏƒÂ²` (1â€¯Ã—â€¯64), `Î³`, `Î²` |
| **Linear2** | 32â€¯Ã—â€¯27 | `W2` (27â€¯Ã—â€¯64), `B2` (1â€¯Ã—â€¯27) |
| **Logits** | 32â€¯Ã—â€¯27 | `logits` |
| **Softmax** | 32â€¯Ã—â€¯27 | `probs` |
| **Loss** | scalar | `loss = -mean(log_probs[range(N), Y])` |

---  

## 4ï¸âƒ£ Manual Backâ€‘Propagation â€“ Core Concepts  

### 4.1 Gradient of the Loss w.r.t. `log_probs` (`d_log_probs`)  
- **Shape:** 32â€¯Ã—â€¯27 (same as `log_probs`).  
- **Derivation:**  
  - `loss = -(1/N) Î£_i log_probs[i, Y[i]]`  
  - `âˆ‚loss/âˆ‚log_probs[i, j] = -1/N` if `j == Y[i]`, else `0`.  
- **Implementation:**  
  ```python
  d_log_probs = torch.zeros_like(log_probs)
  d_log_probs[torch.arange(N), Y] = -1.0 / N
  ```

### 4.2 Backâ€‘prop through `log` â†’ `probs`  
- `log_probs = torch.log(probs)`  
- Local derivative: `âˆ‚log/âˆ‚probs = 1 / probs` (elementâ€‘wise).  
- Chain rule: `d_probs = d_log_probs / probs`.

### 4.3 Backâ€‘prop through Softmax (logits â†’ probs)  
- **Softmax formula:** `p_i = exp(l_i) / Î£_j exp(l_j)`.  
- **Gradient (batch version):**  
  ```python
  probs = torch.softmax(logits, dim=1)
  d_logits = probs.clone()
  d_logits[torch.arange(N), Y] -= 1   # subtract 1 at correct class
  d_logits /= N                        # average over batch
  ```
- **Intuition:**  
  - Pull up probability of the correct class, push down all others.  
  - Sum of each row of `d_logits` = 0 (conservation of probability).

### 4.4 Linear Layer (W2, B2)  
- Forward: `logits = H @ W2.T + B2`.  
- Gradients:  
  - `d_W2 = d_logits.T @ H` (shape 27â€¯Ã—â€¯64).  
  - `d_B2 = d_logits.sum(dim=0, keepdim=True)`.  
  - `d_H  = d_logits @ W2`.

### 4.5 Batch Normalization  
- **Forward (simplified, Î³â€¯=â€¯1, Î²â€¯=â€¯0):**  
  ```
  Î¼   = mean(H, dim=0)                # 1Ã—64
  ÏƒÂ²  = var(H, dim=0, unbiased=False) # 1Ã—64
  HÌ‚   = (H - Î¼) / sqrt(ÏƒÂ² + Îµ)
  ```
- **Backward (key steps):**  
  1. `d_HÌ‚ = d_H_pre` (gradient from next layer).  
  2. `d_ÏƒÂ² = -0.5 * (d_HÌ‚ * (H-Î¼)) * (ÏƒÂ²+Îµ)^(-3/2)` â†’ sum over batch.  
  3. `d_Î¼  = -d_HÌ‚ / sqrt(ÏƒÂ²+Îµ) - 2 * d_ÏƒÂ² * (H-Î¼) / N`.  
  4. `d_H  = d_HÌ‚ / sqrt(ÏƒÂ²+Îµ) + d_ÏƒÂ² * 2*(H-Î¼)/N + d_Î¼ / N`.  
  5. `d_Î³ = (d_H_pre * HÌ‚).sum(dim=0, keepdim=True)` (if Î³ kept).  
  6. `d_Î² = d_H_pre.sum(dim=0, keepdim=True)` (if Î² kept).  

- **Broadcasting rule:**  
  - When a scalar (e.g., `Î¼`) is broadcast to a matrix, the backward pass **sums** the incoming gradients over the broadcasted dimension.

### 4.6 Activation `10â€‘H` (tanh)  
- Forward: `H = tanh(preact)`.  
- Local derivative: `1 - HÂ²`.  
- Backward: `d_preact = d_H * (1 - H**2)`.

### 4.7 Linear Layer (W1, B1)  
- Same pattern as W2/B2, but with `preact = X @ W1.T + B1`.  
- Gradients:  
  - `d_W1 = d_preact.T @ X`.  
  - `d_B1 = d_preact.sum(dim=0, keepdim=True)`.  
  - `d_X  = d_preact @ W1`.

### 4.8 Embedding Lookup (indexing)  
- Forward: `M[i, k, :] = C[Y[i, k]]`.  
- Backward:  
  ```python
  d_C = torch.zeros_like(C)
  for i in range(N):
      for k in range(3):
          idx = Y[i, k]
          d_C[idx] += d_M[i, k]   # accumulate if same idx appears multiple times
  ```

---  

## 5ï¸âƒ£ Exercises (Progressive Refactoring)  

| Exercise | Whatâ€™s changed | Key takeaway |
|----------|----------------|--------------|
| **1** | Compute every intermediate `d_â€¦` tensor (as above). Verify with `torch.allclose`. | Manual gradients match autograd when shapes & broadcasting are handled correctly. |
| **2** | Derive a **single** analytic expression for `d_logits` (softmaxâ€¯+â€¯crossâ€‘entropy). Implement it in one line. | Much faster forwardâ€¯+â€¯backward; shows that many intermediate ops can be collapsed. |
| **3** | Derive a compact formula for **batchâ€‘norm** backward (see Â§4.5). Implement the whole layer in a few lines. | Highlights the â€œsumâ€‘overâ€‘broadcastâ€ pattern; avoids perâ€‘element code. |
| **4** | Assemble all manual pieces into a full training loop (no `loss.backward()`). | Endâ€‘toâ€‘end manual training yields the same loss & samples as the autograd version. |

---  

## 6ï¸âƒ£ Intuitive Insights  

- **Gradient â€œpushâ€‘pullâ€** on logits:  
  - Correct class gets a **negative** gradient (pull up).  
  - Incorrect classes get a **positive** gradient (push down).  
  - Rowâ€‘wise sum =â€¯0 â†’ probability mass conserved.  

- **Batchâ€‘norm variance bias vs. unbiased:**  
  - Training often uses **biased** estimator (`1/N`).  
  - Inference (running stats) should use **unbiased** (`1/(Nâ€‘1)`).  
  - Mismatch can be a subtle bug; the author prefers the unbiased version throughout.  

- **Broadcast â†” Sum Duality:**  
  - **Forward:** broadcasting replicates a smaller tensor across a larger one.  
  - **Backward:** the gradient w.r.t. the broadcasted tensor is the **sum** of the replicated gradients.  

---  

## 7ï¸âƒ£ Next Steps  

- **Recurrent Neural Networks (RNNs) & LSTMs** â€“ extend manual backâ€‘prop to timeâ€‘unrolled architectures.  
- Explore **gradient clipping**, **weight tying**, and **teacher forcing** with manual gradients.  

---  

### ğŸ“Œ TL;DR  

1. Replace `loss.backward()` with explicit tensorâ€‘level derivatives.  
2. Derive and implement compact formulas for softmaxâ€‘crossâ€‘entropy and batchâ€‘norm.  
3. Verify each step against PyTorchâ€™s autograd.  
4. Assemble a full training loop that runs as fast as the autograd version while giving you full visibility into every gradient flow.  

Happy hacking! ğŸš€  