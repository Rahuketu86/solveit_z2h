# ğŸ§  Makeâ€‘More: Characterâ€‘Level Language Modeling Mindâ€‘Map  

*(All concepts are derived from the transcript.  Indentation = hierarchy.)*  

---  

## 1ï¸âƒ£ Overview  
- **Goal** â€“ Reâ€‘implement *micrograd*â€‘style learning on a new repo **makeâ€‘more**.  
- **Dataset** â€“ `names.txt` (~32â€¯000 unique names scraped from a government site).  
- **Useâ€‘case** â€“ Generate novel, nameâ€‘like strings (e.g., babyâ€‘name suggestions).  

---  

## 2ï¸âƒ£ Core Idea: Characterâ€‘Level Language Model  
- **Treat each name as a sequence of characters** (including start & end tokens).  
- **Model task** â€“ Predict the *next* character given the previous one(s).  

---  

## 3ï¸âƒ£ Bigram (2â€‘gram) Model â€“ The Simple Baseline  
### 3.1 Data Preparation  
- Load `names.txt` â†’ massive string â†’ `splitlines()` â†’ list `words`.  
- Compute:  
  - `num_words â‰ˆ 32â€¯000`  
  - `min_len = 2`, `max_len = 15`  

### 3.2 Extracting Bigrams  
- For each word `w`:  
  ```python
  for c1, c2 in zip(w, w[1:]):   # sliding window of size 2
      bigram = (c1, c2)
  ```  
- Add **special start token** `.` before the first char and **end token** `.` after the last char.  

### 3.3 Counting Frequencies (Dictionary â†’ 2â€‘D Tensor)  
- `counts[prev_char, next_char] += 1` (default 0).  
- Convert to a **28â€¯Ã—â€¯28** integer tensor (`torch.int32`).  
  - 26 letters + **start** (`.`) + **end** (`.`).  

### 3.4 Visualisation (matplotlib)  
- Heatâ€‘map of the count matrix.  
- Observations:  
  - Rows/columns for start/end tokens are mostly zeros (they never appear in the â€œwrongâ€ position).  

### 3.5 Refinement of Token Set  
- Collapse to **27â€¯Ã—â€¯27** matrix (single special token).  
- Reâ€‘index:  
  - `.` â†’ indexâ€¯0, `a` â†’ 1, â€¦, `z` â†’ 26.  

### 3.6 From Counts â†’ Probabilities  
- Rowâ€‘wise normalization:  
  ```python
  probs = counts.float() / counts.sum(dim=1, keepdim=True)
  ```  
- Each row now sums to **1** â†’ a categorical distribution for the next character.  

### 3.7 Sampling Names (using the bigram table)  
- Start at indexâ€¯0 (`.`).  
- Loop:  
  1. Grab current row `p = probs[current_idx]`.  
  2. Sample `next_idx = torch.multinomial(p, 1, replacement=True, generator=g)`.  
  3. Break if `next_idx == 0` (end token).  
  4. Append the decoded character.  

### 3.8 Model Evaluation â€“ Likelihood  
- **Likelihood** = product of probabilities assigned to the true bigrams.  
- **Logâ€‘likelihood** = sum of `log(p_i)`.  
- **Negative Logâ€‘Likelihood (NLL)** = `â€‘log_likelihood` â†’ standard loss (lower is better).  
- Example: NLL â‰ˆ **2.4â€“2.5** on the full training set.  

### 3.9 Smoothing (to avoid zero probabilities)  
- Add a small constant (e.g., `+1`) to every count before normalisation.  
- Guarantees nonâ€‘zero probabilities â†’ prevents infinite NLL for unseen bigrams.  

---  

## 4ï¸âƒ£ Neuralâ€‘Network Reâ€‘Implementation (Gradientâ€‘Based)  
### 4.1 Why Switch?  
- Counting works for bigrams but **doesnâ€™t scale** to longer contexts (e.g., 10â€‘grams).  
- Neural nets can learn **compact, differentiable** representations for arbitrary context lengths.  

### 4.2 Data Encoding â€“ Oneâ€‘Hot Vectors  
- Map each character index `i` â†’ 27â€‘dimensional oneâ€‘hot vector `x_i`.  
- Use `torch.nn.functional.one_hot(indices, num_classes=27)`.  
- Cast to `float32` for NN input.  

### 4.3 Model Architecture (initially)  
- **Linear layer** (no bias): `logits = x @ W`  
  - `W` shape **27â€¯Ã—â€¯27** (each row = logâ€‘counts for a given previous character).  
- **Softmax** â†’ probabilities:  
  ```python
  probs = torch.softmax(logits, dim=1)   # exponentiate + normalise internally
  ```  

### 4.4 Forward Pass (vectorised)  
1. Encode all inputs â†’ `X` (Nâ€¯Ã—â€¯27).  
2. Compute `logits = X @ W`.  
3. `probs = softmax(logits)`.  
4. Gather the probability of the *true* next character:  
   ```python
   true_probs = probs[torch.arange(N), targets]   # targets = nextâ€‘char indices
   ```  
5. Compute **NLL loss**:  
   ```python
   loss = -torch.log(true_probs).mean()
   ```  

### 4.5 Backâ€‘Propagation & Parameter Update  
- Zero grads: `W.grad = None`.  
- `loss.backward()` â†’ fills `W.grad`.  
- Gradient descent step (e.g., SGD):  
  ```python
  W.data -= lr * W.grad
  ```  
- Repeat for many epochs â†’ loss drops from ~3.8 â†’ ~2.4 (matches counting approach).  

### 4.6 Practical PyTorch Tips (from the transcript)  
- `torch.tensor` vs `torch.Tensor` â†’ prefer lowercase `torch.tensor` for float tensors.  
- **Broadcasting**: dividing a (27â€¯Ã—â€¯27) matrix by a (27â€¯Ã—â€¯1) column works because the column is broadcast across rows.  
- `requires_grad=True` on parameters to enable autograd.  
- Use `torch.Generator` with a fixed seed for deterministic sampling.  

---  

## 5ï¸âƒ£ Regularisation & Implicit Smoothing  
- **L2 regularisation** (weight decay) on `W`:  
  ```python
  reg = Î» * (W**2).mean()
  loss_total = loss + reg
  ```  
- When `W` â†’ 0, logits become uniform â†’ equivalent to **label smoothing**.  
- Adjust Î» to control the tradeâ€‘off between fitting data and keeping probabilities smooth.  

---  

## 6ï¸âƒ£ Scaling Beyond Bigrams  
### 6.1 Wordâ€‘Level Modeling  
- Extend the same pipeline to **tokens = words** (instead of characters).  
- Larger vocab â†’ larger embedding/linear layers.  

### 6.2 Longer Contexts (nâ€‘grams, RNNs, Transformers)  
- Feed **multiple previous characters** (or embeddings) into deeper networks:  
  - **RNN / LSTM** â†’ hidden state carries history.  
  - **Transformer** â†’ selfâ€‘attention over the whole context.  
- Output layer always produces **logits â†’ softmax â†’ probability distribution** for the next token.  

### 6.3 Why Neural Nets Scale  
- Counting tables would explode (`|V|^k` entries for kâ€‘gram).  
- Parameter sharing in NN (weights) keeps model size **linear** in vocabulary size, not exponential in context length.  

---  

## 7ï¸âƒ£ Future Roadmap (as hinted in the talk)  
1. **Wordâ€‘level language model** â€“ generate full sentences.  
2. **Imageâ€‘text models** â€“ e.g., DALLÂ·E, Stable Diffusion.  
3. **Full transformer implementation** â€“ equivalent to GPTâ€‘2 at character level, then scale up.  

---  

## 8ï¸âƒ£ Quick Reference Cheatâ€‘Sheet  

| Concept | Symbol / Code | Key Insight |
|--------|---------------|-------------|
| **Start token** | `.` (indexâ€¯0) | Marks beginning of a name |
| **End token** | `.` (indexâ€¯0 after collapse) | Marks termination |
| **Bigram count matrix** | `N` (28â€¯Ã—â€¯28) | Raw frequencies |
| **Probability matrix** | `P = N / N.sum(dim=1, keepdim=True)` | Rowâ€‘wise categorical distribution |
| **Oneâ€‘hot encoding** | `x_i = F.one_hot(i, 27).float()` | Turns integer index into NN input |
| **Weight matrix** | `W` (27â€¯Ã—â€¯27) | Learns logâ€‘counts (logits) |
| **Softmax** | `torch.softmax(logits, dim=1)` | Turns logits â†’ probabilities |
| **Negative Logâ€‘Likelihood** | `loss = -log(p_true).mean()` | Optimisation objective |
| **Gradient step** | `W.data -= lr * W.grad` | Simple SGD update |
| **L2 regularisation** | `Î» * (W**2).mean()` | Encourages smoother (more uniform) predictions |
| **Sampling loop** | `while idx != 0: idx = torch.multinomial(P[idx], 1)` | Generates a new name |

---  

### ğŸ‰ Takeâ€‘away  
- **Counting bigrams** gives a perfect baseline (NLL â‰ˆâ€¯2.4).  
- **Training the same model with gradient descent** reproduces the baseline *and* provides a flexible foundation for more powerful architectures (RNNs, Transformers).  
- Understanding **tensor shapes, broadcasting, and autograd** is essential for scaling up.  

*Happy modeling!* ğŸš€